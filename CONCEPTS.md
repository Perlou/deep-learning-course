# 深度学习核心概念汇总

> 本文档汇总深度学习的核心概念，随学习进度持续更新。

---

## 目录

1. [神经网络基础](#1-神经网络基础)
2. [激活函数](#2-激活函数)
3. [损失函数](#3-损失函数)
4. [优化器](#4-优化器)
5. [正则化技术](#5-正则化技术)
6. [卷积神经网络](#6-卷积神经网络)
7. [循环神经网络](#7-循环神经网络)
8. [注意力机制](#8-注意力机制)
9. [Transformer 架构](#9-transformer-架构)
10. [生成模型](#10-生成模型)

---

## 1. 神经网络基础

### 1.1 感知机 (Perceptron)

感知机是神经网络的最基本单元，模拟生物神经元的工作方式。

```
输入: x₁, x₂, ..., xₙ
权重: w₁, w₂, ..., wₙ
偏置: b

输出: y = f(Σ wᵢxᵢ + b)

其中 f 是激活函数
```

### 1.2 多层感知机 (MLP)

多层感知机由输入层、隐藏层和输出层组成：

```
输入层 → 隐藏层₁ → 隐藏层₂ → ... → 输出层
```

**关键概念**：

- **前向传播 (Forward Propagation)**: 数据从输入到输出的计算过程
- **反向传播 (Backpropagation)**: 梯度从输出到输入的传递过程

### 1.3 梯度下降

梯度下降是优化神经网络的核心算法：

```python
# 参数更新公式
θ = θ - η × ∇L(θ)

# θ: 参数
# η: 学习率
# ∇L(θ): 损失函数对参数的梯度
```

**变体**：

- **批量梯度下降 (Batch GD)**: 使用全部数据计算梯度
- **随机梯度下降 (SGD)**: 使用单个样本计算梯度
- **小批量梯度下降 (Mini-batch GD)**: 使用一批样本计算梯度（最常用）

---

## 2. 激活函数

激活函数引入非线性，使神经网络能够学习复杂函数。

### 2.1 常用激活函数

| 函数           | 公式             | 优点                   | 缺点       |
| -------------- | ---------------- | ---------------------- | ---------- |
| **Sigmoid**    | σ(x) = 1/(1+e⁻ˣ) | 输出范围 (0,1)         | 梯度消失   |
| **Tanh**       | tanh(x)          | 零中心化               | 梯度消失   |
| **ReLU**       | max(0, x)        | 计算简单，缓解梯度消失 | 死亡 ReLU  |
| **Leaky ReLU** | max(αx, x)       | 解决死亡 ReLU          | α 需要调参 |
| **GELU**       | x × Φ(x)         | 平滑，Transformer 常用 | 计算稍复杂 |

### 2.2 如何选择？

- **隐藏层**: 首选 ReLU 或其变体
- **输出层**:
  - 二分类: Sigmoid
  - 多分类: Softmax
  - 回归: 无激活函数（线性）
- **Transformer**: GELU

---

## 3. 损失函数

损失函数衡量模型预测与真实值的差距。

### 3.1 回归任务

```python
# 均方误差 (MSE)
L = (1/n) × Σ(y - ŷ)²

# 平均绝对误差 (MAE)
L = (1/n) × Σ|y - ŷ|
```

### 3.2 分类任务

```python
# 二元交叉熵 (BCE)
L = -[y × log(ŷ) + (1-y) × log(1-ŷ)]

# 多类交叉熵 (CE)
L = -Σ yᵢ × log(ŷᵢ)
```

---

## 4. 优化器

### 4.1 SGD 及其变体

```python
# SGD with Momentum
v = β × v + η × ∇L
θ = θ - v
```

### 4.2 自适应学习率优化器

| 优化器           | 特点                     | 适用场景   |
| ---------------- | ------------------------ | ---------- |
| **Adam**         | 结合 Momentum 和 RMSprop | 通用首选   |
| **AdamW**        | Adam + 权重衰减          | 大模型训练 |
| **SGD+Momentum** | 经典，收敛稳定           | CV 模型    |

---

## 5. 正则化技术

防止过拟合的技术。

### 5.1 Dropout

训练时随机丢弃一部分神经元：

```python
# PyTorch 实现
nn.Dropout(p=0.5)  # 50% 的神经元被丢弃
```

### 5.2 Batch Normalization

对每个 mini-batch 进行归一化：

```python
# 归一化公式
x̂ = (x - μ) / √(σ² + ε)
y = γ × x̂ + β
```

### 5.3 权重正则化

```python
# L2 正则化 (权重衰减)
L_total = L + λ × Σ w²

# L1 正则化 (稀疏性)
L_total = L + λ × Σ |w|
```

---

## 6. 卷积神经网络

_（Phase 5 学习后填充）_

---

## 7. 循环神经网络

_（Phase 6 学习后填充）_

---

## 8. 注意力机制

_（Phase 7 学习后填充）_

---

## 9. Transformer 架构

_（Phase 7 学习后填充）_

---

## 10. 生成模型

_（Phase 8 学习后填充）_

---

> 📝 **注意**: 本文档将随着学习进度持续更新，每完成一个阶段都会补充相应的概念内容。
