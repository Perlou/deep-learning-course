# æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§ä¸ä¼˜åŒ–å®Œå…¨æŒ‡å—

## ğŸ“š ç›®å½•

- [ä¸€ã€åŸºç¡€æ¦‚å¿µå›é¡¾](#ä¸€åŸºç¡€æ¦‚å¿µå›é¡¾)
- [äºŒã€ä¼˜åŒ–å™¨è¯¦è§£](#äºŒä¼˜åŒ–å™¨è¯¦è§£)
- [ä¸‰ã€å­¦ä¹ ç‡ç­–ç•¥](#ä¸‰å­¦ä¹ ç‡ç­–ç•¥)
- [å››ã€æ­£åˆ™åŒ–æŠ€æœ¯](#å››æ­£åˆ™åŒ–æŠ€æœ¯)
- [äº”ã€æƒé‡åˆå§‹åŒ–](#äº”æƒé‡åˆå§‹åŒ–)
- [å…­ã€æ‰¹é‡å¤§å°çš„å½±å“](#å…­æ‰¹é‡å¤§å°çš„å½±å“)
- [ä¸ƒã€æ¢¯åº¦é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#ä¸ƒæ¢¯åº¦é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
- [å…«ã€æ··åˆç²¾åº¦è®­ç»ƒ](#å…«æ··åˆç²¾åº¦è®­ç»ƒ)
- [ä¹ã€é«˜çº§è®­ç»ƒæŠ€å·§](#ä¹é«˜çº§è®­ç»ƒæŠ€å·§)
- [åã€å®æˆ˜è°ƒå‚æŒ‡å—](#åå®æˆ˜è°ƒå‚æŒ‡å—)

---

## ä¸€ã€åŸºç¡€æ¦‚å¿µå›é¡¾

### 1.1 ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒï¼Ÿ

æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æœ¬è´¨æ˜¯ä¸€ä¸ª**ä¼˜åŒ–é—®é¢˜**ï¼šé€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚

```
è®­ç»ƒæµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾“å…¥æ•°æ® â†’ å‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•° â†’ é‡å¤  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒå…¬å¼

**å‚æ•°æ›´æ–°çš„åŸºæœ¬å…¬å¼ï¼š**

```
Î¸(t+1) = Î¸(t) - Î· Â· âˆ‡L(Î¸)
```

- `Î¸`: æ¨¡å‹å‚æ•°
- `Î·`: å­¦ä¹ ç‡
- `âˆ‡L(Î¸)`: æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦

### 1.3 æŸå¤±å‡½æ•°

| ä»»åŠ¡ç±»å‹ | å¸¸ç”¨æŸå¤±å‡½æ•°  | å…¬å¼                             |
| -------- | ------------- | -------------------------------- |
| å›å½’     | MSE           | L = (1/n)Î£(y - Å·)Â²               |
| å›å½’     | MAE           | L = (1/n)Î£\|y - Å·\|              |
| äºŒåˆ†ç±»   | BCE           | L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)] |
| å¤šåˆ†ç±»   | Cross Entropy | L = -Î£yáµ¢Â·log(Å·áµ¢)                 |

---

## äºŒã€ä¼˜åŒ–å™¨è¯¦è§£

### 2.1 ä¼˜åŒ–å™¨æ¼”è¿›å›¾

```
SGD â†’ Momentum â†’ NAG â†’ AdaGrad â†’ RMSprop â†’ Adam â†’ AdamW
 â”‚                                                    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    ä¼˜åŒ–å™¨å‘å±•å†ç¨‹
```

### 2.2 å„ä¼˜åŒ–å™¨è¯¦è§£

#### 2.2.1 SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰

**å…¬å¼ï¼š**

```
Î¸ = Î¸ - Î· Â· âˆ‡L(Î¸)
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

**ç‰¹ç‚¹ï¼š**

- âœ… ç®€å•ç›´è§‚
- âœ… æ³›åŒ–æ€§èƒ½å¥½
- âŒ æ”¶æ•›é€Ÿåº¦æ…¢
- âŒ å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜

---

#### 2.2.2 SGD with Momentumï¼ˆåŠ¨é‡ï¼‰

**åŸç†ï¼š** å¼•å…¥"æƒ¯æ€§"ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘éœ‡è¡

**å…¬å¼ï¼š**

```
v(t) = Î³ Â· v(t-1) + Î· Â· âˆ‡L(Î¸)
Î¸ = Î¸ - v(t)
```

**ç›´è§‚ç†è§£ï¼š**

```
æƒ³è±¡ä¸€ä¸ªçƒä»å±±å¡æ»šä¸‹ï¼š

    â—‹ èµ·ç‚¹              æ²¡æœ‰åŠ¨é‡ï¼šâ—‹ â†’ â—‹ â†’ â—‹ (æ¥å›éœ‡è¡)
   â•±
  â•±                     æœ‰åŠ¨é‡ï¼š  â—‹ â†’ â†’ â†’ â—‹ (å¹³æ»‘åŠ é€Ÿ)
 â•±    â•²
â•±      â•²   âŠ™ æœ€ä½ç‚¹
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

---

#### 2.2.3 Adamï¼ˆAdaptive Moment Estimationï¼‰

**å…¬å¼ï¼š**

```
m(t) = Î²â‚Â·m(t-1) + (1-Î²â‚)Â·g(t)        # ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆå‡å€¼ï¼‰
v(t) = Î²â‚‚Â·v(t-1) + (1-Î²â‚‚)Â·g(t)Â²       # äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ–¹å·®ï¼‰

mÌ‚(t) = m(t) / (1-Î²â‚áµ—)                 # åå·®ä¿®æ­£
vÌ‚(t) = v(t) / (1-Î²â‚‚áµ—)

Î¸ = Î¸ - Î· Â· mÌ‚(t) / (âˆšvÌ‚(t) + Îµ)
```

**é»˜è®¤å‚æ•°ï¼š**

- Î²â‚ = 0.9
- Î²â‚‚ = 0.999
- Îµ = 1e-8

**ä»£ç ç¤ºä¾‹ï¼š**

```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
```

**ç‰¹ç‚¹ï¼š**

- âœ… è‡ªé€‚åº”å­¦ä¹ ç‡
- âœ… æ”¶æ•›é€Ÿåº¦å¿«
- âœ… é€‚åˆå¤§å¤šæ•°åœºæ™¯
- âŒ å¯èƒ½æ³›åŒ–ä¸å¦‚ SGD

---

#### 2.2.4 AdamWï¼ˆAdam with Weight Decayï¼‰

**æ”¹è¿›ç‚¹ï¼š** å°†æƒé‡è¡°å‡ä»æ¢¯åº¦æ›´æ–°ä¸­è§£è€¦

```python
# Adamä¸­çš„L2æ­£åˆ™åŒ–ï¼ˆè€¦åˆï¼‰
gradient = gradient + weight_decay * param

# AdamWä¸­çš„æƒé‡è¡°å‡ï¼ˆè§£è€¦ï¼‰
param = param - lr * weight_decay * param
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
```

**æ¨èåœºæ™¯ï¼š** Transformerã€BERT ç­‰å¤§æ¨¡å‹è®­ç»ƒé¦–é€‰

---

### 2.3 ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å¦‚ä½•é€‰æ‹©ä¼˜åŒ–å™¨ï¼Ÿ                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  æ–°æ‰‹/å¿«é€Ÿå®éªŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Adam                    â”‚
â”‚                                                       â”‚
â”‚  è¿½æ±‚æœ€ä½³æ³›åŒ–æ€§èƒ½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ SGD + Momentum          â”‚
â”‚                                                       â”‚
â”‚  Transformer/NLPä»»åŠ¡ â”€â”€â”€â”€â”€â”€â”€â†’ AdamW                   â”‚
â”‚                                                       â”‚
â”‚  ç¨€ç–æ•°æ® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ AdaGrad/RMSprop         â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä¸‰ã€å­¦ä¹ ç‡ç­–ç•¥

### 3.1 å­¦ä¹ ç‡çš„é‡è¦æ€§

```
å­¦ä¹ ç‡å¤ªå¤§ï¼š                    å­¦ä¹ ç‡å¤ªå°ï¼š                  å­¦ä¹ ç‡åˆé€‚ï¼š
     â•±â•²    â•±â•²                         .                          â•²
    â•±  â•²  â•±  â•² éœ‡è¡å‘æ•£              . .                          â•²
   â•±    â•²â•±    â•²                     . . .                          â•²
  â•±            â•²                   . . . . (æ”¶æ•›å¤ªæ…¢)                âŠ™ æ”¶æ•›
```

### 3.2 å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

#### 3.2.1 Step Decayï¼ˆé˜¶æ¢¯è¡°å‡ï¼‰

æ¯éš”å›ºå®š epochsï¼Œå­¦ä¹ ç‡ä¹˜ä»¥è¡°å‡å› å­

```python
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=30,    # æ¯30ä¸ªepoch
    gamma=0.1        # å­¦ä¹ ç‡ä¹˜ä»¥0.1
)

# è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(epochs):
    train(...)
    scheduler.step()
```

**å¯è§†åŒ–ï¼š**

```
lr â”‚
   â”‚â”€â”€â”€â”€â”
   â”‚    â”‚â”€â”€â”€â”€â”
   â”‚         â”‚â”€â”€â”€â”€â”
   â”‚              â”‚â”€â”€â”€â”€
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ epoch
      30   60   90
```

---

#### 3.2.2 Cosine Annealingï¼ˆä½™å¼¦é€€ç«ï¼‰

**å…¬å¼ï¼š**

```
Î·(t) = Î·_min + 0.5Â·(Î·_max - Î·_min)Â·(1 + cos(Ï€t/T))
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=100,      # å‘¨æœŸ
    eta_min=1e-6    # æœ€å°å­¦ä¹ ç‡
)
```

**å¯è§†åŒ–ï¼š**

```
lr â”‚
   â”‚â•²
   â”‚ â•²
   â”‚  â•²     â•±â•²
   â”‚   â•²   â•±  â•²    ä½™å¼¦æ›²çº¿
   â”‚    â•² â•±    â•²
   â”‚     â•²      â•²
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ epoch
```

---

#### 3.2.3 Warmupï¼ˆé¢„çƒ­ï¼‰

**åŸç†ï¼š** è®­ç»ƒåˆæœŸä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼Œé€æ­¥å¢å¤§åˆ°ç›®æ ‡å­¦ä¹ ç‡

**ä¸ºä»€ä¹ˆéœ€è¦ Warmupï¼š**

- åˆå§‹å‚æ•°éšæœºï¼Œæ¢¯åº¦æ–¹å‘ä¸ç¨³å®š
- å¤§å­¦ä¹ ç‡å®¹æ˜“å¯¼è‡´è®­ç»ƒä¸ç¨³å®š
- è®©æ¨¡å‹"çƒ­èº«"åå†åŠ é€Ÿ

```python
def warmup_lr_scheduler(optimizer, warmup_epochs, initial_lr):
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        return 1.0
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

**å¯è§†åŒ–ï¼š**

```
lr â”‚           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚          â•±
   â”‚         â•±   æ­£å¸¸è®­ç»ƒ
   â”‚        â•±
   â”‚       â•±
   â”‚      â•±  Warmupé˜¶æ®µ
   â”‚     â•±
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ epoch
        5
```

---

#### 3.2.4 Warmup + Cosineï¼ˆå¸¸ç”¨ç»„åˆï¼‰

```python
# ä½¿ç”¨transformersåº“çš„å®ç°
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=1000,
    num_training_steps=10000
)
```

**å¯è§†åŒ–ï¼š**

```
lr â”‚
   â”‚     â•±â•²
   â”‚    â•±  â•²
   â”‚   â•±    â•²
   â”‚  â•±      â•²
   â”‚ â•±        â•²
   â”‚â•±          â•²
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ step
      warmup  cosine
```

---

### 3.3 å­¦ä¹ ç‡æŸ¥æ‰¾å™¨ï¼ˆLearning Rate Finderï¼‰

**åŸç†ï¼š** é€æ­¥å¢å¤§å­¦ä¹ ç‡ï¼Œè§‚å¯Ÿ loss å˜åŒ–ï¼Œæ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡èŒƒå›´

```python
# ä½¿ç”¨pytorch-lightningçš„lr_finder
from pytorch_lightning.tuner import Tuner

trainer = Trainer(...)
tuner = Tuner(trainer)
lr_finder = tuner.lr_find(model)
lr_finder.plot(suggest=True)
```

**ç»“æœè§£è¯»ï¼š**

```
loss â”‚
     â”‚â•²
     â”‚ â•²
     â”‚  â•²_____        â† æœ€ä½³å­¦ä¹ ç‡åŒºé—´
     â”‚        â•²
     â”‚         â•²
     â”‚          â•²  â† losså¼€å§‹ä¸Šå‡ï¼Œå­¦ä¹ ç‡å¤ªå¤§
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ lr (log scale)
        â†‘
    é€‰æ‹©è¿™é‡Œçš„å­¦ä¹ ç‡
```

---

## å››ã€æ­£åˆ™åŒ–æŠ€æœ¯

### 4.1 æ­£åˆ™åŒ–æ¦‚è§ˆ

```
æ­£åˆ™åŒ–æŠ€æœ¯
â”œâ”€â”€ æ˜¾å¼æ­£åˆ™åŒ–
â”‚   â”œâ”€â”€ L1æ­£åˆ™åŒ– (Lasso)
â”‚   â”œâ”€â”€ L2æ­£åˆ™åŒ– (Ridge/Weight Decay)
â”‚   â””â”€â”€ Elastic Net
â”‚
â”œâ”€â”€ éšå¼æ­£åˆ™åŒ–
â”‚   â”œâ”€â”€ Dropout
â”‚   â”œâ”€â”€ DropConnect
â”‚   â”œâ”€â”€ DropPath (Stochastic Depth)
â”‚   â””â”€â”€ Early Stopping
â”‚
â””â”€â”€ å½’ä¸€åŒ–æŠ€æœ¯
    â”œâ”€â”€ Batch Normalization
    â”œâ”€â”€ Layer Normalization
    â”œâ”€â”€ Group Normalization
    â””â”€â”€ Instance Normalization
```

---

### 4.2 L1 å’Œ L2 æ­£åˆ™åŒ–

#### L2 æ­£åˆ™åŒ–ï¼ˆWeight Decayï¼‰

**å…¬å¼ï¼š**

```
L_total = L_original + Î» Â· Î£(Î¸Â²)
```

**æ•ˆæœï¼š** ä½¿æƒé‡è¶‹å‘äºè¾ƒå°çš„å€¼ï¼Œä½†ä¸ä¸ºé›¶

```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
```

#### L1 æ­£åˆ™åŒ–

**å…¬å¼ï¼š**

```
L_total = L_original + Î» Â· Î£|Î¸|
```

**æ•ˆæœï¼š** ä½¿éƒ¨åˆ†æƒé‡å˜ä¸º 0ï¼Œäº§ç”Ÿç¨€ç–è§£

```python
# PyTorchä¸­éœ€æ‰‹åŠ¨å®ç°
l1_lambda = 0.001
l1_reg = sum(param.abs().sum() for param in model.parameters())
loss = loss + l1_lambda * l1_reg
```

**å¯¹æ¯”ï¼š**

```
         L1æ­£åˆ™åŒ–                    L2æ­£åˆ™åŒ–

æƒé‡åˆ†å¸ƒ:  [0, 0.5, 0, 0.8, 0]      [0.1, 0.3, 0.2, 0.4, 0.1]
ç‰¹ç‚¹:      ç¨€ç–è§£                    å¹³æ»‘è§£
ç”¨é€”:      ç‰¹å¾é€‰æ‹©                  é˜²æ­¢è¿‡æ‹Ÿåˆ
```

---

### 4.3 Dropout

**åŸç†ï¼š** è®­ç»ƒæ—¶éšæœº"ä¸¢å¼ƒ"ä¸€éƒ¨åˆ†ç¥ç»å…ƒ

```
è®­ç»ƒæ—¶ï¼ˆp=0.5ï¼‰:                    æµ‹è¯•æ—¶:

â—‹ â”€â”¬â”€ â—‹ â”€â”¬â”€ â—‹                     â—‹ â”€â”¬â”€ â—‹ â”€â”¬â”€ â—‹
   â”‚     â”‚                            â”‚     â”‚
â—‹ â”€â”¼â”€ âœ• â”€â”¼â”€ â—‹    éšæœºä¸¢å¼ƒ         â—‹ â”€â”¼â”€ â—‹ â”€â”¼â”€ â—‹  å…¨éƒ¨æ¿€æ´»
   â”‚     â”‚                            â”‚     â”‚
â—‹ â”€â”¼â”€ â—‹ â”€â”¼â”€ âœ•                     â—‹ â”€â”¼â”€ â—‹ â”€â”¼â”€ â—‹
   â”‚     â”‚                            â”‚     â”‚
âœ• â”€â”´â”€ â—‹ â”€â”´â”€ â—‹                     â—‹ â”€â”´â”€ â—‹ â”€â”´â”€ â—‹
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # åªåœ¨è®­ç»ƒæ—¶ç”Ÿæ•ˆ
        x = self.fc2(x)
        return x
```

**Dropout å˜ä½“ï¼š**

| å˜ä½“            | æè¿°                 | é€‚ç”¨åœºæ™¯                   |
| --------------- | -------------------- | -------------------------- |
| Dropout         | éšæœºä¸¢å¼ƒç¥ç»å…ƒ       | å…¨è¿æ¥å±‚                   |
| Spatial Dropout | ä¸¢å¼ƒæ•´ä¸ªç‰¹å¾å›¾é€šé“   | CNN                        |
| DropPath        | éšæœºä¸¢å¼ƒæ•´ä¸ªæ®‹å·®åˆ†æ”¯ | ResNet, Vision Transformer |
| DropConnect     | éšæœºä¸¢å¼ƒè¿æ¥æƒé‡     | å…¨è¿æ¥å±‚                   |

---

### 4.4 Batch Normalization

**å…¬å¼ï¼š**

```
Î¼_B = (1/m) Â· Î£xáµ¢                    # è®¡ç®—æ‰¹æ¬¡å‡å€¼
ÏƒÂ²_B = (1/m) Â· Î£(xáµ¢ - Î¼_B)Â²          # è®¡ç®—æ‰¹æ¬¡æ–¹å·®
xÌ‚áµ¢ = (xáµ¢ - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)       # å½’ä¸€åŒ–
yáµ¢ = Î³ Â· xÌ‚áµ¢ + Î²                      # ç¼©æ”¾å’Œå¹³ç§»
```

**ä½œç”¨ï¼š**

- âœ… åŠ é€Ÿè®­ç»ƒæ”¶æ•›
- âœ… å…è®¸ä½¿ç”¨æ›´å¤§å­¦ä¹ ç‡
- âœ… å‡å°‘å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿæ€§
- âœ… æœ‰ä¸€å®šçš„æ­£åˆ™åŒ–æ•ˆæœ

**ä»£ç ç¤ºä¾‹ï¼š**

```python
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)  # BNå±‚æ”¾åœ¨Convä¹‹å
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        return x
```

**BN çš„ä½ç½®ä¹‹äº‰ï¼š**

```
æ–¹æ¡ˆ1ï¼ˆåŸè®ºæ–‡ï¼‰: Conv â†’ BN â†’ ReLU
æ–¹æ¡ˆ2ï¼ˆå¸¸ç”¨ï¼‰:   Conv â†’ ReLU â†’ BN
æ–¹æ¡ˆ3: BN â†’ ReLU â†’ Conv (Pre-activation ResNet)
```

---

### 4.5 Layer Normalization

**ä¸ BN çš„åŒºåˆ«ï¼š**

```
Batch Norm:                      Layer Norm:
å¯¹æ¯ä¸ªç‰¹å¾åœ¨batchç»´åº¦å½’ä¸€åŒ–         å¯¹æ¯ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–

  Nä¸ªæ ·æœ¬                           Nä¸ªæ ·æœ¬
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ â† â”‚ â† â”‚ â† â”‚ â† å¯¹è¿™ä¸€è¡Œå½’ä¸€åŒ–   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                    â”‚ â†“ â”‚ â†“ â”‚ â†“ â”‚ â† å¯¹æ¯åˆ—å½’ä¸€åŒ–
â”‚ â† â”‚ â† â”‚ â† â”‚                    â”‚ â†“ â”‚ â†“ â”‚ â†“ â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  ç‰¹å¾ç»´åº¦                          ç‰¹å¾ç»´åº¦
```

**é€‚ç”¨åœºæ™¯ï¼š**

- **Batch Norm**: CNN, å¤§ batch size
- **Layer Norm**: RNN, Transformer, å° batch size

```python
# Layer Normalization
self.ln = nn.LayerNorm(hidden_size)

# Group Normalization (BNå’ŒLNçš„æŠ˜ä¸­)
self.gn = nn.GroupNorm(num_groups=32, num_channels=256)
```

---

### 4.6 Early Stopping

**åŸç†ï¼š** ç›‘æ§éªŒè¯é›†æ€§èƒ½ï¼Œå½“ä¸å†æå‡æ—¶åœæ­¢è®­ç»ƒ

```python
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=10)
for epoch in range(epochs):
    train(...)
    val_loss = validate(...)
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print("Early stopping triggered")
        break
```

**å¯è§†åŒ–ï¼š**

```
loss â”‚
     â”‚  â•²     è®­ç»ƒloss
     â”‚   â•² â•²
     â”‚    â•² â•²_____
     â”‚     â•²       â•²____
     â”‚      â•²___        â•²____  è®­ç»ƒlossç»§ç»­ä¸‹é™
     â”‚          â•²___
     â”‚              â•²  éªŒè¯loss
     â”‚               â•²___
     â”‚                   â•²___â”€â”€â”€â”€â”€â”€â”€â”€  éªŒè¯lossä¸å†ä¸‹é™
     â”‚                         â†‘
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ epoch
                          Early Stopç‚¹
```

---

## äº”ã€æƒé‡åˆå§‹åŒ–

### 5.1 ä¸ºä»€ä¹ˆåˆå§‹åŒ–é‡è¦ï¼Ÿ

```
åˆå§‹åŒ–å¤ªå°:               åˆå§‹åŒ–å¤ªå¤§:              è‰¯å¥½åˆå§‹åŒ–:
æ¢¯åº¦æ¶ˆå¤±                  æ¢¯åº¦çˆ†ç‚¸                 ç¨³å®šè®­ç»ƒ

å±‚1 â†’ å±‚2 â†’ å±‚3          å±‚1 â†’ å±‚2 â†’ å±‚3         å±‚1 â†’ å±‚2 â†’ å±‚3
0.9 â†’ 0.1 â†’ 0.01         2.0 â†’ 10 â†’ 100          1.0 â†’ 1.0 â†’ 1.0
    â†“    â†“                   â†“    â†“                  â†“    â†“
  æ¿€æ´»å€¼è¶Šæ¥è¶Šå°            æ¿€æ´»å€¼çˆ†ç‚¸              æ¿€æ´»å€¼ç¨³å®š
```

### 5.2 å¸¸ç”¨åˆå§‹åŒ–æ–¹æ³•

#### Xavier/Glorot åˆå§‹åŒ–

**é€‚ç”¨äºï¼š** Sigmoid, Tanh æ¿€æ´»å‡½æ•°

**å…¬å¼ï¼š**

```
W ~ U(-âˆš(6/(n_in + n_out)), âˆš(6/(n_in + n_out)))  # å‡åŒ€åˆ†å¸ƒ
W ~ N(0, 2/(n_in + n_out))                         # æ­£æ€åˆ†å¸ƒ
```

```python
nn.init.xavier_uniform_(layer.weight)
nn.init.xavier_normal_(layer.weight)
```

#### He/Kaiming åˆå§‹åŒ–

**é€‚ç”¨äºï¼š** ReLU åŠå…¶å˜ä½“

**å…¬å¼ï¼š**

```
W ~ N(0, 2/n_in)
```

```python
nn.init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='relu')
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
```

### 5.3 åˆå§‹åŒ–é€‰æ‹©æŒ‡å—

| æ¿€æ´»å‡½æ•°     | æ¨èåˆå§‹åŒ– | PyTorch ä»£ç                                   |
| ------------ | ---------- | --------------------------------------------- |
| Sigmoid/Tanh | Xavier     | `xavier_uniform_`                             |
| ReLU         | He         | `kaiming_uniform_(nonlinearity='relu')`       |
| Leaky ReLU   | He         | `kaiming_uniform_(nonlinearity='leaky_relu')` |
| SELU         | LeCun      | `normal_(std=1/sqrt(fan_in))`                 |

### 5.4 å®Œæ•´åˆå§‹åŒ–ç¤ºä¾‹

```python
def init_weights(module):
    if isinstance(module, nn.Linear):
        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Conv2d):
        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, nn.BatchNorm2d):
        nn.init.ones_(module.weight)
        nn.init.zeros_(module.bias)

# åº”ç”¨åˆå§‹åŒ–
model.apply(init_weights)
```

---

## å…­ã€æ‰¹é‡å¤§å°çš„å½±å“

### 6.1 Batch Size çš„æƒè¡¡

```
å° Batch Size                        å¤§ Batch Size
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… æ­£åˆ™åŒ–æ•ˆæœå¥½   â”‚                  â”‚ âœ… è®­ç»ƒæ›´ç¨³å®š     â”‚
â”‚ âœ… æ³›åŒ–æ€§èƒ½æ›´å¥½   â”‚                  â”‚ âœ… GPUåˆ©ç”¨ç‡é«˜    â”‚
â”‚ âœ… å†…å­˜å ç”¨å°‘    â”‚                  â”‚ âœ… å¹¶è¡Œæ•ˆç‡é«˜     â”‚
â”‚ âŒ è®­ç»ƒé€Ÿåº¦æ…¢    â”‚                  â”‚ âŒ æ³›åŒ–å¯èƒ½å˜å·®    â”‚
â”‚ âŒ æ¢¯åº¦å™ªå£°å¤§    â”‚                  â”‚ âŒ å†…å­˜å ç”¨å¤§     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Batch Size ä¸å­¦ä¹ ç‡çš„å…³ç³»

**çº¿æ€§ç¼©æ”¾æ³•åˆ™ï¼š**

```
å½“batch sizeå¢å¤§kå€æ—¶ï¼Œå­¦ä¹ ç‡ä¹Ÿåº”å¢å¤§kå€

ä¾‹å¦‚: batch_size: 32 â†’ 256 (8å€)
      learning_rate: 0.001 â†’ 0.008 (8å€)
```

**ä½†å®è·µä¸­éœ€è¦é…åˆ Warmup ä½¿ç”¨ï¼**

### 6.3 æ¢¯åº¦ç´¯ç§¯ï¼ˆå°æ˜¾å­˜æ¨¡æ‹Ÿå¤§ Batchï¼‰

```python
accumulation_steps = 4  # æ¨¡æ‹Ÿ4å€batch size

optimizer.zero_grad()
for i, (inputs, labels) in enumerate(dataloader):
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss = loss / accumulation_steps  # å½’ä¸€åŒ–æŸå¤±
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**å›¾ç¤ºï¼š**

```
å¸¸è§„è®­ç»ƒ (batch=32):
[Batch1] â†’ backward â†’ update â†’ [Batch2] â†’ backward â†’ update

æ¢¯åº¦ç´¯ç§¯ (å®é™…batch=8, ç´¯ç§¯4æ¬¡=ç­‰æ•ˆbatch=32):
[Batch1] â†’ backward â”€â”¬â†’ [Batch2] â†’ backward â”€â”¬â†’ ... â†’ [Batch4] â†’ backward â†’ update
                     â”‚                        â”‚
                   ç´¯ç§¯æ¢¯åº¦                  ç´¯ç§¯æ¢¯åº¦
```

---

## ä¸ƒã€æ¢¯åº¦é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 7.1 æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸

**æ¢¯åº¦æ¶ˆå¤±ï¼š**

```
æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦ç»è¿‡å¤šæ¬¡è¿ä¹˜å˜å¾—æå°

å±‚10 â† å±‚9 â† å±‚8 â† ... â† å±‚1 â† Loss
0.0001   0.001  0.01       0.1    1.0

é—®é¢˜ï¼šæµ…å±‚å‡ ä¹æ— æ³•æ›´æ–°
```

**æ¢¯åº¦çˆ†ç‚¸ï¼š**

```
æ¢¯åº¦ç»è¿‡å¤šæ¬¡è¿ä¹˜å˜å¾—æå¤§

å±‚10 â† å±‚9 â† å±‚8 â† ... â† å±‚1 â† Loss
10000   1000   100        10    1.0

é—®é¢˜ï¼šå‚æ•°æ›´æ–°è¿‡å¤§ï¼Œè®­ç»ƒä¸ç¨³å®š
```

### 7.2 è§£å†³æ–¹æ¡ˆ

#### 7.2.1 æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰

**æŒ‰èŒƒæ•°è£å‰ªï¼ˆæ¨èï¼‰ï¼š**

```python
# å½“æ¢¯åº¦èŒƒæ•°è¶…è¿‡max_normæ—¶ï¼Œç¼©æ”¾æ¢¯åº¦
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# ä½¿ç”¨ç¤ºä¾‹
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

**æŒ‰å€¼è£å‰ªï¼š**

```python
# å°†æ¢¯åº¦è£å‰ªåˆ°[-clip_value, clip_value]
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

**å¯è§†åŒ–ï¼š**

```
è£å‰ªå‰:                          è£å‰ªå:
æ¢¯åº¦å‘é‡: (3, 4)                 æ¢¯åº¦å‘é‡: (0.6, 0.8)
èŒƒæ•°: 5                          èŒƒæ•°: 1.0 (max_norm)

    â†‘                               â†‘
    â”‚    *(3,4)                     â”‚  *(0.6,0.8)
    â”‚   /                           â”‚ /
    â”‚  /                            â”‚/
    â””â”€â”€â”€â”€â”€â”€â†’                        â””â”€â”€â”€â”€â”€â”€â†’
```

#### 7.2.2 æ®‹å·®è¿æ¥ï¼ˆSkip Connectionsï¼‰

```python
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = out + residual  # æ®‹å·®è¿æ¥
        return F.relu(out)
```

**å›¾ç¤ºï¼š**

```
è¾“å…¥ x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                           â”‚
   â†“                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  Conv1   â”‚                   â”‚
â”‚   BN1    â”‚                   â”‚
â”‚  ReLU    â”‚                   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚
     â”‚                         â”‚
     â†“                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  Conv2   â”‚                   â”‚
â”‚   BN2    â”‚                   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚
     â”‚                         â”‚
     â†“                         â”‚
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€+ â†â”˜  (ç›¸åŠ )
     â”‚
     â†“
   ReLU
     â”‚
     â†“
   è¾“å‡º
```

---

## å…«ã€æ··åˆç²¾åº¦è®­ç»ƒ

### 8.1 ä»€ä¹ˆæ˜¯æ··åˆç²¾åº¦ï¼Ÿ

```
FP32 (å•ç²¾åº¦):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  32ä½
FP16 (åŠç²¾åº¦):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  16ä½

æ··åˆç²¾åº¦: å‰å‘/åå‘ä¼ æ’­ç”¨FP16, å…³é”®è®¡ç®—ç”¨FP32
```

### 8.2 ä¼˜åŠ¿

| æŒ‡æ ‡     | FP32 | æ··åˆç²¾åº¦ |
| -------- | ---- | -------- |
| å†…å­˜å ç”¨ | åŸºå‡† | ~50%     |
| è®­ç»ƒé€Ÿåº¦ | åŸºå‡† | 1.5-3x   |
| ç²¾åº¦æŸå¤± | -    | å‡ ä¹æ—    |

### 8.3 PyTorch å®ç°

```python
from torch.cuda.amp import autocast, GradScaler

# åˆ›å»ºGradScaler
scaler = GradScaler()

for inputs, labels in dataloader:
    optimizer.zero_grad()

    # ä½¿ç”¨autocastè¿›è¡Œæ··åˆç²¾åº¦å‰å‘ä¼ æ’­
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, labels)

    # ä½¿ç”¨scalerè¿›è¡Œç¼©æ”¾åå‘ä¼ æ’­
    scaler.scale(loss).backward()

    # ä½¿ç”¨scaleræ›´æ–°å‚æ•°
    scaler.step(optimizer)
    scaler.update()
```

### 8.4 æ··åˆç²¾åº¦å·¥ä½œåŸç†

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           æ··åˆç²¾åº¦è®­ç»ƒæµç¨‹            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                                    â”‚
        â†“                                                    â†“
    å‰å‘ä¼ æ’­                                             åå‘ä¼ æ’­
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  FP16   â”‚  è®¡ç®—é€Ÿåº¦å¿«                              â”‚  FP16   â”‚
  â”‚  æƒé‡   â”‚  å†…å­˜å ç”¨å°‘                              â”‚  æ¢¯åº¦   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                                    â”‚
        â”‚                                                    â†“
        â”‚                                            Loss Scaling
        â”‚                                           (é˜²æ­¢æ¢¯åº¦ä¸‹æº¢)
        â”‚                                                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“      â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    FP32æƒé‡     â”‚  ä¸»æƒé‡å‰¯æœ¬
                    â”‚     æ›´æ–°        â”‚  ä¿è¯ç²¾åº¦
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä¹ã€é«˜çº§è®­ç»ƒæŠ€å·§

### 9.1 è¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰

```python
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = torchvision.models.resnet50(pretrained=True)

# æ–¹æ³•1: å†»ç»“æ‰€æœ‰å±‚ï¼Œåªè®­ç»ƒæœ€ååˆ†ç±»å±‚
for param in model.parameters():
    param.requires_grad = False

model.fc = nn.Linear(model.fc.in_features, num_classes)

# æ–¹æ³•2: å·®å¼‚åŒ–å­¦ä¹ ç‡
optimizer = torch.optim.Adam([
    {'params': model.layer4.parameters(), 'lr': 1e-4},
    {'params': model.fc.parameters(), 'lr': 1e-3}
], lr=1e-5)  # å…¶ä»–å±‚ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
```

**è¿ç§»å­¦ä¹ ç­–ç•¥ï¼š**

```
æ•°æ®é‡å° + ç›¸ä¼¼ä»»åŠ¡:  å†»ç»“å¤§éƒ¨åˆ†å±‚ï¼Œåªè®­ç»ƒé¡¶å±‚
æ•°æ®é‡å° + ä¸åŒä»»åŠ¡:  å†»ç»“åº•å±‚ï¼Œå¾®è°ƒé¡¶å±‚
æ•°æ®é‡å¤§ + ç›¸ä¼¼ä»»åŠ¡:  å…¨ç½‘ç»œå¾®è°ƒï¼Œå°å­¦ä¹ ç‡
æ•°æ®é‡å¤§ + ä¸åŒä»»åŠ¡:  å…¨ç½‘ç»œè®­ç»ƒï¼Œå¯ä½¿ç”¨é¢„è®­ç»ƒåˆå§‹åŒ–
```

### 9.2 çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰

**åŸç†ï¼š** ç”¨å¤§æ¨¡å‹ï¼ˆTeacherï¼‰çš„çŸ¥è¯†æŒ‡å¯¼å°æ¨¡å‹ï¼ˆStudentï¼‰è®­ç»ƒ

```python
def distillation_loss(student_logits, teacher_logits, labels,
                      temperature=3.0, alpha=0.7):
    """
    è’¸é¦æŸå¤± = Î± * è½¯æ ‡ç­¾æŸå¤± + (1-Î±) * ç¡¬æ ‡ç­¾æŸå¤±
    """
    # è½¯æ ‡ç­¾æŸå¤±
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=1),
        F.softmax(teacher_logits / temperature, dim=1),
        reduction='batchmean'
    ) * (temperature ** 2)

    # ç¡¬æ ‡ç­¾æŸå¤±
    hard_loss = F.cross_entropy(student_logits, labels)

    return alpha * soft_loss + (1 - alpha) * hard_loss
```

**å›¾ç¤ºï¼š**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Teacher    â”‚
                    â”‚  (å¤§æ¨¡å‹)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚  Soft Labels â”‚  å¸¦æ¸©åº¦çš„è½¯æ¦‚ç‡åˆ†å¸ƒ
                    â”‚ [0.7, 0.2, 0.1] â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚            â”‚            â”‚
              â†“            â†“            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Student â”‚  â”‚  Loss   â”‚  â”‚ Ground  â”‚
        â”‚ (å°æ¨¡å‹) â”‚â†â”€â”‚ ç»“åˆ    â”‚â†â”€â”‚ Truth   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.3 æ¨¡å‹é›†æˆï¼ˆModel Ensembleï¼‰

```python
# æ–¹æ³•1: ç®€å•å¹³å‡
def ensemble_predict(models, x):
    predictions = [model(x) for model in models]
    return torch.mean(torch.stack(predictions), dim=0)

# æ–¹æ³•2: åŠ æƒå¹³å‡
def weighted_ensemble(models, weights, x):
    predictions = [w * model(x) for w, model in zip(weights, models)]
    return sum(predictions)

# æ–¹æ³•3: æŠ•ç¥¨
def voting_ensemble(models, x):
    predictions = [model(x).argmax(dim=1) for model in models]
    stacked = torch.stack(predictions, dim=1)
    return torch.mode(stacked, dim=1).values
```

### 9.4 æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰

**åŸç†ï¼š** é¿å…æ¨¡å‹è¿‡äºè‡ªä¿¡ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

```python
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        n_classes = pred.size(-1)

        # å°†ç¡¬æ ‡ç­¾è½¬æ¢ä¸ºè½¯æ ‡ç­¾
        # [0, 1, 0, 0] â†’ [0.025, 0.925, 0.025, 0.025]
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (n_classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)

        return torch.mean(torch.sum(-true_dist * F.log_softmax(pred, dim=-1), dim=-1))
```

**å¯¹æ¯”ï¼š**

```
ç¡¬æ ‡ç­¾:  [0, 0, 1, 0, 0]  # åªæœ‰æ­£ç¡®ç±»åˆ«ä¸º1

è½¯æ ‡ç­¾ (smoothing=0.1):
         [0.025, 0.025, 0.9, 0.025, 0.025]  # åˆ†æ•£ä¸€äº›æ¦‚ç‡ç»™å…¶ä»–ç±»
```

### 9.5 Mixup æ•°æ®å¢å¼º

**åŸç†ï¼š** å¯¹è®­ç»ƒæ ·æœ¬å’Œæ ‡ç­¾è¿›è¡Œçº¿æ€§æ’å€¼æ··åˆ

```python
def mixup_data(x, y, alpha=0.2):
    """
    æ··åˆä¸¤ä¸ªæ ·æœ¬
    """
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]

    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# è®­ç»ƒå¾ªç¯
for x, y in dataloader:
    mixed_x, y_a, y_b, lam = mixup_data(x, y)
    outputs = model(mixed_x)
    loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)
```

**å¯è§†åŒ–ï¼š**

```
å›¾ç‰‡A (çŒ«):          å›¾ç‰‡B (ç‹—):           æ··åˆç»“æœ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ±    â”‚    +    â”‚  ğŸ•    â”‚    =     â”‚ ğŸ±+ğŸ•  â”‚
â”‚         â”‚  Î»=0.7 â”‚         â”‚ (1-Î»)=0.3â”‚ é€æ˜å åŠ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ ‡ç­¾:[1,0]          æ ‡ç­¾:[0,1]           æ ‡ç­¾:[0.7, 0.3]
```

---

## åã€å®æˆ˜è°ƒå‚æŒ‡å—

### 10.1 è°ƒå‚é¡ºåº

```
Step 1: ç¡®ä¿ä»£ç æ­£ç¡®
        â†“
        åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹ŸåˆéªŒè¯
        â†“
Step 2: è°ƒæ•´å­¦ä¹ ç‡
        â†“
        ä½¿ç”¨å­¦ä¹ ç‡finderæˆ–ç½‘æ ¼æœç´¢
        â†“
Step 3: è°ƒæ•´batch size
        â†“
        åœ¨æ˜¾å­˜å…è®¸èŒƒå›´å†…å°è¯•ä¸åŒå¤§å°
        â†“
Step 4: è°ƒæ•´ç½‘ç»œç»“æ„
        â†“
        æ·±åº¦ã€å®½åº¦ã€æ­£åˆ™åŒ–
        â†“
Step 5: è°ƒæ•´å…¶ä»–è¶…å‚æ•°
        â†“
        dropout rate, weight decayç­‰
        â†“
Step 6: ä½¿ç”¨é«˜çº§æŠ€å·§
        â†“
        å­¦ä¹ ç‡è°ƒåº¦ã€æ•°æ®å¢å¼ºã€mixupç­‰
```

### 10.2 è¶…å‚æ•°æœç´¢ç­–ç•¥

```python
# ç½‘æ ¼æœç´¢
from sklearn.model_selection import ParameterGrid

param_grid = {
    'lr': [1e-4, 1e-3, 1e-2],
    'batch_size': [16, 32, 64],
    'weight_decay': [1e-5, 1e-4, 1e-3]
}

for params in ParameterGrid(param_grid):
    model = create_model()
    train_and_evaluate(model, **params)
```

```python
# éšæœºæœç´¢ï¼ˆæ›´é«˜æ•ˆï¼‰
import random

def random_search(n_trials=20):
    for _ in range(n_trials):
        params = {
            'lr': 10 ** random.uniform(-5, -2),
            'batch_size': random.choice([16, 32, 64, 128]),
            'weight_decay': 10 ** random.uniform(-6, -3),
            'dropout': random.uniform(0.1, 0.5)
        }
        train_and_evaluate(**params)
```

### 10.3 å¸¸è§é—®é¢˜è¯Šæ–­

```
é—®é¢˜: è®­ç»ƒlossä¸ä¸‹é™
â”œâ”€â”€ æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦å¤ªå°
â”œâ”€â”€ æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®
â”œâ”€â”€ æ£€æŸ¥æ ‡ç­¾æ˜¯å¦æœ‰é—®é¢˜
â””â”€â”€ æ£€æŸ¥æ¨¡å‹è¾“å‡ºç»´åº¦æ˜¯å¦åŒ¹é…

é—®é¢˜: è®­ç»ƒlossä¸‹é™ä½†éªŒè¯lossä¸é™
â”œâ”€â”€ è¿‡æ‹Ÿåˆ â†’ æ·»åŠ æ­£åˆ™åŒ–
â”œâ”€â”€ æ•°æ®æ³„éœ² â†’ æ£€æŸ¥æ•°æ®åˆ’åˆ†
â””â”€â”€ éªŒè¯é›†å¤ªå° â†’ å¢åŠ éªŒè¯é›†

é—®é¢˜: losså˜æˆNaN
â”œâ”€â”€ å­¦ä¹ ç‡å¤ªå¤§ â†’ å‡å°å­¦ä¹ ç‡
â”œâ”€â”€ æ¢¯åº¦çˆ†ç‚¸ â†’ æ·»åŠ æ¢¯åº¦è£å‰ª
â”œâ”€â”€ æ•°æ®æœ‰é—®é¢˜ â†’ æ£€æŸ¥æ˜¯å¦æœ‰NaN/Inf
â””â”€â”€ æ•°å€¼ä¸ç¨³å®š â†’ æ·»åŠ epsæˆ–ä½¿ç”¨log_softmax

é—®é¢˜: è®­ç»ƒä¸ç¨³å®š
â”œâ”€â”€ æ·»åŠ BatchNorm/LayerNorm
â”œâ”€â”€ ä½¿ç”¨warmup
â”œâ”€â”€ å‡å°å­¦ä¹ ç‡
â””â”€â”€ ä½¿ç”¨æ¢¯åº¦è£å‰ª
```

### 10.4 è®­ç»ƒç›‘æ§æ¨¡æ¿

```python
import wandb  # æˆ– tensorboard

def train_with_monitoring():
    wandb.init(project='my-project')

    for epoch in range(epochs):
        model.train()
        train_loss = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()

            # ç›‘æ§æ¢¯åº¦èŒƒæ•°
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            train_loss += loss.item()

            # è®°å½•æ¯ä¸ªbatch
            if batch_idx % 100 == 0:
                wandb.log({
                    'batch_loss': loss.item(),
                    'grad_norm': grad_norm,
                    'lr': optimizer.param_groups[0]['lr']
                })

        # éªŒè¯
        val_loss, val_acc = validate(model, val_loader)

        # è®°å½•æ¯ä¸ªepoch
        wandb.log({
            'epoch': epoch,
            'train_loss': train_loss / len(train_loader),
            'val_loss': val_loss,
            'val_acc': val_acc
        })

        scheduler.step()
```

### 10.5 å®Œæ•´è®­ç»ƒä»£ç æ¨¡æ¿

```python
import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler

class Trainer:
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model.to(config.device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.lr,
            weight_decay=config.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.epochs
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

        # æ··åˆç²¾åº¦
        self.scaler = GradScaler()

        # Early Stopping
        self.early_stopping = EarlyStopping(patience=config.patience)

        # æœ€ä½³æ¨¡å‹
        self.best_val_loss = float('inf')

    def train_epoch(self):
        self.model.train()
        total_loss = 0

        for data, target in self.train_loader:
            data, target = data.to(self.config.device), target.to(self.config.device)

            self.optimizer.zero_grad()

            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with autocast():
                output = self.model(data)
                loss = self.criterion(output, target)

            # åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()

            # æ¢¯åº¦è£å‰ª
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            # æ›´æ–°å‚æ•°
            self.scaler.step(self.optimizer)
            self.scaler.update()

            total_loss += loss.item()

        return total_loss / len(self.train_loader)

    @torch.no_grad()
    def validate(self):
        self.model.eval()
        total_loss = 0
        correct = 0

        for data, target in self.val_loader:
            data, target = data.to(self.config.device), target.to(self.config.device)
            output = self.model(data)
            total_loss += self.criterion(output, target).item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()

        val_loss = total_loss / len(self.val_loader)
        val_acc = correct / len(self.val_loader.dataset)

        return val_loss, val_acc

    def train(self):
        for epoch in range(self.config.epochs):
            train_loss = self.train_epoch()
            val_loss, val_acc = self.validate()
            self.scheduler.step()

            print(f'Epoch {epoch}: Train Loss={train_loss:.4f}, '
                  f'Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                torch.save(self.model.state_dict(), 'best_model.pt')

            # Early Stopping
            self.early_stopping(val_loss)
            if self.early_stopping.early_stop:
                print("Early stopping triggered")
                break
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹é€ŸæŸ¥è¡¨

| ç±»åˆ«       | æŠ€å·§                      | å¸¸ç”¨é…ç½®              |
| ---------- | ------------------------- | --------------------- |
| **ä¼˜åŒ–å™¨** | AdamW                     | lr=1e-3~1e-4, wd=0.01 |
| **å­¦ä¹ ç‡** | Cosine + Warmup           | warmup_ratio=0.1      |
| **æ­£åˆ™åŒ–** | Dropout + Label Smoothing | p=0.1~0.5, smooth=0.1 |
| **å½’ä¸€åŒ–** | BatchNorm/LayerNorm       | è§†åœºæ™¯é€‰æ‹©            |
| **æ¢¯åº¦**   | æ¢¯åº¦è£å‰ª                  | max_norm=1.0          |
| **ç²¾åº¦**   | æ··åˆç²¾åº¦                  | FP16                  |
| **åˆå§‹åŒ–** | He åˆå§‹åŒ–                 | å¯¹ ReLU               |
| **Batch**  | æ¢¯åº¦ç´¯ç§¯                  | æŒ‰æ˜¾å­˜è°ƒæ•´            |

### è®­ç»ƒ Checklist

- [ ] ä»£ç åœ¨å°æ•°æ®ä¸Šèƒ½è¿‡æ‹Ÿåˆ
- [ ] ä½¿ç”¨äº†é€‚å½“çš„æƒé‡åˆå§‹åŒ–
- [ ] é€‰æ‹©äº†åˆé€‚çš„ä¼˜åŒ–å™¨ï¼ˆAdamW æ¨èï¼‰
- [ ] è®¾ç½®äº†å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
- [ ] æ·»åŠ äº†å¿…è¦çš„æ­£åˆ™åŒ–
- [ ] ä½¿ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒ
- [ ] æ·»åŠ äº†æ¢¯åº¦è£å‰ª
- [ ] è®¾ç½®äº† Early Stopping
- [ ] ä¿å­˜äº†æœ€ä½³æ¨¡å‹ checkpoint
- [ ] è®°å½•äº†è®­ç»ƒæ—¥å¿—å’Œæ›²çº¿

---

_æ–‡æ¡£ç‰ˆæœ¬: v1.0_  
_é€‚ç”¨æ¡†æ¶: PyTorch_
