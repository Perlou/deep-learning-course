"""
03-vanishing-gradient.py - æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜

æœ¬èŠ‚å­¦ä¹ :
1. æ¢¯åº¦æ¶ˆå¤±çš„åŸå› 
2. æ¢¯åº¦çˆ†ç‚¸çš„åŸå› 
3. å¯è§†åŒ–æ¢¯åº¦é—®é¢˜
4. è§£å†³æ–¹æ¡ˆ
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams["font.sans-serif"] = ["Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

print("=" * 60)
print("ç¬¬3èŠ‚: æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜")
print("=" * 60)

# =============================================================================
# 1. æ¢¯åº¦æ¶ˆå¤±çš„åŸå› 
# =============================================================================
print("""
ğŸ“š æ¢¯åº¦æ¶ˆå¤±çš„æ•°å­¦åŸç†

é“¾å¼æ³•åˆ™å¯¼è‡´çš„æ¢¯åº¦è¡°å‡:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  âˆ‚hâ‚œ/âˆ‚hâ‚ = âˆ‚hâ‚œ/âˆ‚hâ‚œâ‚‹â‚ Â· âˆ‚hâ‚œâ‚‹â‚/âˆ‚hâ‚œâ‚‹â‚‚ Â· ... Â· âˆ‚hâ‚‚/âˆ‚hâ‚         â”‚
â”‚                                                              â”‚
â”‚  å…¶ä¸­æ¯ä¸€é¡¹:                                                  â”‚
â”‚    âˆ‚hâ‚–/âˆ‚hâ‚–â‚‹â‚ = Wâ‚•â‚•áµ€ Â· diag(tanh'(zâ‚–))                       â”‚
â”‚                                                              â”‚
â”‚  tanh çš„å¯¼æ•°èŒƒå›´: (0, 1]                                      â”‚
â”‚  æœ€å¤§å€¼ = 1 (å½“ z = 0 æ—¶)                                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®é—®é¢˜:
  â€¢ å¦‚æœ ||Wâ‚•â‚•|| < 1 ä¸” |tanh'| < 1: æ¢¯åº¦æŒ‡æ•°çº§è¡°å‡ â†’ æ¶ˆå¤±
  â€¢ å¦‚æœ ||Wâ‚•â‚•|| > 1: æ¢¯åº¦æŒ‡æ•°çº§å¢é•¿ â†’ çˆ†ç‚¸
""")


# =============================================================================
# 2. å¯è§†åŒ– tanh åŠå…¶å¯¼æ•°
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 2. å¯è§†åŒ– tanh åŠå…¶å¯¼æ•°")
print("-" * 60)

x = np.linspace(-5, 5, 200)
y_tanh = np.tanh(x)
y_tanh_deriv = 1 - np.tanh(x) ** 2

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# tanh å‡½æ•°
axes[0].plot(x, y_tanh, "b-", linewidth=2, label="tanh(x)")
axes[0].axhline(y=0, color="k", linestyle="-", linewidth=0.5)
axes[0].axhline(y=1, color="r", linestyle="--", alpha=0.5)
axes[0].axhline(y=-1, color="r", linestyle="--", alpha=0.5)
axes[0].set_xlabel("x")
axes[0].set_ylabel("tanh(x)")
axes[0].set_title("tanh å‡½æ•°")
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# tanh å¯¼æ•°
axes[1].plot(x, y_tanh_deriv, "orange", linewidth=2, label="tanh'(x)")
axes[1].axhline(y=1, color="r", linestyle="--", alpha=0.5, label="æœ€å¤§å€¼=1")
axes[1].axhline(y=0.25, color="g", linestyle="--", alpha=0.5)
axes[1].fill_between(x, y_tanh_deriv, alpha=0.3)
axes[1].set_xlabel("x")
axes[1].set_ylabel("tanh'(x)")
axes[1].set_title("tanh å¯¼æ•° (æœ€å¤§å€¼=1, |x|>2 æ—¶æ¥è¿‘0)")
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("outputs/tanh_derivative.png", dpi=100)
plt.close()
print("tanh å¯¼æ•°å›¾å·²ä¿å­˜: outputs/tanh_derivative.png")


# =============================================================================
# 3. æ¢¯åº¦æ¶ˆå¤±çš„æ•°å€¼å®éªŒ
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 3. æ¢¯åº¦æ¶ˆå¤±çš„æ•°å€¼å®éªŒ")
print("-" * 60)


def compute_gradient_norms(model, seq_len, input_size=10, hidden_size=50):
    """è®¡ç®—ä¸åŒæ—¶é—´æ­¥çš„æ¢¯åº¦èŒƒæ•°"""
    x = torch.randn(1, seq_len, input_size, requires_grad=True)
    h0 = torch.zeros(1, 1, hidden_size)

    output, _ = model(x, h0)

    # åªå¯¹æœ€åä¸€ä¸ªæ—¶åˆ»çš„è¾“å‡ºæ±‚å¯¼
    loss = output[:, -1, :].sum()
    loss.backward()

    # è®¡ç®—è¾“å…¥å„æ—¶é—´æ­¥çš„æ¢¯åº¦èŒƒæ•°
    grad_norms = []
    for t in range(seq_len):
        grad_norm = x.grad[0, t, :].norm().item()
        grad_norms.append(grad_norm)

    return grad_norms


# æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
seq_lengths = [10, 25, 50, 100]
hidden_size = 50
input_size = 10

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for idx, seq_len in enumerate(seq_lengths):
    rnn = nn.RNN(input_size, hidden_size, batch_first=True)
    grad_norms = compute_gradient_norms(rnn, seq_len, input_size, hidden_size)

    # ä»æœ€åæ—¶åˆ»å¾€å‰çœ‹æ¢¯åº¦
    time_steps = list(range(seq_len))
    distance_from_end = [seq_len - t for t in time_steps]

    axes[idx].plot(distance_from_end, grad_norms, "b-o", markersize=3)
    axes[idx].set_xlabel("è·ç¦»æœ€åæ—¶åˆ»çš„æ­¥æ•°")
    axes[idx].set_ylabel("æ¢¯åº¦èŒƒæ•°")
    axes[idx].set_title(f"åºåˆ—é•¿åº¦ = {seq_len}")
    axes[idx].set_yscale("log")  # å¯¹æ•°åˆ»åº¦æ›´æ¸…æ¥š
    axes[idx].grid(True, alpha=0.3)
    axes[idx].invert_xaxis()  # åè½¬ x è½´

plt.suptitle("RNN æ¢¯åº¦æ¶ˆå¤±ç°è±¡ (è¶Šè¿œç¦»è¾“å‡ºï¼Œæ¢¯åº¦è¶Šå°)", fontsize=14)
plt.tight_layout()
plt.savefig("outputs/gradient_vanishing.png", dpi=100)
plt.close()
print("æ¢¯åº¦æ¶ˆå¤±å¯è§†åŒ–å·²ä¿å­˜: outputs/gradient_vanishing.png")


# =============================================================================
# 4. æ¢¯åº¦çˆ†ç‚¸å®éªŒ
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 4. æ¢¯åº¦çˆ†ç‚¸å®éªŒ")
print("-" * 60)


def demonstrate_gradient_explosion():
    """æ¼”ç¤ºæ¢¯åº¦çˆ†ç‚¸"""
    # ä½¿ç”¨è¾ƒå¤§çš„æƒé‡åˆå§‹åŒ–
    rnn = nn.RNN(10, 50, batch_first=True)

    # äººä¸ºæ”¾å¤§æƒé‡
    with torch.no_grad():
        rnn.weight_hh_l0 *= 2.0

    x = torch.randn(1, 50, 10, requires_grad=True)
    h0 = torch.zeros(1, 1, 50)

    output, _ = rnn(x, h0)
    loss = output[:, -1, :].sum()

    try:
        loss.backward()
        grad_norm = x.grad.norm().item()
        print(f"  æ¢¯åº¦èŒƒæ•°: {grad_norm:.2f}")
        if grad_norm > 1000:
            print("  âš ï¸ æ¢¯åº¦çˆ†ç‚¸ï¼")
        elif grad_norm < 0.01:
            print("  âš ï¸ æ¢¯åº¦æ¶ˆå¤±ï¼")
        else:
            print("  âœ“ æ¢¯åº¦æ­£å¸¸")
    except RuntimeError as e:
        print(f"  å‘ç”Ÿé”™è¯¯: {e}")


print("\næµ‹è¯•æ¢¯åº¦çˆ†ç‚¸:")
demonstrate_gradient_explosion()


# =============================================================================
# 5. è§£å†³æ–¹æ¡ˆ
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 5. è§£å†³æ–¹æ¡ˆ")
print("-" * 60)

print("""
è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„æ–¹æ³•:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ–¹æ³•              â”‚ è§£å†³é—®é¢˜     â”‚ è¯´æ˜                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LSTM/GRU          â”‚ æ¢¯åº¦æ¶ˆå¤±    â”‚ é—¨æ§æœºåˆ¶æä¾›æ¢¯åº¦é«˜é€Ÿå…¬è·¯  â”‚
â”‚  æ¢¯åº¦è£å‰ª          â”‚ æ¢¯åº¦çˆ†ç‚¸    â”‚ é™åˆ¶æ¢¯åº¦èŒƒæ•°             â”‚
â”‚  æƒé‡åˆå§‹åŒ–        â”‚ ä¸¤è€…        â”‚ æ­£äº¤åˆå§‹åŒ–ä¿æŒæ¢¯åº¦å¹…åº¦    â”‚
â”‚  LayerNorm         â”‚ ä¸¤è€…        â”‚ å½’ä¸€åŒ–éšè—çŠ¶æ€           â”‚
â”‚  æ®‹å·®è¿æ¥          â”‚ æ¢¯åº¦æ¶ˆå¤±    â”‚ æä¾›æ¢¯åº¦ç›´é€šè·¯å¾„         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# æ¢¯åº¦è£å‰ªç¤ºä¾‹
print("\næ¢¯åº¦è£å‰ªç¤ºä¾‹:")


def train_with_gradient_clipping():
    """ä½¿ç”¨æ¢¯åº¦è£å‰ªè®­ç»ƒ"""
    model = nn.RNN(10, 50, batch_first=True)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    x = torch.randn(1, 100, 10)
    h0 = torch.zeros(1, 1, 50)

    output, _ = model(x, h0)
    loss = output.sum()

    optimizer.zero_grad()
    loss.backward()

    # è£å‰ªå‰
    total_norm_before = 0
    for p in model.parameters():
        if p.grad is not None:
            total_norm_before += p.grad.data.norm(2).item() ** 2
    total_norm_before = total_norm_before**0.5

    # æ¢¯åº¦è£å‰ª
    max_norm = 1.0
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)

    # è£å‰ªå
    total_norm_after = 0
    for p in model.parameters():
        if p.grad is not None:
            total_norm_after += p.grad.data.norm(2).item() ** 2
    total_norm_after = total_norm_after**0.5

    print(f"  è£å‰ªå‰æ¢¯åº¦èŒƒæ•°: {total_norm_before:.4f}")
    print(f"  è£å‰ªåæ¢¯åº¦èŒƒæ•°: {total_norm_after:.4f}")
    print(f"  æœ€å¤§å…è®¸èŒƒæ•°: {max_norm}")


train_with_gradient_clipping()


# =============================================================================
# 6. LSTM å¦‚ä½•è§£å†³æ¢¯åº¦æ¶ˆå¤±
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 6. LSTM å¦‚ä½•è§£å†³æ¢¯åº¦æ¶ˆå¤±")
print("-" * 60)

print("""
LSTM çš„æ¢¯åº¦ä¼ æ’­:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  ç»†èƒçŠ¶æ€æ›´æ–°: Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ                     â”‚
â”‚                                                              â”‚
â”‚  æ¢¯åº¦: âˆ‚Câ‚œ/âˆ‚Câ‚œâ‚‹â‚ = fâ‚œ  (ä»…ä»…æ˜¯é—å¿˜é—¨çš„å€¼ï¼)                  â”‚
â”‚                                                              â”‚
â”‚  å¯¹æ¯”:                                                        â”‚
â”‚    RNN:  âˆ‚hâ‚œ/âˆ‚hâ‚œâ‚‹â‚ = Wâ‚•â‚•áµ€ Â· diag(tanh'(z))  (çŸ©é˜µä¹˜æ³•)       â”‚
â”‚    LSTM: âˆ‚Câ‚œ/âˆ‚Câ‚œâ‚‹â‚ = fâ‚œ                      (æ ‡é‡ä¹˜æ³•)       â”‚
â”‚                                                              â”‚
â”‚  å½“ fâ‚œ â‰ˆ 1 æ—¶ï¼Œæ¢¯åº¦å‡ ä¹æ— æŸä¼ æ’­ï¼                            â”‚
â”‚  è¿™å°±æ˜¯"ä¿¡æ¯é«˜é€Ÿå…¬è·¯"çš„å«ä¹‰                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# å¯¹æ¯” RNN å’Œ LSTM çš„æ¢¯åº¦
print("\nå¯¹æ¯” RNN å’Œ LSTM çš„æ¢¯åº¦ä¼ æ’­:")


def compare_rnn_lstm_gradients(seq_len=50):
    """å¯¹æ¯” RNN å’Œ LSTM çš„æ¢¯åº¦ä¼ æ’­"""
    input_size = 10
    hidden_size = 50

    results = {}

    for name, model_class in [("RNN", nn.RNN), ("LSTM", nn.LSTM)]:
        model = model_class(input_size, hidden_size, batch_first=True)
        x = torch.randn(1, seq_len, input_size, requires_grad=True)

        output, _ = model(x)
        loss = output[:, -1, :].sum()
        loss.backward()

        # è®¡ç®—å„æ—¶é—´æ­¥æ¢¯åº¦èŒƒæ•°
        grad_norms = [x.grad[0, t, :].norm().item() for t in range(seq_len)]
        results[name] = grad_norms

    return results


seq_len = 50
results = compare_rnn_lstm_gradients(seq_len)

# å¯è§†åŒ–å¯¹æ¯”
plt.figure(figsize=(10, 5))
time_steps = list(range(seq_len))
distance_from_end = [seq_len - t for t in time_steps]

plt.plot(distance_from_end, results["RNN"], "r-o", label="RNN", markersize=3)
plt.plot(distance_from_end, results["LSTM"], "b-o", label="LSTM", markersize=3)
plt.xlabel("è·ç¦»æœ€åæ—¶åˆ»çš„æ­¥æ•°")
plt.ylabel("æ¢¯åº¦èŒƒæ•° (å¯¹æ•°åˆ»åº¦)")
plt.title("RNN vs LSTM æ¢¯åº¦ä¼ æ’­å¯¹æ¯”")
plt.yscale("log")
plt.legend()
plt.grid(True, alpha=0.3)
plt.gca().invert_xaxis()
plt.tight_layout()
plt.savefig("outputs/rnn_vs_lstm_gradient.png", dpi=100)
plt.close()
print("RNN vs LSTM æ¢¯åº¦å¯¹æ¯”å›¾å·²ä¿å­˜: outputs/rnn_vs_lstm_gradient.png")

print(f"\n  RNN ç¬¬ä¸€æ—¶åˆ»æ¢¯åº¦èŒƒæ•°: {results['RNN'][0]:.6f}")
print(f"  LSTM ç¬¬ä¸€æ—¶åˆ»æ¢¯åº¦èŒƒæ•°: {results['LSTM'][0]:.6f}")
print(f"  æ¯”å€¼ (LSTM/RNN): {results['LSTM'][0] / results['RNN'][0]:.2f}x")


# =============================================================================
# 7. ç»ƒä¹ 
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“ ç»ƒä¹ é¢˜")
print("-" * 60)

print("""
1. ä¸ºä»€ä¹ˆ sigmoid å’Œ tanh å®¹æ˜“å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ
   ç­”ï¼šå®ƒä»¬çš„å¯¼æ•°å€¼åŸŸåœ¨ (0, 1) æˆ– (0, 0.25)ï¼Œå¤šæ¬¡ç›¸ä¹˜åè¶‹äº 0

2. æ¢¯åº¦è£å‰ªæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
   ç­”ï¼šå½“æ¢¯åº¦èŒƒæ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œç­‰æ¯”ä¾‹ç¼©å°æ‰€æœ‰æ¢¯åº¦

3. LSTM ä¸­é—å¿˜é—¨å€¼ä¸º 1 æ—¶ï¼Œæ¢¯åº¦ä¼ æ’­æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ
   ç­”ï¼šæ¢¯åº¦å¯ä»¥å®Œå…¨æ— æŸåœ°ä¼ é€’åˆ°å‰ä¸€æ—¶åˆ»

4. ä¸ºä»€ä¹ˆæ­£äº¤åˆå§‹åŒ–æœ‰åŠ©äºç¼“è§£æ¢¯åº¦é—®é¢˜ï¼Ÿ
   ç­”ï¼šæ­£äº¤çŸ©é˜µçš„ç‰¹å¾å€¼éƒ½æ˜¯ 1ï¼Œæ¢¯åº¦ä¸ä¼šæŒ‡æ•°çº§æ”¾å¤§æˆ–ç¼©å°
""")

print("\nâœ… ç¬¬3èŠ‚å®Œæˆï¼")
print("ä¸‹ä¸€èŠ‚ï¼š04-lstm.py - LSTM é—¨æ§æœºåˆ¶")
