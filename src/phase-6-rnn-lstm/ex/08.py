import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams["font.sans-serif"] = ["Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

print("=" * 60)
print("ç¬¬8èŠ‚: åŸºç¡€æ³¨æ„åŠ›æœºåˆ¶")
print("=" * 60)


# =============================================================================
# 3. æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 3. æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›")
print("-" * 60)


class BahdanauAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, decoder_hidden, encoder_outputs):
        batch_size = encoder_outputs.size(0)
        src_len = encoder_outputs.size(1)

        decoder_hidden = decoder_hidden.unsqueeze(1).expand(-1, src_len, -1)

        energy = torch.tanh(self.W1(decoder_hidden) + self.W2(encoder_outputs))

        scores = self.v(energy).squeeze(-1)
        attention_weights = F.softmax(scores, dim=-1)

        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

        return context, attention_weights


# æµ‹è¯•
attention = BahdanauAttention(hidden_dim=256)
decoder_h = torch.randn(4, 256)
encoder_out = torch.randn(4, 20, 256)

context, weights = attention(decoder_h, encoder_out)

print(f"Bahdanau æ³¨æ„åŠ›:")
print(f"  è§£ç å™¨çŠ¶æ€: {decoder_h.shape}")
print(f"  ç¼–ç å™¨è¾“å‡º: {encoder_out.shape}")
print(f"  ä¸Šä¸‹æ–‡å‘é‡: {context.shape}")
print(f"  æ³¨æ„åŠ›æƒé‡: {weights.shape}")
print(f"  æƒé‡å’Œ (åº”ä¸º1): {weights.sum(dim=-1).mean().item():.4f}")


# =============================================================================
# 4. Luong (Multiplicative) æ³¨æ„åŠ›
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 4. Luong (Multiplicative) æ³¨æ„åŠ›")
print("-" * 60)


class LuongAttention(nn.Module):
    """Luong (Multiplicative) æ³¨æ„åŠ› - æ›´ç®€å•é«˜æ•ˆ"""

    def __init__(self, hidden_dim, method="dot"):
        super().__init__()
        self.method = method

        if method == "general":
            self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)
        elif method == "concat":
            self.W = nn.Linear(hidden_dim * 2, hidden_dim, bias=False)
            self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: [batch, hidden_dim]
            encoder_outputs: [batch, src_len, hidden_dim]
        """
        if self.method == "dot":
            # ç›´æ¥ç‚¹ç§¯: sáµ€ Â· h
            scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(-1)).squeeze(
                -1
            )

        elif self.method == "general":
            # å¸¦æƒé‡çš„ç‚¹ç§¯: sáµ€ Â· W Â· h
            scores = torch.bmm(
                encoder_outputs, self.W(decoder_hidden).unsqueeze(-1)
            ).squeeze(-1)

        elif self.method == "concat":
            # æ‹¼æ¥: váµ€ Â· tanh(W Â· [s; h])
            src_len = encoder_outputs.size(1)
            decoder_hidden = decoder_hidden.unsqueeze(1).expand(-1, src_len, -1)
            concat = torch.cat([decoder_hidden, encoder_outputs], dim=-1)
            scores = self.v(torch.tanh(self.W(concat))).squeeze(-1)

        attention_weights = F.softmax(scores, dim=-1)
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

        return context, attention_weights


print("Luong æ³¨æ„åŠ›å˜ä½“:")
for method in ["dot", "general", "concat"]:
    attn = LuongAttention(256, method=method)
    ctx, wts = attn(decoder_h, encoder_out)
    params = sum(p.numel() for p in attn.parameters())
    print(f"  {method:8s}: ä¸Šä¸‹æ–‡ {ctx.shape}, å‚æ•°é‡ {params:,}")
