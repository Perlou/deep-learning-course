import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams["font.sans-serif"] = ["Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

print("=" * 60)
print("ç¬¬2èŠ‚: æ—¶é—´åå‘ä¼ æ’­ (BPTT)")
print("=" * 60)

print("\n" + "=" * 60)
print("ğŸ“Œ 3. æ‰‹åŠ¨å®ç° BPTT")
print("-" * 60)


class RNNWithBPTT:
    """å¸¦æœ‰ BPTT çš„ RNN å®ç°"""

    def __init__(self, input_size, hidden_size, output_size):
        self.hidden_size = hidden_size

        # åˆå§‹åŒ–æƒé‡
        scale = 0.01
        self.Wxh = np.random.randn(hidden_size, input_size) * scale
        self.Whh = np.random.randn(hidden_size, hidden_size) * scale
        self.Why = np.random.randn(output_size, hidden_size) * scale
        self.bh = np.zeros((hidden_size, 1))
        self.by = np.zeros((output_size, 1))

    def forward(self, inputs, targets, h_prev=None):
        if h_prev is None:
            h_prev = np.zeros((self.hidden_size, 1))

        self.inputs = inputs
        self.hs = {-1: h_prev.copy()}
        self.ys = {}
        self.ps = {}

        loss = 0
        for t in range(len(inputs)):
            x = inputs[t].reshape(-1, 1)

            self.hs[t] = np.tanh(self.Wxh @ x + self.Whh @ self.hs[t - 1] + self.bh)

            self.ys[t] = self.Why @ self.hs[t] + self.by
            self.ps[t] = np.exp(self.ys[t]) / np.sum(np.exp(self.ys[t]))

            loss += -np.log(self.ps[t][targets[t], 0])

        self.targets = targets
        return loss

    def backward(self):
        """BPTT åå‘ä¼ æ’­"""
        # åˆå§‹åŒ–æ¢¯åº¦
        dWxh = np.zeros_like(self.Wxh)
        dWhh = np.zeros_like(self.Whh)
        dWhy = np.zeros_like(self.Why)
        dbh = np.zeros_like(self.bh)
        dby = np.zeros_like(self.by)

        dh_next = np.zeros((self.hidden_size, 1))

        # åå‘éå†æ—¶é—´æ­¥
        for t in reversed(range(len(self.inputs))):
            # è¾“å‡ºå±‚æ¢¯åº¦
            dy = self.ps[t].copy()
            dy[self.targets[t]] -= 1  # softmax + cross-entropy çš„æ¢¯åº¦

            dWhy += dy @ self.hs[t].T
            dby += dy

            # éšè—å±‚æ¢¯åº¦ (å…³é”®ï¼šåŒ…å«æ¥è‡ªæœªæ¥çš„æ¢¯åº¦)
            dh = self.Why.T @ dy + dh_next

            # tanh æ±‚å¯¼: d(tanh(x))/dx = 1 - tanhÂ²(x)
            dh_raw = (1 - self.hs[t] ** 2) * dh

            # å‚æ•°æ¢¯åº¦
            x = self.inputs[t].reshape(-1, 1)
            dWxh += dh_raw @ x.T
            dWhh += dh_raw @ self.hs[t - 1].T
            dbh += dh_raw

            # ä¼ é€’åˆ°ä¸Šä¸€æ—¶åˆ»
            dh_next = self.Whh.T @ dh_raw

        # æ¢¯åº¦è£å‰ª
        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
            np.clip(dparam, -5, 5, out=dparam)

        return dWxh, dWhh, dWhy, dbh, dby


# æµ‹è¯• BPTT
print("\næµ‹è¯• BPTT å®ç°:")
input_size = 10
hidden_size = 20
output_size = 10
seq_len = 5

rnn = RNNWithBPTT(input_size, hidden_size, output_size)

# åˆ›å»º one-hot ç¼–ç çš„è¾“å…¥
inputs = [np.eye(input_size)[np.random.randint(0, input_size)] for _ in range(seq_len)]
targets = [np.random.randint(0, output_size) for _ in range(seq_len)]

# å‰å‘ + åå‘
loss = rnn.forward(inputs, targets)
grads = rnn.backward()

print(f"  æŸå¤±: {loss:.4f}")
print(f"  dWxh èŒƒæ•°: {np.linalg.norm(grads[0]):.4f}")
print(f"  dWhh èŒƒæ•°: {np.linalg.norm(grads[1]):.4f}")
print(f"  dWhy èŒƒæ•°: {np.linalg.norm(grads[2]):.4f}")

print("\n" + "=" * 60)
print("ğŸ“Œ 4. PyTorch è‡ªåŠ¨åå‘ä¼ æ’­")
print("-" * 60)


class SimpleRNNPyTorch(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out)
        return out


model = SimpleRNNPyTorch(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()

# åˆ›å»ºè¾“å…¥
x = torch.randn(1, seq_len, input_size, requires_grad=True)
targets_pt = torch.randint(0, output_size, (seq_len,))

# å‰å‘ä¼ æ’­
output = model(x).squeeze(0)
loss = criterion(output, targets_pt)

# åå‘ä¼ æ’­
loss.backward()

print(f"  PyTorch æŸå¤±: {loss.item():.4f}")
print(f"  è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
print(f"  è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x.grad.norm().item():.4f}")
