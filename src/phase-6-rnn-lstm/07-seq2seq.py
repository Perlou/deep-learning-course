"""
07-seq2seq.py - åºåˆ—åˆ°åºåˆ—æ¨¡å‹ (Seq2Seq)

æœ¬èŠ‚å­¦ä¹ :
1. Seq2Seq æ¶æ„
2. ç¼–ç å™¨-è§£ç å™¨ç»“æ„
3. æ‰‹åŠ¨å®ç° Seq2Seq
4. Teacher Forcing
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

print("=" * 60)
print("ç¬¬7èŠ‚: åºåˆ—åˆ°åºåˆ—æ¨¡å‹ (Seq2Seq)")
print("=" * 60)

# =============================================================================
# 1. Seq2Seq æ ¸å¿ƒæ€æƒ³
# =============================================================================
print("""
ğŸ“š Seq2Seq (Sequence to Sequence)

æ ¸å¿ƒæ¶æ„: ç¼–ç å™¨-è§£ç å™¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  è¾“å…¥åºåˆ— â†’ [ç¼–ç å™¨] â†’ ä¸Šä¸‹æ–‡å‘é‡ â†’ [è§£ç å™¨] â†’ è¾“å‡ºåºåˆ—       â”‚
â”‚                                                              â”‚
â”‚  "How are you" â†’ [Encoder] â†’ context â†’ [Decoder] â†’ "ä½ å¥½å—"  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¯¦ç»†ç»“æ„:
          ç¼–ç å™¨ (Encoder)              è§£ç å™¨ (Decoder)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  How   are   you     â”‚       â”‚  <SOS>  ä½    å¥½   å—  â”‚
  â”‚   â†“     â†“     â†“      â”‚       â”‚    â†“    â†“    â†“    â†“  â”‚
  â”‚  LSTMâ†’LSTMâ†’LSTM      â”‚  â†’â†’   â”‚  LSTMâ†’LSTMâ†’LSTMâ†’LSTM â”‚
  â”‚         â†“            â”‚  h,c  â”‚    â†“    â†“    â†“    â†“  â”‚
  â”‚     (h, c)           â”‚       â”‚   ä½    å¥½   å—  <EOS> â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         ç¼–ç éšè—çŠ¶æ€         åˆå§‹åŒ–è§£ç å™¨       é¢„æµ‹è¾“å‡º
""")


# =============================================================================
# 2. ç¼–ç å™¨å®ç°
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 2. ç¼–ç å™¨å®ç°")
print("-" * 60)


class Encoder(nn.Module):
    """Seq2Seq ç¼–ç å™¨"""

    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embed_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        """
        Args:
            src: [batch, src_len] æºåºåˆ—
        Returns:
            outputs: [batch, src_len, hidden_dim]
            (h, c): æœ€ç»ˆéšè—çŠ¶æ€
        """
        embedded = self.dropout(self.embedding(src))
        outputs, (h, c) = self.lstm(embedded)
        return outputs, (h, c)


# æµ‹è¯•ç¼–ç å™¨
encoder = Encoder(vocab_size=5000, embed_dim=256, hidden_dim=512)
src = torch.randint(1, 5000, (4, 20))  # 4ä¸ªæ ·æœ¬ï¼Œé•¿åº¦20
enc_outputs, (h, c) = encoder(src)

print(f"ç¼–ç å™¨:")
print(f"  è¾“å…¥ (æºåºåˆ—): {src.shape}")
print(f"  ç¼–ç è¾“å‡º: {enc_outputs.shape}")
print(f"  æœ€ç»ˆéšè—çŠ¶æ€: h={h.shape}, c={c.shape}")


# =============================================================================
# 3. è§£ç å™¨å®ç°
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 3. è§£ç å™¨å®ç°")
print("-" * 60)


class Decoder(nn.Module):
    """Seq2Seq è§£ç å™¨"""

    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embed_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
        )
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, trg, hidden):
        """
        å•æ­¥è§£ç 
        Args:
            trg: [batch, 1] å½“å‰è¾“å…¥ token
            hidden: (h, c) æ¥è‡ªç¼–ç å™¨æˆ–ä¸Šä¸€æ­¥
        Returns:
            output: [batch, vocab_size] é¢„æµ‹åˆ†å¸ƒ
            hidden: æ›´æ–°åçš„éšè—çŠ¶æ€
        """
        embedded = self.dropout(self.embedding(trg))
        output, hidden = self.lstm(embedded, hidden)
        prediction = self.fc(output.squeeze(1))
        return prediction, hidden


# æµ‹è¯•è§£ç å™¨
decoder = Decoder(vocab_size=3000, embed_dim=256, hidden_dim=512)
trg_token = torch.randint(1, 3000, (4, 1))  # å½“å‰ token
pred, new_hidden = decoder(trg_token, (h, c))

print(f"\nè§£ç å™¨:")
print(f"  è¾“å…¥ (å½“å‰ token): {trg_token.shape}")
print(f"  è¾“å‡º (è¯æ±‡è¡¨åˆ†å¸ƒ): {pred.shape}")


# =============================================================================
# 4. å®Œæ•´ Seq2Seq æ¨¡å‹
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 4. å®Œæ•´ Seq2Seq æ¨¡å‹")
print("-" * 60)


class Seq2Seq(nn.Module):
    """å®Œæ•´çš„ Seq2Seq æ¨¡å‹"""

    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        """
        Args:
            src: [batch, src_len] æºåºåˆ—
            trg: [batch, trg_len] ç›®æ ‡åºåˆ—
            teacher_forcing_ratio: ä½¿ç”¨çœŸå®æ ‡ç­¾çš„æ¦‚ç‡
        Returns:
            outputs: [batch, trg_len, vocab_size]
        """
        batch_size = src.size(0)
        trg_len = trg.size(1)
        trg_vocab_size = self.decoder.vocab_size

        # å­˜å‚¨è¾“å‡º
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)

        # ç¼–ç 
        _, hidden = self.encoder(src)

        # è§£ç å™¨çš„ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯ <SOS> token (å‡è®¾æ˜¯ trg çš„ç¬¬ä¸€ä¸ª)
        input_token = trg[:, 0:1]

        for t in range(1, trg_len):
            # è§£ç ä¸€æ­¥
            output, hidden = self.decoder(input_token, hidden)
            outputs[:, t] = output

            # Teacher Forcing: éšæœºå†³å®šä½¿ç”¨çœŸå®æ ‡ç­¾è¿˜æ˜¯é¢„æµ‹ç»“æœ
            teacher_force = np.random.random() < teacher_forcing_ratio
            top1 = output.argmax(1).unsqueeze(1)
            input_token = trg[:, t : t + 1] if teacher_force else top1

        return outputs


# åˆ›å»ºå®Œæ•´æ¨¡å‹
device = torch.device("cpu")
encoder = Encoder(vocab_size=5000, embed_dim=256, hidden_dim=512).to(device)
decoder = Decoder(vocab_size=3000, embed_dim=256, hidden_dim=512).to(device)
model = Seq2Seq(encoder, decoder, device)

# æµ‹è¯•
src = torch.randint(1, 5000, (4, 20)).to(device)
trg = torch.randint(1, 3000, (4, 15)).to(device)
outputs = model(src, trg)

print(f"\nSeq2Seq æ¨¡å‹:")
print(f"  æºåºåˆ—: {src.shape}")
print(f"  ç›®æ ‡åºåˆ—: {trg.shape}")
print(f"  è¾“å‡º: {outputs.shape}")
print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")


# =============================================================================
# 5. Teacher Forcing
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 5. Teacher Forcing")
print("-" * 60)

print("""
Teacher Forcing ç­–ç•¥:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  è®­ç»ƒæ—¶å†³å®šè§£ç å™¨çš„ä¸‹ä¸€ä¸ªè¾“å…¥:                               â”‚
â”‚                                                              â”‚
â”‚  Teacher Forcing (ä½¿ç”¨çœŸå®æ ‡ç­¾):                             â”‚
â”‚    ä¼˜ç‚¹: è®­ç»ƒæ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«                                â”‚
â”‚    ç¼ºç‚¹: æ¨ç†æ—¶å¯èƒ½é‡åˆ°ä»æœªè§è¿‡çš„é”™è¯¯                        â”‚
â”‚                                                              â”‚
â”‚  Free Running (ä½¿ç”¨è‡ªå·±çš„é¢„æµ‹):                              â”‚
â”‚    ä¼˜ç‚¹: æ›´æ¥è¿‘çœŸå®æ¨ç†åœºæ™¯                                  â”‚
â”‚    ç¼ºç‚¹: è®­ç»ƒåˆæœŸå¯èƒ½ä¸ç¨³å®š                                  â”‚
â”‚                                                              â”‚
â”‚  è§£å†³æ–¹æ¡ˆ: Scheduled Sampling                                â”‚
â”‚    å¼€å§‹æ—¶é«˜ teacher_forcing_ratio (å¦‚ 1.0)                   â”‚
â”‚    é€æ¸é™ä½åˆ°è¾ƒä½å€¼ (å¦‚ 0.5)                                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")


# =============================================================================
# 6. æ¨ç† (ç”Ÿæˆ)
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 6. æ¨ç† (ç”Ÿæˆ)")
print("-" * 60)


def greedy_decode(model, src, max_len, sos_idx, eos_idx):
    """è´ªå©ªè§£ç """
    model.eval()
    with torch.no_grad():
        # ç¼–ç 
        _, hidden = model.encoder(src)

        # åˆå§‹è¾“å…¥æ˜¯ <SOS>
        input_token = torch.tensor([[sos_idx]]).to(src.device)

        outputs = [sos_idx]

        for _ in range(max_len):
            output, hidden = model.decoder(input_token, hidden)
            top1 = output.argmax(1).item()
            outputs.append(top1)

            if top1 == eos_idx:
                break

            input_token = torch.tensor([[top1]]).to(src.device)

        return outputs


# æ¨¡æ‹Ÿæ¨ç†
print("è´ªå©ªè§£ç ç¤ºä¾‹:")
src_single = torch.randint(1, 5000, (1, 10)).to(device)
result = greedy_decode(model, src_single, max_len=20, sos_idx=1, eos_idx=2)
print(f"  è¾“å…¥é•¿åº¦: {src_single.size(1)}")
print(f"  è¾“å‡ºé•¿åº¦: {len(result)}")
print(f"  è¾“å‡ºåºåˆ—: {result[:10]}...")


# =============================================================================
# 7. Seq2Seq çš„å±€é™æ€§
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 7. Seq2Seq çš„å±€é™æ€§")
print("-" * 60)

print("""
åŸºç¡€ Seq2Seq çš„é—®é¢˜:

1. ä¿¡æ¯ç“¶é¢ˆ
   æ‰€æœ‰è¾“å…¥ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šå¤§å°çš„å‘é‡
   é•¿åºåˆ—ä¿¡æ¯ä¸¢å¤±ä¸¥é‡

2. æ¢¯åº¦é—®é¢˜
   è™½ç„¶ LSTM ç¼“è§£äº†ï¼Œä½†è¶…é•¿åºåˆ—ä»æœ‰é—®é¢˜

3. å¯¹é½é—®é¢˜
   ä¸çŸ¥é“è¾“å‡ºçš„å“ªä¸ªè¯å¯¹åº”è¾“å…¥çš„å“ªä¸ªè¯

è§£å†³æ–¹æ¡ˆ: æ³¨æ„åŠ›æœºåˆ¶ (ä¸‹ä¸€èŠ‚)
  â€¢ ä¸ä¾èµ–å•ä¸€ä¸Šä¸‹æ–‡å‘é‡
  â€¢ æ¯ä¸ªè¾“å‡ºæ­¥éª¤å…³æ³¨ä¸åŒçš„è¾“å…¥ä½ç½®
  â€¢ å¯è§†åŒ–å¯¹é½å…³ç³»
""")


# =============================================================================
# 8. ç»ƒä¹ 
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“ ç»ƒä¹ é¢˜")
print("-" * 60)

print("""
1. Seq2Seq ä¸­ç¼–ç å™¨çš„æœ€ç»ˆè¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ
   ç­”ï¼šæœ€ç»ˆçš„éšè—çŠ¶æ€ (h, c)ï¼Œç”¨äºåˆå§‹åŒ–è§£ç å™¨

2. Teacher Forcing çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
   ç­”ï¼šç”¨çœŸå®æ ‡ç­¾ä½œä¸ºä¸‹ä¸€æ­¥è¾“å…¥ï¼ŒåŠ é€Ÿè®­ç»ƒ

3. ä¸ºä»€ä¹ˆéœ€è¦ <SOS> å’Œ <EOS> æ ‡è®°ï¼Ÿ
   ç­”ï¼š<SOS> æ ‡è®°è§£ç å¼€å§‹ï¼Œ<EOS> æ ‡è®°è§£ç ç»“æŸ

4. åŸºç¡€ Seq2Seq çš„ä¿¡æ¯ç“¶é¢ˆé—®é¢˜å¦‚ä½•è§£å†³ï¼Ÿ
   ç­”ï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©è§£ç å™¨è®¿é—®æ‰€æœ‰ç¼–ç è¾“å‡º
""")

print("\nâœ… ç¬¬7èŠ‚å®Œæˆï¼")
print("ä¸‹ä¸€èŠ‚ï¼š08-attention-basic.py - åŸºç¡€æ³¨æ„åŠ›æœºåˆ¶")
