"""
08-attention-basic.py - åŸºç¡€æ³¨æ„åŠ›æœºåˆ¶

æœ¬èŠ‚å­¦ä¹ :
1. æ³¨æ„åŠ›æœºåˆ¶çš„ç›´è§‰
2. æ³¨æ„åŠ›è®¡ç®—å…¬å¼
3. æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›
4. Seq2Seq + Attention
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams["font.sans-serif"] = ["Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

print("=" * 60)
print("ç¬¬8èŠ‚: åŸºç¡€æ³¨æ„åŠ›æœºåˆ¶")
print("=" * 60)

# =============================================================================
# 1. æ³¨æ„åŠ›æœºåˆ¶çš„ç›´è§‰
# =============================================================================
print("""
ğŸ“š æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)

æ ¸å¿ƒæ€æƒ³: è§£ç æ—¶åŠ¨æ€å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  ç¿»è¯‘ "I love you" â†’ "æˆ‘çˆ±ä½ "                                â”‚
â”‚                                                              â”‚
â”‚  ç”Ÿæˆ "æˆ‘" æ—¶:  å…³æ³¨ "I"      (æƒé‡é«˜)                       â”‚
â”‚  ç”Ÿæˆ "çˆ±" æ—¶:  å…³æ³¨ "love"   (æƒé‡é«˜)                       â”‚
â”‚  ç”Ÿæˆ "ä½ " æ—¶:  å…³æ³¨ "you"    (æƒé‡é«˜)                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ³¨æ„åŠ› vs åŸºç¡€ Seq2Seq:
  
  åŸºç¡€ Seq2Seq:
    æ‰€æœ‰ä¿¡æ¯ â†’ å‹ç¼©æˆå›ºå®šå‘é‡ â†’ è§£ç  (ä¿¡æ¯ç“¶é¢ˆ)
  
  å¸¦æ³¨æ„åŠ›:
    æ¯ä¸ªè§£ç æ­¥éª¤ â†’ æŸ¥çœ‹æ‰€æœ‰ç¼–ç è¾“å‡º â†’ åŠ æƒæ±‚å’Œ (åŠ¨æ€å…³æ³¨)
""")


# =============================================================================
# 2. æ³¨æ„åŠ›è®¡ç®—å…¬å¼
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 2. æ³¨æ„åŠ›è®¡ç®—å…¬å¼")
print("-" * 60)

print("""
æ ‡å‡†æ³¨æ„åŠ›è®¡ç®— (Bahdanau Attention):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (energy):                                 â”‚
â”‚     eâ‚œáµ¢ = attention_score(sâ‚œâ‚‹â‚, háµ¢)                         â”‚
â”‚     é€šå¸¸: eâ‚œáµ¢ = váµ€ Â· tanh(Wâ‚Â·sâ‚œâ‚‹â‚ + Wâ‚‚Â·háµ¢)                  â”‚
â”‚                                                              â”‚
â”‚  2. è®¡ç®—æ³¨æ„åŠ›æƒé‡ (softmax):                                â”‚
â”‚     Î±â‚œáµ¢ = softmax(eâ‚œáµ¢) = exp(eâ‚œáµ¢) / Î£â±¼ exp(eâ‚œâ±¼)             â”‚
â”‚                                                              â”‚
â”‚  3. è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ (åŠ æƒæ±‚å’Œ):                               â”‚
â”‚     câ‚œ = Î£áµ¢ Î±â‚œáµ¢ Â· háµ¢                                         â”‚
â”‚                                                              â”‚
â”‚  ç¬¦å·:                                                        â”‚
â”‚    sâ‚œâ‚‹â‚: è§£ç å™¨ä¸Šä¸€æ—¶åˆ»éšè—çŠ¶æ€ (Query)                      â”‚
â”‚    háµ¢:   ç¼–ç å™¨ç¬¬ i æ—¶åˆ»è¾“å‡º (Key & Value)                   â”‚
â”‚    Î±â‚œáµ¢:  æ³¨æ„åŠ›æƒé‡ (å…³æ³¨ç¨‹åº¦)                               â”‚
â”‚    câ‚œ:   ä¸Šä¸‹æ–‡å‘é‡ (åŠ æƒåçš„è¾“å…¥ä¿¡æ¯)                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")


# =============================================================================
# 3. æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 3. æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›")
print("-" * 60)


class BahdanauAttention(nn.Module):
    """Bahdanau (Additive) æ³¨æ„åŠ›"""

    def __init__(self, hidden_dim):
        super().__init__()
        self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)  # å¤„ç†è§£ç å™¨çŠ¶æ€
        self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)  # å¤„ç†ç¼–ç å™¨è¾“å‡º
        self.v = nn.Linear(hidden_dim, 1, bias=False)  # è¾“å‡ºåˆ†æ•°

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: [batch, hidden_dim] è§£ç å™¨å½“å‰çŠ¶æ€
            encoder_outputs: [batch, src_len, hidden_dim] ç¼–ç å™¨æ‰€æœ‰è¾“å‡º
        Returns:
            context: [batch, hidden_dim] ä¸Šä¸‹æ–‡å‘é‡
            attention_weights: [batch, src_len] æ³¨æ„åŠ›æƒé‡
        """
        batch_size = encoder_outputs.size(0)
        src_len = encoder_outputs.size(1)

        # æ‰©å±•è§£ç å™¨çŠ¶æ€ä»¥åŒ¹é…ç¼–ç å™¨è¾“å‡ºçš„ç»´åº¦
        decoder_hidden = decoder_hidden.unsqueeze(1).expand(-1, src_len, -1)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # energy: [batch, src_len, hidden_dim]
        energy = torch.tanh(self.W1(decoder_hidden) + self.W2(encoder_outputs))

        # åˆ†æ•°: [batch, src_len]
        scores = self.v(energy).squeeze(-1)

        # æ³¨æ„åŠ›æƒé‡: [batch, src_len]
        attention_weights = F.softmax(scores, dim=-1)

        # ä¸Šä¸‹æ–‡å‘é‡: [batch, hidden_dim]
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

        return context, attention_weights


# æµ‹è¯•
attention = BahdanauAttention(hidden_dim=256)
decoder_h = torch.randn(4, 256)
encoder_out = torch.randn(4, 20, 256)

context, weights = attention(decoder_h, encoder_out)

print(f"Bahdanau æ³¨æ„åŠ›:")
print(f"  è§£ç å™¨çŠ¶æ€: {decoder_h.shape}")
print(f"  ç¼–ç å™¨è¾“å‡º: {encoder_out.shape}")
print(f"  ä¸Šä¸‹æ–‡å‘é‡: {context.shape}")
print(f"  æ³¨æ„åŠ›æƒé‡: {weights.shape}")
print(f"  æƒé‡å’Œ (åº”ä¸º1): {weights.sum(dim=-1).mean().item():.4f}")


# =============================================================================
# 4. Luong (Multiplicative) æ³¨æ„åŠ›
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 4. Luong (Multiplicative) æ³¨æ„åŠ›")
print("-" * 60)


class LuongAttention(nn.Module):
    """Luong (Multiplicative) æ³¨æ„åŠ› - æ›´ç®€å•é«˜æ•ˆ"""

    def __init__(self, hidden_dim, method="dot"):
        super().__init__()
        self.method = method

        if method == "general":
            self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)
        elif method == "concat":
            self.W = nn.Linear(hidden_dim * 2, hidden_dim, bias=False)
            self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: [batch, hidden_dim]
            encoder_outputs: [batch, src_len, hidden_dim]
        """
        if self.method == "dot":
            # ç›´æ¥ç‚¹ç§¯: sáµ€ Â· h
            scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(-1)).squeeze(
                -1
            )

        elif self.method == "general":
            # å¸¦æƒé‡çš„ç‚¹ç§¯: sáµ€ Â· W Â· h
            scores = torch.bmm(
                encoder_outputs, self.W(decoder_hidden).unsqueeze(-1)
            ).squeeze(-1)

        elif self.method == "concat":
            # æ‹¼æ¥: váµ€ Â· tanh(W Â· [s; h])
            src_len = encoder_outputs.size(1)
            decoder_hidden = decoder_hidden.unsqueeze(1).expand(-1, src_len, -1)
            concat = torch.cat([decoder_hidden, encoder_outputs], dim=-1)
            scores = self.v(torch.tanh(self.W(concat))).squeeze(-1)

        attention_weights = F.softmax(scores, dim=-1)
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

        return context, attention_weights


print("Luong æ³¨æ„åŠ›å˜ä½“:")
for method in ["dot", "general", "concat"]:
    attn = LuongAttention(256, method=method)
    ctx, wts = attn(decoder_h, encoder_out)
    params = sum(p.numel() for p in attn.parameters())
    print(f"  {method:8s}: ä¸Šä¸‹æ–‡ {ctx.shape}, å‚æ•°é‡ {params:,}")


# =============================================================================
# 5. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 5. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡")
print("-" * 60)

# æ¨¡æ‹Ÿä¸€ä¸ªæ³¨æ„åŠ›çŸ©é˜µ
np.random.seed(42)
src_words = ["I", "love", "machine", "learning", "<EOS>"]
trg_words = ["<SOS>", "æˆ‘", "çˆ±", "æœºå™¨å­¦ä¹ ", "<EOS>"]

# åˆ›å»ºä¸€ä¸ªæœ‰æ„ä¹‰çš„æ³¨æ„åŠ›çŸ©é˜µ
attention_matrix = np.array(
    [
        [0.9, 0.05, 0.02, 0.02, 0.01],  # "æˆ‘" å…³æ³¨ "I"
        [0.05, 0.85, 0.05, 0.03, 0.02],  # "çˆ±" å…³æ³¨ "love"
        [0.02, 0.03, 0.45, 0.48, 0.02],  # "æœºå™¨å­¦ä¹ " å…³æ³¨ "machine" + "learning"
        [0.01, 0.02, 0.02, 0.05, 0.90],  # "<EOS>" å…³æ³¨ "<EOS>"
    ]
)

plt.figure(figsize=(8, 6))
plt.imshow(attention_matrix, cmap="Blues", aspect="auto")
plt.colorbar(label="æ³¨æ„åŠ›æƒé‡")
plt.xticks(range(len(src_words)), src_words, fontsize=12)
plt.yticks(range(len(trg_words) - 1), trg_words[1:], fontsize=12)
plt.xlabel("æºåºåˆ— (è‹±æ–‡)")
plt.ylabel("ç›®æ ‡åºåˆ— (ä¸­æ–‡)")
plt.title("æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾")

# æ·»åŠ æ•°å€¼æ ‡æ³¨
for i in range(attention_matrix.shape[0]):
    for j in range(attention_matrix.shape[1]):
        plt.text(
            j,
            i,
            f"{attention_matrix[i, j]:.2f}",
            ha="center",
            va="center",
            color="white" if attention_matrix[i, j] > 0.5 else "black",
        )

plt.tight_layout()
plt.savefig("outputs/attention_weights.png", dpi=100)
plt.close()
print("æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾å·²ä¿å­˜: outputs/attention_weights.png")


# =============================================================================
# 6. Seq2Seq + Attention
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 6. Seq2Seq with Attention")
print("-" * 60)


class AttentionDecoder(nn.Module):
    """å¸¦æ³¨æ„åŠ›çš„è§£ç å™¨"""

    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.attention = BahdanauAttention(hidden_dim)
        # LSTM è¾“å…¥ = embedding + context
        self.lstm = nn.LSTM(embed_dim + hidden_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim * 2, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_token, hidden, encoder_outputs):
        """
        Args:
            input_token: [batch, 1]
            hidden: (h, c)
            encoder_outputs: [batch, src_len, hidden_dim]
        """
        # åµŒå…¥: [batch, 1, embed_dim]
        embedded = self.dropout(self.embedding(input_token))

        # æ³¨æ„åŠ›: [batch, hidden_dim]
        h = hidden[0].squeeze(0)  # [batch, hidden_dim]
        context, attention_weights = self.attention(h, encoder_outputs)

        # æ‹¼æ¥åµŒå…¥å’Œä¸Šä¸‹æ–‡ä½œä¸º LSTM è¾“å…¥
        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=-1)

        # LSTM
        output, hidden = self.lstm(lstm_input, hidden)

        # é¢„æµ‹: æ‹¼æ¥ LSTM è¾“å‡ºå’Œä¸Šä¸‹æ–‡
        output = torch.cat([output.squeeze(1), context], dim=-1)
        prediction = self.fc(output)

        return prediction, hidden, attention_weights


# æµ‹è¯•
attn_decoder = AttentionDecoder(vocab_size=3000, embed_dim=256, hidden_dim=512)
input_token = torch.randint(1, 3000, (4, 1))
h = torch.randn(1, 4, 512)
c = torch.randn(1, 4, 512)
encoder_outputs = torch.randn(4, 20, 512)

pred, new_hidden, attn_weights = attn_decoder(input_token, (h, c), encoder_outputs)

print(f"å¸¦æ³¨æ„åŠ›çš„è§£ç å™¨:")
print(f"  é¢„æµ‹è¾“å‡º: {pred.shape}")
print(f"  æ³¨æ„åŠ›æƒé‡: {attn_weights.shape}")
print(f"  å‚æ•°é‡: {sum(p.numel() for p in attn_decoder.parameters()):,}")


# =============================================================================
# 7. æ³¨æ„åŠ›çš„å¥½å¤„
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Œ 7. æ³¨æ„åŠ›çš„å¥½å¤„")
print("-" * 60)

print("""
æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ¿:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  1. è§£å†³ä¿¡æ¯ç“¶é¢ˆ                                             â”‚
â”‚     ä¸å†å‹ç¼©åˆ°å›ºå®šå‘é‡ï¼Œå¯è®¿é—®æ‰€æœ‰ç¼–ç è¾“å‡º                   â”‚
â”‚                                                              â”‚
â”‚  2. ç¼“è§£æ¢¯åº¦é—®é¢˜                                             â”‚
â”‚     æ³¨æ„åŠ›æä¾›äº†æ›´çŸ­çš„æ¢¯åº¦è·¯å¾„                               â”‚
â”‚                                                              â”‚
â”‚  3. å¯è§£é‡Šæ€§                                                 â”‚
â”‚     æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼Œç†è§£æ¨¡å‹å…³æ³¨ä»€ä¹ˆ                       â”‚
â”‚                                                              â”‚
â”‚  4. å¤„ç†é•¿åºåˆ—                                               â”‚
â”‚     æ¯ä¸ªè¾“å‡ºæ­¥éª¤å¯ä»¥ç›´æ¥å…³æ³¨ä»»æ„è¾“å…¥ä½ç½®                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä»æ³¨æ„åŠ›åˆ° Transformer:
  â€¢ Self-Attention: Query, Key, Value éƒ½æ¥è‡ªåŒä¸€åºåˆ—
  â€¢ Multi-Head Attention: å¤šä¸ªæ³¨æ„åŠ›å¤´å¹¶è¡Œ
  â€¢ Transformer: å®Œå…¨åŸºäºæ³¨æ„åŠ›ï¼Œæ²¡æœ‰ RNN
""")


# =============================================================================
# 8. ç»ƒä¹ 
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“ ç»ƒä¹ é¢˜")
print("-" * 60)

print("""
1. æ³¨æ„åŠ›æƒé‡ Î± ä»£è¡¨ä»€ä¹ˆï¼Ÿ
   ç­”ï¼šè§£ç å½“å‰è¯æ—¶ï¼Œå¯¹æ¯ä¸ªè¾“å…¥è¯çš„å…³æ³¨ç¨‹åº¦

2. Bahdanau å’Œ Luong æ³¨æ„åŠ›çš„ä¸»è¦åŒºåˆ«ï¼Ÿ
   ç­”ï¼šBahdanau æ˜¯åŠ æ³•æ³¨æ„åŠ›ï¼ŒLuong æ˜¯ä¹˜æ³•æ³¨æ„åŠ›ï¼›
       Bahdanau è®¡ç®—æ›´å¤æ‚ä½†æ›´çµæ´»

3. ä¸Šä¸‹æ–‡å‘é‡ c æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ
   ç­”ï¼šç¼–ç å™¨è¾“å‡ºçš„åŠ æƒæ±‚å’Œï¼Œæƒé‡æ˜¯æ³¨æ„åŠ›åˆ†æ•°

4. ä¸ºä»€ä¹ˆæ³¨æ„åŠ›èƒ½ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ
   ç­”ï¼šæ³¨æ„åŠ›æä¾›äº†ä»è¾“å‡ºåˆ°è¾“å…¥çš„ç›´æ¥è¿æ¥ï¼Œç¼©çŸ­æ¢¯åº¦è·¯å¾„
""")

print("\nâœ… ç¬¬8èŠ‚å®Œæˆï¼")
print("ä¸‹ä¸€èŠ‚ï¼š09-time-series.py - æ—¶é—´åºåˆ—é¢„æµ‹")
