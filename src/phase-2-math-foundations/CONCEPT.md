# æ·±åº¦å­¦ä¹ æ•°å­¦åŸºç¡€å®Œå…¨æŒ‡å—

> ğŸ“š ä»é›¶å¼€å§‹ï¼Œç³»ç»ŸæŒæ¡æ·±åº¦å­¦ä¹ æ‰€éœ€çš„å…¨éƒ¨æ•°å­¦çŸ¥è¯†

---

## ç›®å½•

1. [å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦æ•°å­¦](#1-å¼•è¨€ä¸ºä»€ä¹ˆéœ€è¦æ•°å­¦)
2. [çº¿æ€§ä»£æ•°](#2-çº¿æ€§ä»£æ•°)
3. [å¾®ç§¯åˆ†](#3-å¾®ç§¯åˆ†)
4. [æ¦‚ç‡è®ºä¸ç»Ÿè®¡](#4-æ¦‚ç‡è®ºä¸ç»Ÿè®¡)
5. [ä¼˜åŒ–ç†è®º](#5-ä¼˜åŒ–ç†è®º)
6. [ä¿¡æ¯è®ºåŸºç¡€](#6-ä¿¡æ¯è®ºåŸºç¡€)
7. [å®æˆ˜åº”ç”¨](#7-å®æˆ˜åº”ç”¨)

---

## 1. å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦æ•°å­¦

### 1.1 æ•°å­¦åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åœ°ä½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ·±åº¦å­¦ä¹ æŠ€æœ¯æ ˆ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åº”ç”¨å±‚    â”‚  å›¾åƒè¯†åˆ«ã€NLPã€æ¨èç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶...           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ¡†æ¶å±‚    â”‚  PyTorch, TensorFlow, JAX...                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç®—æ³•å±‚    â”‚  CNN, RNN, Transformer, GAN...                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•°å­¦å±‚    â”‚  çº¿æ€§ä»£æ•° + å¾®ç§¯åˆ† + æ¦‚ç‡è®º + ä¼˜åŒ–ç†è®º          â”‚  â† æœ¬æ–‡é‡ç‚¹
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å››å¤§æ•°å­¦æ”¯æŸ±

| æ•°å­¦é¢†åŸŸ     | åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ä½œç”¨ | æ ¸å¿ƒæ¦‚å¿µ         |
| ------------ | ------------------ | ---------------- |
| **çº¿æ€§ä»£æ•°** | æ•°æ®è¡¨ç¤ºä¸å˜æ¢     | å‘é‡ã€çŸ©é˜µã€å¼ é‡ |
| **å¾®ç§¯åˆ†**   | æ¨¡å‹ä¼˜åŒ–ä¸è®­ç»ƒ     | æ¢¯åº¦ã€é“¾å¼æ³•åˆ™   |
| **æ¦‚ç‡è®º**   | ä¸ç¡®å®šæ€§å»ºæ¨¡       | åˆ†å¸ƒã€è´å¶æ–¯     |
| **ä¼˜åŒ–ç†è®º** | å‚æ•°å­¦ä¹            | æ¢¯åº¦ä¸‹é™ã€æ”¶æ•›æ€§ |

---

## 2. çº¿æ€§ä»£æ•°

### 2.1 æ ‡é‡ã€å‘é‡ã€çŸ©é˜µä¸å¼ é‡

#### 2.1.1 åŸºæœ¬å®šä¹‰

```
æ ‡é‡ (Scalar)     â†’  å•ä¸ªæ•°å€¼            â†’  x = 5
å‘é‡ (Vector)     â†’  ä¸€ç»´æ•°ç»„            â†’  v = [1, 2, 3]
çŸ©é˜µ (Matrix)     â†’  äºŒç»´æ•°ç»„            â†’  M = [[1,2], [3,4]]
å¼ é‡ (Tensor)     â†’  å¤šç»´æ•°ç»„            â†’  T[batch, channel, height, width]
```

**ç›´è§‚ç†è§£ï¼š**

```
æ ‡é‡ï¼šæ¸©åº¦ = 25Â°C

å‘é‡ï¼šä½ç½® = (x, y, z) = (3, 4, 5)
      â”Œâ”€â”€â”€â”
  v = â”‚ 3 â”‚  âˆˆ â„Â³
      â”‚ 4 â”‚
      â”‚ 5 â”‚
      â””â”€â”€â”€â”˜

çŸ©é˜µï¼šå›¾åƒçš„ç°åº¦å€¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  M = â”‚ 128  64  32 â”‚  âˆˆ â„Â²Ë£Â³
      â”‚ 255 128  64 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¼ é‡ï¼šå½©è‰²å›¾åƒæ‰¹æ¬¡
  T âˆˆ â„^(batch Ã— channel Ã— height Ã— width)
  T âˆˆ â„^(32 Ã— 3 Ã— 224 Ã— 224)
```

#### 2.1.2 å‘é‡çš„å‡ ä½•æ„ä¹‰

```
                    y
                    â†‘
                    â”‚     â€¢ v = (3, 4)
                    â”‚    /â”‚
                    â”‚   / â”‚
                    â”‚  /  â”‚ 4
                    â”‚ /   â”‚
                    â”‚/Î¸   â”‚
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”´â”€â”€â”€â†’ x
                    0     3

å‘é‡çš„æ€§è´¨ï¼š
â€¢ æ¨¡é•¿ (magnitude): â€–vâ€– = âˆš(3Â² + 4Â²) = 5
â€¢ æ–¹å‘ (direction): Î¸ = arctan(4/3) â‰ˆ 53.13Â°
```

### 2.2 å‘é‡è¿ç®—

#### 2.2.1 å‘é‡åŠ æ³•ä¸æ•°ä¹˜

```python
# å‘é‡åŠ æ³•ï¼šå¯¹åº”å…ƒç´ ç›¸åŠ 
a = [1, 2, 3]
b = [4, 5, 6]
a + b = [5, 7, 9]

# æ•°ä¹˜ï¼šæ¯ä¸ªå…ƒç´ ä¹˜ä»¥æ ‡é‡
c = 2
c * a = [2, 4, 6]
```

**å‡ ä½•æ„ä¹‰ï¼š**

```
å‘é‡åŠ æ³•ï¼ˆå¹³è¡Œå››è¾¹å½¢æ³•åˆ™ï¼‰:

           a + b
          â†—
         /
        /  b
       â†—â”€â”€â”€â”€â†’
      /
     a

æ•°ä¹˜ï¼ˆç¼©æ”¾ï¼‰:

    â”€â”€â†’ a
    â”€â”€â”€â”€â”€â”€â”€â”€â†’ 2a
    â†â”€â”€ -a
```

#### 2.2.2 ç‚¹ç§¯ (Dot Product / Inner Product)

**å®šä¹‰ï¼š**

$$\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = a_1b_1 + a_2b_2 + ... + a_nb_n$$

**å‡ ä½•æ„ä¹‰ï¼š**

$$\mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos\theta$$

```
              b
             â†—
            /
           / Î¸
    â”€â”€â”€â”€â”€â”€â€¢â”€â”€â”€â”€â”€â”€â”€â”€â†’ a

    aÂ·b > 0  â†’  Î¸ < 90Â°  (åŒå‘)
    aÂ·b = 0  â†’  Î¸ = 90Â°  (å‚ç›´)
    aÂ·b < 0  â†’  Î¸ > 90Â°  (åå‘)
```

**æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ï¼š**

```python
# 1. ç¥ç»ç½‘ç»œçš„çº¿æ€§å±‚
output = weights Â· input + bias

# 2. æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ç›¸ä¼¼åº¦è®¡ç®—
similarity = query Â· key

# 3. ä½™å¼¦ç›¸ä¼¼åº¦
cos_sim = (a Â· b) / (â€–aâ€– Ã— â€–bâ€–)
```

#### 2.2.3 å‘é‡èŒƒæ•° (Norm)

| èŒƒæ•°ç±»å‹ | å…¬å¼                              | å‡ ä½•æ„ä¹‰   |
| -------- | --------------------------------- | ---------- |
| L0 èŒƒæ•°  | $\|\|x\|\|\_0 = $ éé›¶å…ƒç´ ä¸ªæ•°    | ç¨€ç–åº¦     |
| L1 èŒƒæ•°  | $\|\|x\|\|_1 = \sum\|x_i\|$       | æ›¼å“ˆé¡¿è·ç¦» |
| L2 èŒƒæ•°  | $\|\|x\|\|_2 = \sqrt{\sum x_i^2}$ | æ¬§æ°è·ç¦»   |
| Lâˆ èŒƒæ•°  | $\|\|x\|\|_\infty = \max\|x_i\|$  | æœ€å¤§ç»å¯¹å€¼ |

```
L1 vs L2 èŒƒæ•°çš„ç­‰å€¼çº¿ï¼š

    L1 (è±å½¢)           L2 (åœ†å½¢)
        â—‡                  â—‹
       / \               /   \
      /   \             |     |
     â—‡     â—‡            |     |
      \   /             |     |
       \ /               \   /
        â—‡                  â—‹
```

### 2.3 çŸ©é˜µè¿ç®—

#### 2.3.1 çŸ©é˜µä¹˜æ³•

**å®šä¹‰ï¼šè‹¥ A âˆˆ â„^(mÃ—n), B âˆˆ â„^(nÃ—p), åˆ™ C = AB âˆˆ â„^(mÃ—p)**

$$C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}$$

```
çŸ©é˜µä¹˜æ³•ç¤ºæ„å›¾ï¼š

    A (mÃ—n)         B (nÃ—p)         C (mÃ—p)

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚â†’â†’â†’â†’â†’â†’â†’â”‚  Ã—  â”‚â†“      â”‚  =  â”‚  â€¢    â”‚
    â”‚       â”‚     â”‚â†“      â”‚     â”‚       â”‚
    â”‚       â”‚     â”‚â†“      â”‚     â”‚       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜

    ç¬¬iè¡Œ        Ã— ç¬¬jåˆ—        = C[i,j]
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
import numpy as np

A = np.array([[1, 2],
              [3, 4]])  # 2Ã—2

B = np.array([[5, 6],
              [7, 8]])  # 2Ã—2

C = A @ B  # æˆ– np.dot(A, B)
# C = [[1*5+2*7, 1*6+2*8],
#      [3*5+4*7, 3*6+4*8]]
# C = [[19, 22],
#      [43, 50]]
```

#### 2.3.2 é‡è¦çŸ©é˜µç±»å‹

```python
# 1. å•ä½çŸ©é˜µ (Identity Matrix)
I = [[1, 0, 0],
     [0, 1, 0],
     [0, 0, 1]]
# æ€§è´¨: AI = IA = A

# 2. å¯¹è§’çŸ©é˜µ (Diagonal Matrix)
D = [[d1, 0,  0 ],
     [0,  d2, 0 ],
     [0,  0,  d3]]

# 3. å¯¹ç§°çŸ©é˜µ (Symmetric Matrix)
S = [[1, 2, 3],
     [2, 4, 5],
     [3, 5, 6]]
# æ€§è´¨: S = S^T

# 4. æ­£äº¤çŸ©é˜µ (Orthogonal Matrix)
# æ€§è´¨: Q^T Q = QQ^T = I
# å³: Q^(-1) = Q^T
```

#### 2.3.3 çŸ©é˜µè½¬ç½®ä¸é€†çŸ©é˜µ

**è½¬ç½® (Transpose)ï¼š**

$$\mathbf{A}^T_{ij} = \mathbf{A}_{ji}$$

```
åŸçŸ©é˜µ A:           è½¬ç½® A^T:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  2  3 â”‚   â†’    â”‚ 1  4  â”‚
â”‚ 4  5  6 â”‚        â”‚ 2  5  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ 3  6  â”‚
   2Ã—3             â””â”€â”€â”€â”€â”€â”€â”€â”˜
                      3Ã—2
```

**é€†çŸ©é˜µ (Inverse)ï¼š**

$$\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$$

```python
import numpy as np

A = np.array([[4, 7],
              [2, 6]])

A_inv = np.linalg.inv(A)
# éªŒè¯: A @ A_inv â‰ˆ I
```

**æ·±åº¦å­¦ä¹ åº”ç”¨ï¼šæƒé‡çŸ©é˜µæ±‚é€†ç”¨äºæŸäº›æ­£åˆ™åŒ–æŠ€æœ¯**

### 2.4 ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡

#### 2.4.1 å®šä¹‰

å¯¹äºæ–¹é˜µ Aï¼Œå¦‚æœå­˜åœ¨éé›¶å‘é‡ v å’Œæ ‡é‡ Î»ï¼Œä½¿å¾—ï¼š

$$\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$$

åˆ™ Î» ç§°ä¸º**ç‰¹å¾å€¼**ï¼Œv ç§°ä¸ºå¯¹åº”çš„**ç‰¹å¾å‘é‡**ã€‚

**å‡ ä½•ç†è§£ï¼š**

```
ç‰¹å¾å‘é‡æ˜¯ç»è¿‡çŸ©é˜µå˜æ¢åæ–¹å‘ä¸å˜çš„å‘é‡
ç‰¹å¾å€¼è¡¨ç¤ºåœ¨è¯¥æ–¹å‘ä¸Šçš„ç¼©æ”¾å€æ•°

åŸå§‹å‘é‡                 å˜æ¢å
    â†‘ v                    â†‘ Î»v (åŒæ–¹å‘ï¼Œä»…ç¼©æ”¾)
    â”‚                      â”‚
    â”‚                      â”‚
â”€â”€â”€â”€â”¼â”€â”€â”€â”€            â”€â”€â”€â”€â”¼â”€â”€â”€â”€
```

#### 2.4.2 è®¡ç®—æ–¹æ³•

```python
import numpy as np

A = np.array([[4, -2],
              [1,  1]])

# è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
eigenvalues, eigenvectors = np.linalg.eig(A)

print("ç‰¹å¾å€¼:", eigenvalues)      # [3., 2.]
print("ç‰¹å¾å‘é‡:\n", eigenvectors)
```

#### 2.4.3 åº”ç”¨åœºæ™¯

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç‰¹å¾åˆ†è§£çš„åº”ç”¨                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PCAé™ç»´          â”‚ åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡å®šä¹‰ä¸»æˆåˆ†æ–¹å‘     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PageRank         â”‚ è½¬ç§»çŸ©é˜µçš„ä¸»ç‰¹å¾å‘é‡                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è°±èšç±»           â”‚ æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¥ç»ç½‘ç»œåˆå§‹åŒ–    â”‚ æƒé‡çŸ©é˜µçš„è°±åˆ†æ                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.5 çŸ©é˜µåˆ†è§£

#### 2.5.1 å¥‡å¼‚å€¼åˆ†è§£ (SVD)

ä»»æ„çŸ©é˜µ A âˆˆ â„^(mÃ—n) å¯åˆ†è§£ä¸ºï¼š

$$\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$$

```
      A           =      U      Ã—     Î£      Ã—    V^T
   (m Ã— n)           (m Ã— m)      (m Ã— n)      (n Ã— n)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚     â”‚       â”‚   â”‚Ïƒ1       â”‚   â”‚         â”‚
â”‚  åŸå§‹   â”‚  =  â”‚ å·¦å¥‡å¼‚ â”‚ Ã— â”‚  Ïƒ2     â”‚ Ã— â”‚ å³å¥‡å¼‚  â”‚
â”‚  çŸ©é˜µ   â”‚     â”‚ å‘é‡   â”‚   â”‚    Ïƒ3   â”‚   â”‚ å‘é‡    â”‚
â”‚         â”‚     â”‚       â”‚   â”‚      ...â”‚   â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
import numpy as np

A = np.array([[1, 2],
              [3, 4],
              [5, 6]])

U, S, Vt = np.linalg.svd(A)

# é‡æ„åŸçŸ©é˜µ
A_reconstructed = U @ np.diag(S) @ Vt[:2, :]
```

**åº”ç”¨ï¼š**

- å›¾åƒå‹ç¼©
- æ¨èç³»ç»Ÿ
- é™å™ª

#### 2.5.2 ä½ç§©è¿‘ä¼¼

ä¿ç•™å‰ k ä¸ªæœ€å¤§å¥‡å¼‚å€¼ï¼š

```
åŸå§‹å›¾åƒ (1000Ã—1000)           å‹ç¼©å (k=50)
    å­˜å‚¨: 1,000,000              å­˜å‚¨: 50Ã—(1000+1000+1) â‰ˆ 100,000

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚            â”‚               â”‚
    â”‚   é«˜æ¸…å›¾åƒ    â”‚    â”€â”€â†’     â”‚  è¿‘ä¼¼å›¾åƒ     â”‚
    â”‚               â”‚            â”‚   (90%å‹ç¼©)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.6 å¹¿æ’­æœºåˆ¶ (Broadcasting)

åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å¤„ç†ä¸åŒå½¢çŠ¶å¼ é‡çš„è¿ç®—è§„åˆ™ï¼š

```python
# è§„åˆ™ï¼šä»æœ€åä¸€ä¸ªç»´åº¦å¼€å§‹å¯¹é½ï¼Œç»´åº¦ä¸º1çš„å¯ä»¥æ‰©å±•

# ä¾‹1: å‘é‡ + æ ‡é‡
[1, 2, 3] + 10 = [11, 12, 13]

# ä¾‹2: çŸ©é˜µ + å‘é‡ (æŒ‰è¡Œå¹¿æ’­)
[[1, 2, 3],     [10, 20, 30]     [[11, 22, 33],
 [4, 5, 6]]  +                 =  [14, 25, 36]]

# ä¾‹3: åˆ—å‘é‡ + è¡Œå‘é‡
[[1],           [[10, 20, 30]]     [[11, 21, 31],
 [2],       +                    =  [12, 22, 32],
 [3]]                               [13, 23, 33]]
```

```
å¹¿æ’­è§„åˆ™å›¾è§£ï¼š

    Shape (3, 1)     Shape (1, 4)      Shape (3, 4)

    â”Œâ”€â”€â”€â”            â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚ 1 â”‚            â”‚ a â”‚ b â”‚ c â”‚ d â”‚
    â”œâ”€â”€â”€â”¤     +      â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
    â”‚ 2 â”‚                   â†“
    â”œâ”€â”€â”€â”¤            â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚ 3 â”‚            â”‚1+aâ”‚1+bâ”‚1+câ”‚1+dâ”‚
    â””â”€â”€â”€â”˜            â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
      â†“              â”‚2+aâ”‚2+bâ”‚2+câ”‚2+dâ”‚
                     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
                     â”‚3+aâ”‚3+bâ”‚3+câ”‚3+dâ”‚
                     â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

---

## 3. å¾®ç§¯åˆ†

### 3.1 å¯¼æ•°çš„æœ¬è´¨

#### 3.1.1 å¯¼æ•°å®šä¹‰

$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$

**å‡ ä½•æ„ä¹‰ï¼šå‡½æ•°åœ¨æŸç‚¹çš„åˆ‡çº¿æ–œç‡**

```
        y
        â”‚
        â”‚        â•± â† åˆ‡çº¿ (æ–œç‡ = f'(xâ‚€))
        â”‚      â•±
        â”‚    â—â•±  â† f(xâ‚€)
        â”‚  â•±
        â”‚â•±    æ›²çº¿ f(x)
        â”‚
    â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
        â”‚    xâ‚€
```

#### 3.1.2 å¸¸ç”¨å¯¼æ•°å…¬å¼

| å‡½æ•° f(x)                      | å¯¼æ•° f'(x)                                        | æ·±åº¦å­¦ä¹ åº”ç”¨ |
| ------------------------------ | ------------------------------------------------- | ------------ |
| $x^n$                          | $nx^{n-1}$                                        | å¤šé¡¹å¼æ‹Ÿåˆ   |
| $e^x$                          | $e^x$                                             | Softmax      |
| $\ln(x)$                       | $1/x$                                             | äº¤å‰ç†µæŸå¤±   |
| $\sin(x)$                      | $\cos(x)$                                         | ä½ç½®ç¼–ç      |
| $\frac{1}{1+e^{-x}}$ (Sigmoid) | $\sigma(x)(1-\sigma(x))$                          | æ¿€æ´»å‡½æ•°     |
| $\max(0, x)$ (ReLU)            | $\begin{cases} 1 & x>0 \\ 0 & x\leq0 \end{cases}$ | æ¿€æ´»å‡½æ•°     |
| $\tanh(x)$                     | $1 - \tanh^2(x)$                                  | æ¿€æ´»å‡½æ•°     |

### 3.2 åå¯¼æ•°ä¸æ¢¯åº¦

#### 3.2.1 åå¯¼æ•°

å¯¹äºå¤šå…ƒå‡½æ•° $f(x_1, x_2, ..., x_n)$ï¼Œå¯¹æŸä¸€å˜é‡æ±‚å¯¼ï¼š

$$\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(..., x_i+h, ...) - f(..., x_i, ...)}{h}$$

**ç¤ºä¾‹ï¼š**

$$f(x, y) = x^2 + 3xy + y^2$$

$$\frac{\partial f}{\partial x} = 2x + 3y$$

$$\frac{\partial f}{\partial y} = 3x + 2y$$

#### 3.2.2 æ¢¯åº¦ (Gradient)

æ¢¯åº¦æ˜¯æ‰€æœ‰åå¯¼æ•°ç»„æˆçš„å‘é‡ï¼š

$$\nabla f = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}\right]$$

**å…³é”®æ€§è´¨ï¼šæ¢¯åº¦æŒ‡å‘å‡½æ•°å€¼å¢é•¿æœ€å¿«çš„æ–¹å‘**

```
ç­‰é«˜çº¿å›¾ä¸æ¢¯åº¦æ–¹å‘ï¼š

        y
        â”‚
    â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
        â”‚    â•­â”€â”€â”€â”€â”€â”€â•®
        â”‚  â•­â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â•®
        â”‚â•­â”€â”¼â”€â”¼â”€â”€â€¢â”€â”€â”€â”¼â”€â”¼â”€â•®
        â”‚â•°â”€â”¼â”€â”¼â”€â”€â†—â”€â”€â”€â”¼â”€â”¼â”€â•¯    â† æ¢¯åº¦æ–¹å‘ (å‚ç›´äºç­‰é«˜çº¿)
        â”‚  â•°â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â•¯
        â”‚    â•°â”€â”€â”€â”€â”€â”€â•¯

        æ¢¯åº¦ä¸‹é™ï¼šæ²¿ç€ -âˆ‡f æ–¹å‘ç§»åŠ¨
```

### 3.3 é“¾å¼æ³•åˆ™ (Chain Rule)

#### 3.3.1 åŸºæœ¬å½¢å¼

è‹¥ $y = f(g(x))$ï¼Œåˆ™ï¼š

$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx} = f'(g(x)) \cdot g'(x)$$

**ç¤ºä¾‹ï¼š**

$$y = (3x + 2)^5$$

ä»¤ $u = 3x + 2$ï¼Œåˆ™ $y = u^5$

$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} = 5u^4 \cdot 3 = 15(3x+2)^4$$

#### 3.3.2 å¤šå˜é‡é“¾å¼æ³•åˆ™

```
è®¡ç®—å›¾ï¼š

    xâ‚ â”€â”€â”€â”€â”€â”
            â†“
    xâ‚‚ â”€â”€â”€â†’ f â”€â”€â”€â†’ g â”€â”€â”€â†’ h â”€â”€â”€â†’ L (æŸå¤±)
            â†‘
    xâ‚ƒ â”€â”€â”€â”€â”€â”˜

åå‘ä¼ æ’­ï¼š

    âˆ‚L     âˆ‚L   âˆ‚h   âˆ‚g   âˆ‚f
   â”€â”€â”€ = â”€â”€â”€ Â· â”€â”€ Â· â”€â”€ Â· â”€â”€â”€
   âˆ‚xáµ¢    âˆ‚h   âˆ‚g   âˆ‚f   âˆ‚xáµ¢
```

### 3.4 åå‘ä¼ æ’­ç®—æ³•

#### 3.4.1 å‰å‘ä¼ æ’­

```
è¾“å…¥å±‚        éšè—å±‚           è¾“å‡ºå±‚          æŸå¤±
  x    â”€â”€â†’    h = Wx + b  â”€â”€â†’  y = Ïƒ(h)  â”€â”€â†’   L

  å…·ä½“ä¾‹å­ï¼š
  x = [1, 2]
  W = [[0.1, 0.2],
       [0.3, 0.4]]
  b = [0.1, 0.1]

  h = WÂ·x + b = [0.6, 1.2]
  y = sigmoid(h) = [0.65, 0.77]
```

#### 3.4.2 åå‘ä¼ æ’­è®¡ç®—å›¾

```
å‰å‘ä¼ æ’­ (Forward Pass):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†’

    â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”
    â”‚ x â”‚ â”€â”€â†’  â”‚ Ã— â”‚ â”€â”€â†’  â”‚ + â”‚ â”€â”€â†’  â”‚ Ïƒ â”‚ â”€â”€â†’  â”‚ L â”‚
    â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜
                 â†‘          â†‘
    â”Œâ”€â”€â”€â”        â”‚  â”Œâ”€â”€â”€â”   â”‚
    â”‚ W â”‚ â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ b â”‚ â”€â”€â”˜
    â””â”€â”€â”€â”˜           â””â”€â”€â”€â”˜

â†â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
åå‘ä¼ æ’­ (Backward Pass):

è®¡ç®—é¡ºåºï¼š
1. âˆ‚L/âˆ‚y        (æŸå¤±å¯¹è¾“å‡ºçš„å¯¼æ•°)
2. âˆ‚L/âˆ‚h = âˆ‚L/âˆ‚y Â· Ïƒ'(h)    (é€šè¿‡æ¿€æ´»å‡½æ•°)
3. âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚h Â· x^T       (æƒé‡æ¢¯åº¦)
4. âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚h             (åç½®æ¢¯åº¦)
5. âˆ‚L/âˆ‚x = W^T Â· âˆ‚L/âˆ‚h       (ä¼ é€’ç»™ä¸Šä¸€å±‚)
```

#### 3.4.3 ä»£ç å®ç°

```python
import numpy as np

class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        # åˆå§‹åŒ–æƒé‡
        self.W1 = np.random.randn(hidden_size, input_size) * 0.01
        self.b1 = np.zeros((hidden_size, 1))
        self.W2 = np.random.randn(output_size, hidden_size) * 0.01
        self.b2 = np.zeros((output_size, 1))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        s = self.sigmoid(x)
        return s * (1 - s)

    def forward(self, X):
        # å‰å‘ä¼ æ’­
        self.z1 = self.W1 @ X + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = self.W2 @ self.a1 + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2

    def backward(self, X, Y, learning_rate=0.01):
        m = X.shape[1]  # æ ·æœ¬æ•°

        # åå‘ä¼ æ’­
        dZ2 = self.a2 - Y  # è¾“å‡ºå±‚è¯¯å·®
        dW2 = (1/m) * dZ2 @ self.a1.T
        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)

        dZ1 = (self.W2.T @ dZ2) * self.sigmoid_derivative(self.z1)
        dW1 = (1/m) * dZ1 @ X.T
        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)

        # æ›´æ–°å‚æ•°
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
```

### 3.5 é›…å¯æ¯”çŸ©é˜µä¸æµ·æ£®çŸ©é˜µ

#### 3.5.1 é›…å¯æ¯”çŸ©é˜µ (Jacobian Matrix)

å¯¹äºå‘é‡å‡½æ•° $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ï¼š

$$
\mathbf{J} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}
$$

**åº”ç”¨ï¼šæ‰¹é‡è®¡ç®—æ¢¯åº¦ã€å˜æ¢çš„å±€éƒ¨çº¿æ€§è¿‘ä¼¼**

#### 3.5.2 æµ·æ£®çŸ©é˜µ (Hessian Matrix)

äºŒé˜¶åå¯¼æ•°çŸ©é˜µï¼š

$$
\mathbf{H} = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$

**åº”ç”¨ï¼š**

- åˆ¤æ–­æå€¼ç‚¹ç±»å‹
- ç‰›é¡¿æ³•ä¼˜åŒ–
- æ›²ç‡åˆ†æ

```
æµ·æ£®çŸ©é˜µä¸æå€¼åˆ¤æ–­ï¼š

H æ­£å®š  (æ‰€æœ‰ç‰¹å¾å€¼ > 0)  â†’  å±€éƒ¨æœ€å°å€¼
H è´Ÿå®š  (æ‰€æœ‰ç‰¹å¾å€¼ < 0)  â†’  å±€éƒ¨æœ€å¤§å€¼
H ä¸å®š  (ç‰¹å¾å€¼æœ‰æ­£æœ‰è´Ÿ)  â†’  éç‚¹

        éç‚¹ç¤ºæ„å›¾ï¼š
           â•±â•²
          â•±  â•²
         â•± â—  â•²    â† ä¸€ä¸ªæ–¹å‘æ˜¯æœ€å¤§å€¼
        â•±      â•²      å¦ä¸€ä¸ªæ–¹å‘æ˜¯æœ€å°å€¼
       â•±        â•²
```

---

## 4. æ¦‚ç‡è®ºä¸ç»Ÿè®¡

### 4.1 æ¦‚ç‡åŸºç¡€

#### 4.1.1 åŸºæœ¬æ¦‚å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ ·æœ¬ç©ºé—´ (Sample Space) Î©ï¼šæ‰€æœ‰å¯èƒ½ç»“æœçš„é›†åˆ                â”‚
â”‚ äº‹ä»¶ (Event)ï¼šæ ·æœ¬ç©ºé—´çš„å­é›†                                â”‚
â”‚ æ¦‚ç‡ (Probability)ï¼šäº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§ï¼ŒP(A) âˆˆ [0, 1]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ¦‚ç‡å…¬ç†ï¼š**

1. **éè´Ÿæ€§**ï¼š$P(A) \geq 0$
2. **è§„èŒƒæ€§**ï¼š$P(\Omega) = 1$
3. **å¯åŠ æ€§**ï¼šè‹¥ $A \cap B = \emptyset$ï¼Œåˆ™ $P(A \cup B) = P(A) + P(B)$

#### 4.1.2 æ¡ä»¶æ¦‚ç‡ä¸è´å¶æ–¯å®šç†

**æ¡ä»¶æ¦‚ç‡ï¼š**

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

**è´å¶æ–¯å®šç†ï¼š**

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

```
è´å¶æ–¯å®šç†çš„ç›´è§‚ç†è§£ï¼š

    å…ˆéªŒæ¦‚ç‡         ä¼¼ç„¶å‡½æ•°         åéªŒæ¦‚ç‡
    P(Î¸)      Ã—     P(D|Î¸)     âˆ     P(Î¸|D)

    (åˆå§‹ä¿¡å¿µ)       (æ•°æ®è¯æ®)        (æ›´æ–°åçš„ä¿¡å¿µ)

ç¤ºä¾‹ï¼šåƒåœ¾é‚®ä»¶åˆ†ç±»
    P(åƒåœ¾é‚®ä»¶|åŒ…å«"å…è´¹")
    = P(åŒ…å«"å…è´¹"|åƒåœ¾é‚®ä»¶) Ã— P(åƒåœ¾é‚®ä»¶) / P(åŒ…å«"å…è´¹")
```

### 4.2 éšæœºå˜é‡ä¸æ¦‚ç‡åˆ†å¸ƒ

#### 4.2.1 ç¦»æ•£å‹éšæœºå˜é‡

**æ¦‚ç‡è´¨é‡å‡½æ•° (PMF)ï¼š**

$$P(X = x_i) = p_i, \quad \sum_i p_i = 1$$

```
ä¼¯åŠªåˆ©åˆ†å¸ƒ (æŠ›ç¡¬å¸):
              P(X)
                â”‚
            1.0 â”¤
                â”‚
            0.5 â”¼â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€
                â”‚   â”‚       â”‚
            0.0 â”¼â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€
                    0   â”‚   1    X
                      å¤±è´¥   æˆåŠŸ
```

#### 4.2.2 è¿ç»­å‹éšæœºå˜é‡

**æ¦‚ç‡å¯†åº¦å‡½æ•° (PDF)ï¼š**

$$P(a \leq X \leq b) = \int_a^b f(x) dx$$

```
é«˜æ–¯åˆ†å¸ƒ (æ­£æ€åˆ†å¸ƒ):

        f(x)
          â”‚        â•­â”€â”€â”€â•®
          â”‚       â•±     â•²
          â”‚      â•±       â•²
          â”‚     â•±         â•²
          â”‚    â•±           â•²
          â”‚   â•±             â•²
          â”‚â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”€â”€â†’ x
             Î¼-2Ïƒ  Î¼  Î¼+2Ïƒ

    çº¦68%çš„æ•°æ®è½åœ¨ [Î¼-Ïƒ, Î¼+Ïƒ]
    çº¦95%çš„æ•°æ®è½åœ¨ [Î¼-2Ïƒ, Î¼+2Ïƒ]
    çº¦99.7%çš„æ•°æ®è½åœ¨ [Î¼-3Ïƒ, Î¼+3Ïƒ]
```

### 4.3 å¸¸è§æ¦‚ç‡åˆ†å¸ƒ

#### 4.3.1 ç¦»æ•£åˆ†å¸ƒ

| åˆ†å¸ƒ           | å…¬å¼                                                     | åº”ç”¨åœºæ™¯                |
| -------------- | -------------------------------------------------------- | ----------------------- |
| **ä¼¯åŠªåˆ©åˆ†å¸ƒ** | $P(X=1)=p$                                               | äºŒåˆ†ç±»é—®é¢˜              |
| **äºŒé¡¹åˆ†å¸ƒ**   | $P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$                      | n æ¬¡ç‹¬ç«‹è¯•éªŒä¸­æˆåŠŸ k æ¬¡ |
| **æ³Šæ¾åˆ†å¸ƒ**   | $P(X=k)=\frac{\lambda^k e^{-\lambda}}{k!}$               | ç¨€æœ‰äº‹ä»¶è®¡æ•°            |
| **å¤šé¡¹åˆ†å¸ƒ**   | $P(\mathbf{x})=\frac{n!}{\prod_i x_i!}\prod_i p_i^{x_i}$ | å¤šåˆ†ç±»é—®é¢˜              |

#### 4.3.2 è¿ç»­åˆ†å¸ƒ

| åˆ†å¸ƒ          | æ¦‚ç‡å¯†åº¦å‡½æ•°                                                       | åº”ç”¨åœºæ™¯      |
| ------------- | ------------------------------------------------------------------ | ------------- |
| **å‡åŒ€åˆ†å¸ƒ**  | $f(x)=\frac{1}{b-a}$                                               | éšæœºåˆå§‹åŒ–    |
| **é«˜æ–¯åˆ†å¸ƒ**  | $f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ | å™ªå£°å»ºæ¨¡ã€VAE |
| **æŒ‡æ•°åˆ†å¸ƒ**  | $f(x)=\lambda e^{-\lambda x}$                                      | ç­‰å¾…æ—¶é—´      |
| **Beta åˆ†å¸ƒ** | $f(x)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$         | æ¦‚ç‡çš„æ¦‚ç‡    |

#### 4.3.3 ä»£ç å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# é«˜æ–¯åˆ†å¸ƒ
x = np.linspace(-4, 4, 100)
mu, sigma = 0, 1
gaussian = stats.norm.pdf(x, mu, sigma)

# ç»˜å›¾
plt.figure(figsize=(10, 6))
plt.plot(x, gaussian, label=f'N({mu}, {sigma}Â²)')
plt.fill_between(x, gaussian, alpha=0.3)
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Gaussian Distribution')
plt.legend()
plt.grid(True)
plt.show()
```

### 4.4 æœŸæœ›ã€æ–¹å·®ä¸åæ–¹å·®

#### 4.4.1 æœŸæœ› (Expectation)

**ç¦»æ•£ï¼š** $E[X] = \sum_i x_i \cdot P(X=x_i)$

**è¿ç»­ï¼š** $E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx$

**æ€§è´¨ï¼š**

- $E[aX + b] = aE[X] + b$
- $E[X + Y] = E[X] + E[Y]$
- è‹¥ X,Y ç‹¬ç«‹ï¼š$E[XY] = E[X]E[Y]$

#### 4.4.2 æ–¹å·® (Variance)

$$Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$

**æ€§è´¨ï¼š**

- $Var(aX + b) = a^2 Var(X)$
- è‹¥ X,Y ç‹¬ç«‹ï¼š$Var(X + Y) = Var(X) + Var(Y)$

```
æ–¹å·®çš„ç›´è§‚ç†è§£ï¼š

    ä½æ–¹å·®                    é«˜æ–¹å·®

      â”‚â—â—â—â—â”‚                â—â”‚    â”‚â—
    â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€            â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€
      Î¼                       Î¼

    æ•°æ®é›†ä¸­åœ¨å‡å€¼é™„è¿‘        æ•°æ®åˆ†æ•£åœ¨å‡å€¼ä¸¤ä¾§
```

#### 4.4.3 åæ–¹å·®ä¸ç›¸å…³ç³»æ•°

**åæ–¹å·®ï¼š**

$$Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$$

**ç›¸å…³ç³»æ•°ï¼š**

$$\rho_{XY} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y} \in [-1, 1]$$

```
ç›¸å…³æ€§ç¤ºæ„å›¾ï¼š

Ï â‰ˆ 1 (æ­£ç›¸å…³)     Ï â‰ˆ 0 (æ— ç›¸å…³)     Ï â‰ˆ -1 (è´Ÿç›¸å…³)

    yâ”‚    â—â—         yâ”‚  â—    â—        yâ”‚â—â—
     â”‚  â—â—            â”‚â—   â—            â”‚  â—â—
     â”‚â—â—              â”‚  â—   â—          â”‚    â—â—
     â””â”€â”€â”€â”€â†’x          â””â”€â”€â”€â”€â†’x           â””â”€â”€â”€â”€â†’x
```

**åæ–¹å·®çŸ©é˜µï¼š**

$$
\Sigma = \begin{bmatrix}
Var(X_1) & Cov(X_1,X_2) & \cdots \\
Cov(X_2,X_1) & Var(X_2) & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$

### 4.5 æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)

#### 4.5.1 åŸºæœ¬æ€æƒ³

ç»™å®šè§‚æµ‹æ•°æ® $D = \{x_1, x_2, ..., x_n\}$ï¼Œæ‰¾åˆ°å‚æ•° Î¸ ä½¿å¾—æ•°æ®å‡ºç°çš„æ¦‚ç‡æœ€å¤§ï¼š

$$\hat{\theta}_{MLE} = \arg\max_\theta P(D|\theta) = \arg\max_\theta \prod_{i=1}^n P(x_i|\theta)$$

ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œé€šå¸¸æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ï¼š

$$\hat{\theta}_{MLE} = \arg\max_\theta \sum_{i=1}^n \log P(x_i|\theta)$$

#### 4.5.2 ç¤ºä¾‹ï¼šé«˜æ–¯åˆ†å¸ƒå‚æ•°ä¼°è®¡

```python
import numpy as np

# å‡è®¾æ•°æ®æ¥è‡ªæ­£æ€åˆ†å¸ƒ
data = np.array([2.3, 1.9, 2.5, 2.1, 2.0, 2.4, 2.2])

# MLEä¼°è®¡
mu_mle = np.mean(data)           # Î¼çš„MLE = æ ·æœ¬å‡å€¼
sigma_mle = np.std(data, ddof=0)  # Ïƒçš„MLE = æ ·æœ¬æ ‡å‡†å·®

print(f"Î¼_MLE = {mu_mle:.3f}")
print(f"Ïƒ_MLE = {sigma_mle:.3f}")
```

### 4.6 é‡‡æ ·æ–¹æ³•

#### 4.6.1 å¸¸è§é‡‡æ ·æ–¹æ³•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      é‡‡æ ·æ–¹æ³•                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é€†å˜æ¢é‡‡æ ·         â”‚ é€šè¿‡å‡åŒ€åˆ†å¸ƒå’ŒCDFé€†å‡½æ•°ç”Ÿæˆæ ·æœ¬          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ‹’ç»é‡‡æ ·           â”‚ ç”¨ç®€å•åˆ†å¸ƒè¿‘ä¼¼å¤æ‚åˆ†å¸ƒ                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é‡è¦æ€§é‡‡æ ·         â”‚ åŠ æƒé‡‡æ ·ï¼Œç”¨äºè’™ç‰¹å¡æ´›ä¼°è®¡               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MCMC              â”‚ é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼Œç”¨äºé«˜ç»´åˆ†å¸ƒé‡‡æ ·      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é‡å‚æ•°åŒ–æŠ€å·§       â”‚ VAEä¸­å°†é‡‡æ ·å˜ä¸ºç¡®å®šæ€§å˜æ¢               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.6.2 é‡å‚æ•°åŒ–æŠ€å·§ (Reparameterization Trick)

ç”¨äº VAE ä¸­ä½¿é‡‡æ ·è¿‡ç¨‹å¯å¾®ï¼š

```
åŸå§‹é‡‡æ ·ï¼š          z ~ N(Î¼, ÏƒÂ²)     â† ä¸å¯å¾®ï¼

é‡å‚æ•°åŒ–ï¼š          Îµ ~ N(0, 1)      â† é‡‡æ ·ä¸ä¾èµ–å‚æ•°
                   z = Î¼ + ÏƒÂ·Îµ      â† ç¡®å®šæ€§å˜æ¢ï¼Œå¯å¾®ï¼

è®¡ç®—å›¾ï¼š

    Î¼ â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”œâ”€â”€â†’ z = Î¼ + ÏƒÂ·Îµ â”€â”€â†’ è§£ç å™¨
    Ïƒ â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â†‘
    Îµ ~ N(0,1)â”˜ (ä¸å‚ä¸æ¢¯åº¦è®¡ç®—)
```

---

## 5. ä¼˜åŒ–ç†è®º

### 5.1 ä¼˜åŒ–é—®é¢˜åŸºç¡€

#### 5.1.1 ä¼˜åŒ–é—®é¢˜çš„ä¸€èˆ¬å½¢å¼

$$\min_{\theta} L(\theta) \quad \text{subject to} \quad g_i(\theta) \leq 0, \quad h_j(\theta) = 0$$

æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸æ˜¯**æ— çº¦æŸä¼˜åŒ–**ï¼š

$$\min_{\theta} L(\theta; D) = \min_{\theta} \frac{1}{N} \sum_{i=1}^N \ell(f(x_i; \theta), y_i)$$

#### 5.1.2 å‡¸å‡½æ•°ä¸éå‡¸å‡½æ•°

```
å‡¸å‡½æ•°ï¼š                      éå‡¸å‡½æ•°ï¼ˆæ·±åº¦å­¦ä¹ ä¸­å¸¸è§ï¼‰ï¼š

    Lâ”‚                            Lâ”‚     â•­â•®
     â”‚    â•²      â•±                 â”‚    â•±  â•²  â•­â•®
     â”‚     â•²    â•±                  â”‚   â•±    â•²â•±  â•²
     â”‚      â•²  â•±                   â”‚  â•±          â•²
     â”‚       â•²â•± â† å…¨å±€æœ€å°          â”‚ â•±   â† å±€éƒ¨æœ€å°
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î¸                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î¸

ç‰¹ç‚¹ï¼šä»»æ„ä¸¤ç‚¹è¿çº¿              ç‰¹ç‚¹ï¼šå­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€å°å€¼
      åœ¨æ›²çº¿ä¸Šæ–¹                      å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
```

### 5.2 æ¢¯åº¦ä¸‹é™ç®—æ³•

#### 5.2.1 åŸºæœ¬æ¢¯åº¦ä¸‹é™

$$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$$

```
æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ï¼š

    L(Î¸)
      â”‚          â—Î¸â‚€
      â”‚         â•±
      â”‚        â—Î¸â‚
      â”‚       â•±
      â”‚      â—Î¸â‚‚
      â”‚     â•±
      â”‚    â—Î¸â‚ƒ
      â”‚   â•±
      â”‚  â— Î¸* (æœ€ä¼˜è§£)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î¸
```

#### 5.2.2 ä¸‰ç§æ¢¯åº¦ä¸‹é™å˜ä½“

| æ–¹æ³•                   | æ‰¹é‡å¤§å°   | æ›´æ–°å…¬å¼                                       | ç‰¹ç‚¹       |
| ---------------------- | ---------- | ---------------------------------------------- | ---------- |
| **æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD)** | å…¨éƒ¨æ•°æ®   | $\theta = \theta - \eta \nabla L(\theta)$      | ç¨³å®šä½†æ…¢   |
| **éšæœºæ¢¯åº¦ä¸‹é™ (SGD)** | 1 ä¸ªæ ·æœ¬   | $\theta = \theta - \eta \nabla \ell_i(\theta)$ | å¿«ä½†ä¸ç¨³å®š |
| **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™**     | mini-batch | $\theta = \theta - \eta \nabla L_B(\theta)$    | å¹³è¡¡é€‰æ‹©   |

```python
def mini_batch_gradient_descent(X, y, batch_size, learning_rate, epochs):
    theta = initialize_parameters()
    n_samples = X.shape[0]

    for epoch in range(epochs):
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(n_samples)
        X_shuffled = X[indices]
        y_shuffled = y[indices]

        # æŒ‰æ‰¹æ¬¡æ›´æ–°
        for i in range(0, n_samples, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]

            # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°
            gradient = compute_gradient(X_batch, y_batch, theta)
            theta = theta - learning_rate * gradient

    return theta
```

### 5.3 é«˜çº§ä¼˜åŒ–ç®—æ³•

#### 5.3.1 åŠ¨é‡ (Momentum)

ç´¯ç§¯å†å²æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼š

$$v_t = \gamma v_{t-1} + \eta \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

```
ç‰©ç†ç±»æ¯”ï¼š

    å°çƒæ»šä¸‹å±±å¡

    â”‚â•²            æ— åŠ¨é‡ï¼šæ¯æ­¥ç‹¬ç«‹
    â”‚ â•² â—â†’
    â”‚  â•²
    â”‚   â—â†’        æœ‰åŠ¨é‡ï¼šæƒ¯æ€§ç´¯ç§¯
    â”‚    â•²â—â†’â†’â†’
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    åŠ¨é‡å¸®åŠ©è¶Šè¿‡å±€éƒ¨æœ€å°å€¼çš„"å°å‘"
```

#### 5.3.2 AdaGrad

è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¯¹ä¸åŒå‚æ•°ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼š

$$G_t = G_{t-1} + (\nabla L(\theta_t))^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla L(\theta_t)$$

**ç‰¹ç‚¹ï¼š** é¢‘ç¹æ›´æ–°çš„å‚æ•°å­¦ä¹ ç‡è‡ªåŠ¨é™ä½

#### 5.3.3 RMSprop

è§£å†³ AdaGrad å­¦ä¹ ç‡ä¸‹é™è¿‡å¿«çš„é—®é¢˜ï¼š

$$E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho)(\nabla L(\theta_t))^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \nabla L(\theta_t)$$

#### 5.3.4 Adam (æœ€å¸¸ç”¨)

ç»“åˆ Momentum å’Œ RMSprop çš„ä¼˜ç‚¹ï¼š

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla L(\theta_t)$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla L(\theta_t))^2$$
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

```python
class Adam:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None  # ä¸€é˜¶çŸ©ä¼°è®¡
        self.v = None  # äºŒé˜¶çŸ©ä¼°è®¡
        self.t = 0     # æ—¶é—´æ­¥

    def update(self, params, grads):
        if self.m is None:
            self.m = np.zeros_like(params)
            self.v = np.zeros_like(params)

        self.t += 1

        # æ›´æ–°ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)

        # åå·®ä¿®æ­£
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)

        # å‚æ•°æ›´æ–°
        params = params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)

        return params
```

#### 5.3.5 ä¼˜åŒ–å™¨å¯¹æ¯”

```
ä¼˜åŒ–è½¨è¿¹å¯¹æ¯”ï¼ˆç­‰é«˜çº¿å›¾ï¼‰ï¼š

          SGD                  Momentum               Adam
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    â•­â”€â”€â”€â•®    â”‚      â”‚    â•­â”€â”€â”€â•®    â”‚      â”‚    â•­â”€â”€â”€â•®    â”‚
    â”‚  â•­â”€â”¼â—â†’â†’â”¼â”€â•®  â”‚      â”‚  â•­â”€â”¼â—â†’ â”¼â”€â•®  â”‚      â”‚  â•­â”€â”¼â—  â”¼â”€â•®  â”‚
    â”‚ â•­â”¼â”€â”¤â†’â†’â†’â”œâ”€â”¼â•® â”‚      â”‚ â•­â”¼â”€â”¤ â†“ â”œâ”€â”¼â•® â”‚      â”‚ â•­â”¼â”€â”¤â†“  â”œâ”€â”¼â•® â”‚
    â”‚ â”‚â”œâ”€â”¤â†’â—â†’â”œâ”€â”¤â”‚ â”‚      â”‚ â”‚â”œâ”€â”¤â†’â—â†’â”œâ”€â”¤â”‚ â”‚      â”‚ â”‚â”œâ”€â”¤â†’â— â”œâ”€â”¤â”‚ â”‚
    â”‚ â•°â”¼â”€â”¤â†â†â†â”œâ”€â”¼â•¯ â”‚      â”‚ â•°â”¼â”€â”¤ â†“ â”œâ”€â”¼â•¯ â”‚      â”‚ â•°â”¼â”€â”¤ â†“ â”œâ”€â”¼â•¯ â”‚
    â”‚  â•°â”€â”¼â”€â”€â”€â”¼â”€â•¯  â”‚      â”‚  â•°â”€â”¼â†’â—â†â”¼â”€â•¯  â”‚      â”‚  â•°â”€â”¼â”€â—â”€â”¼â”€â•¯  â”‚
    â”‚    â•°â”€â”€â”€â•¯    â”‚      â”‚    â•°â”€â”€â”€â•¯    â”‚      â”‚    â•°â”€â”€â”€â•¯    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        éœ‡è¡ä¸¥é‡              è¾ƒå¹³æ»‘                æœ€ç›´æ¥
```

### 5.4 å­¦ä¹ ç‡è°ƒåº¦

```python
# å¸¸è§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

# 1. é˜¶æ¢¯è¡°å‡ (Step Decay)
def step_decay(epoch, initial_lr, drop_rate=0.5, epochs_drop=10):
    return initial_lr * (drop_rate ** (epoch // epochs_drop))

# 2. æŒ‡æ•°è¡°å‡ (Exponential Decay)
def exponential_decay(epoch, initial_lr, decay_rate=0.95):
    return initial_lr * (decay_rate ** epoch)

# 3. ä½™å¼¦é€€ç« (Cosine Annealing)
def cosine_annealing(epoch, initial_lr, T_max):
    return initial_lr * (1 + np.cos(np.pi * epoch / T_max)) / 2

# 4. Warmup + Decay
def warmup_decay(epoch, initial_lr, warmup_epochs, total_epochs):
    if epoch < warmup_epochs:
        return initial_lr * epoch / warmup_epochs
    else:
        return initial_lr * (1 - (epoch - warmup_epochs) / (total_epochs - warmup_epochs))
```

```
å­¦ä¹ ç‡è°ƒåº¦å¯è§†åŒ–ï¼š

    lr
     â”‚  â”Œâ”€â”€Step Decay
     â”‚  â”‚  â•­â”€â”€Exponential
     â”‚  â–¼  â”‚  â•­â”€â”€Cosine
     â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚
     â”‚     â–¼â–¼â–¼â”‚
     â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–¼â•®
     â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â•²
     â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ epoch
```

### 5.5 æ­£åˆ™åŒ–æŠ€æœ¯

#### 5.5.1 L1 å’Œ L2 æ­£åˆ™åŒ–

$$L_{reg} = L + \lambda \|\theta\|_p$$

| æ­£åˆ™åŒ– | å…¬å¼                       | æ•ˆæœ               |
| ------ | -------------------------- | ------------------ |
| L1     | $\lambda \sum\|\theta_i\|$ | äº§ç”Ÿç¨€ç–è§£         |
| L2     | $\lambda \sum\theta_i^2$   | æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡å¤§ |

```
L1 vs L2 çš„è§£ç©ºé—´ï¼š

    L1 æ­£åˆ™åŒ–               L2 æ­£åˆ™åŒ–
    (è±å½¢çº¦æŸ)              (åœ†å½¢çº¦æŸ)

       Î¸â‚‚                     Î¸â‚‚
        â”‚  â—‡                   â”‚  â—‹
        â”‚ â•±â—†â•²                  â”‚â•­â—â”€â•®
        â”‚â•±  â•²                  â”‚â”‚ â•²â”‚
    â”€â”€â”€â”€â—†â”€â”€â”€â”€â—‡â”€â”€â†’ Î¸â‚      â”€â”€â”€â”€â—‹â—â”€â”€â—‹â”€â”€â†’ Î¸â‚
        â”‚â•²  â•±                  â”‚â•°â”€â”€â•¯
        â”‚ â•²â—†â•±                  â”‚
        â”‚                      â”‚

    è§£å¾€å¾€åœ¨é¡¶ç‚¹            è§£ä¸ä¸€å®šç¨€ç–
    (Î¸â‚æˆ–Î¸â‚‚ä¸º0)
```

#### 5.5.2 Dropout

è®­ç»ƒæ—¶éšæœºå…³é—­ç¥ç»å…ƒï¼š

```
æ­£å¸¸ç½‘ç»œ                    Dropout
    â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹               â—‹â”€â”€â”€â—â”€â”€â”€â—‹
   â•±â”‚â•² â•±â”‚â•² â•±â”‚â•²             â•±â”‚    â”‚â•² â•±â”‚â•²
  â—‹â”€â”¼â”€â—‹â”€â”¼â”€â—‹â”€â”¼â”€â—‹           â—‹â”€â”¼â”€â—â”€â”€â”¼â”€â—‹â”€â”¼â”€â—‹
   â•²â”‚â•± â•²â”‚â•± â•²â”‚â•±             â•²â”‚    â”‚â•± â•²â”‚â•±
    â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹               â—â”€â”€â”€â—‹â”€â”€â”€â—‹

                          â— = è¢«å…³é—­çš„ç¥ç»å…ƒ
```

```python
def dropout(x, keep_prob, training=True):
    if training:
        mask = np.random.binomial(1, keep_prob, size=x.shape)
        return x * mask / keep_prob  # ç¼©æ”¾ä»¥ä¿æŒæœŸæœ›
    return x
```

#### 5.5.3 Batch Normalization

æ ‡å‡†åŒ–æ¯ä¸€å±‚çš„è¾“å…¥ï¼š

$$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
$$y = \gamma \hat{x} + \beta$$

```python
def batch_normalization(x, gamma, beta, epsilon=1e-5):
    # è®¡ç®—æ‰¹æ¬¡ç»Ÿè®¡é‡
    mu = np.mean(x, axis=0)
    var = np.var(x, axis=0)

    # æ ‡å‡†åŒ–
    x_norm = (x - mu) / np.sqrt(var + epsilon)

    # ç¼©æ”¾å’Œå¹³ç§»
    out = gamma * x_norm + beta

    return out
```

---

## 6. ä¿¡æ¯è®ºåŸºç¡€

### 6.1 ç†µ (Entropy)

#### 6.1.1 å®šä¹‰

$$H(X) = -\sum_{i} P(x_i) \log P(x_i) = E[-\log P(X)]$$

**ç›´è§‚ç†è§£ï¼š** è¡¡é‡éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§

```
ä½ç†µ (ç¡®å®šæ€§é«˜)                é«˜ç†µ (ä¸ç¡®å®šæ€§é«˜)

  P(x)                          P(x)
    â”‚                             â”‚
    â”‚ â–ˆâ–ˆâ–ˆâ–ˆ                        â”‚ â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ
    â”‚ â–ˆâ–ˆâ–ˆâ–ˆ                        â”‚ â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ
    â”‚ â–ˆâ–ˆâ–ˆâ–ˆ                        â”‚ â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ x                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x

    H â‰ˆ 0                         H â‰ˆ log(n)
    (å‡ ä¹ç¡®å®š)                    (å®Œå…¨éšæœº)
```

**ç¤ºä¾‹ï¼š**

```python
import numpy as np

def entropy(p):
    """è®¡ç®—ç¦»æ•£åˆ†å¸ƒçš„ç†µ"""
    p = np.array(p)
    p = p[p > 0]  # å»é™¤é›¶æ¦‚ç‡
    return -np.sum(p * np.log2(p))

# å…¬å¹³ç¡¬å¸
print(f"å…¬å¹³ç¡¬å¸: {entropy([0.5, 0.5]):.2f} bits")  # 1.00 bits

# ä¸å…¬å¹³ç¡¬å¸
print(f"ä¸å…¬å¹³ç¡¬å¸: {entropy([0.9, 0.1]):.2f} bits")  # 0.47 bits

# ç¡®å®šæ€§
print(f"ç¡®å®šæ€§: {entropy([1.0, 0.0]):.2f} bits")  # 0.00 bits
```

### 6.2 äº¤å‰ç†µ (Cross-Entropy)

#### 6.2.1 å®šä¹‰

$$H(P, Q) = -\sum_{i} P(x_i) \log Q(x_i) = E_P[-\log Q(X)]$$

**åº”ç”¨ï¼šåˆ†ç±»ä»»åŠ¡çš„æŸå¤±å‡½æ•°**

```
çœŸå®åˆ†å¸ƒ P = [1, 0, 0]  (ç±»åˆ«0)
é¢„æµ‹åˆ†å¸ƒ Q = [0.7, 0.2, 0.1]

äº¤å‰ç†µ = -1Â·log(0.7) - 0Â·log(0.2) - 0Â·log(0.1)
       = -log(0.7)
       â‰ˆ 0.357
```

#### 6.2.2 äºŒåˆ†ç±»äº¤å‰ç†µ

$$L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$$

```python
def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):
    """äºŒåˆ†ç±»äº¤å‰ç†µ"""
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):
    """å¤šåˆ†ç±»äº¤å‰ç†µ"""
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=-1))
```

### 6.3 KL æ•£åº¦ (Kullback-Leibler Divergence)

#### 6.3.1 å®šä¹‰

$$D_{KL}(P \| Q) = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)} = H(P, Q) - H(P)$$

**æ€§è´¨ï¼š**

- $D_{KL}(P \| Q) \geq 0$
- $D_{KL}(P \| Q) = 0$ å½“ä¸”ä»…å½“ $P = Q$
- ä¸å¯¹ç§°ï¼š$D_{KL}(P \| Q) \neq D_{KL}(Q \| P)$

```
KLæ•£åº¦çš„ä¸å¯¹ç§°æ€§ï¼š

    P (çœŸå®åˆ†å¸ƒ)           Q (è¿‘ä¼¼åˆ†å¸ƒ)

      â•­â”€â”€â•®                   â•­â•® â•­â•®
     â•±    â•²                 â•±  â•²â•±  â•²
    â•±      â•²               â•±        â•²
   â•±â”€â”€â”€â”€â”€â”€â”€â”€â•²             â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²

    D_KL(P||Q): è¡¡é‡ç”¨Qè¿‘ä¼¼Pçš„ä¿¡æ¯æŸå¤±
    D_KL(Q||P): è¡¡é‡ç”¨Pè¿‘ä¼¼Qçš„ä¿¡æ¯æŸå¤±

    ä¸¤è€…å¯èƒ½ç›¸å·®å¾ˆå¤§ï¼
```

#### 6.3.2 åº”ç”¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KLæ•£åº¦çš„åº”ç”¨                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VAEæŸå¤±å‡½æ•°      â”‚ ä½¿éšç©ºé—´åˆ†å¸ƒæ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ çŸ¥è¯†è’¸é¦         â”‚ ä½¿å­¦ç”Ÿæ¨¡å‹è¾“å‡ºæ¥è¿‘æ•™å¸ˆæ¨¡å‹                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å¼ºåŒ–å­¦ä¹  (PPO)   â”‚ é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.4 äº’ä¿¡æ¯ (Mutual Information)

$$I(X; Y) = H(X) + H(Y) - H(X, Y) = D_{KL}(P(X,Y) \| P(X)P(Y))$$

**ç›´è§‚ç†è§£ï¼š** ä¸¤ä¸ªå˜é‡å…±äº«çš„ä¿¡æ¯é‡

```
äº’ä¿¡æ¯çš„æ–‡æ°å›¾ï¼š

    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚               â”‚               â”‚
    â”‚    H(X|Y)     â”‚    H(Y|X)     â”‚
    â”‚               â”‚               â”‚
    â”‚         â•­â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â•®         â”‚
    â”‚         â”‚           â”‚         â”‚
    â”‚         â”‚   I(X;Y)  â”‚         â”‚
    â”‚         â”‚           â”‚         â”‚
    â”‚         â•°â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â•¯         â”‚
    â”‚               â”‚               â”‚
    â”‚    H(X)       â”‚      H(Y)     â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

    I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
```

---

## 7. å®æˆ˜åº”ç”¨

### 7.1 å®Œæ•´ç¤ºä¾‹ï¼šä»é›¶å®ç°ç¥ç»ç½‘ç»œ

```python
import numpy as np

class NeuralNetwork:
    """ä»é›¶å¼€å§‹å®ç°çš„ç¥ç»ç½‘ç»œ"""

    def __init__(self, layer_dims):
        """
        layer_dims: æ¯å±‚ç¥ç»å…ƒæ•°é‡ï¼Œå¦‚ [784, 128, 64, 10]
        """
        self.params = {}
        self.L = len(layer_dims) - 1  # å±‚æ•°

        # Xavieråˆå§‹åŒ–
        for l in range(1, self.L + 1):
            self.params[f'W{l}'] = np.random.randn(
                layer_dims[l], layer_dims[l-1]
            ) * np.sqrt(2.0 / layer_dims[l-1])
            self.params[f'b{l}'] = np.zeros((layer_dims[l], 1))

    def relu(self, Z):
        return np.maximum(0, Z)

    def relu_derivative(self, Z):
        return (Z > 0).astype(float)

    def softmax(self, Z):
        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))
        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)

    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.cache = {'A0': X}
        A = X

        # éšè—å±‚ä½¿ç”¨ReLU
        for l in range(1, self.L):
            Z = self.params[f'W{l}'] @ A + self.params[f'b{l}']
            A = self.relu(Z)
            self.cache[f'Z{l}'] = Z
            self.cache[f'A{l}'] = A

        # è¾“å‡ºå±‚ä½¿ç”¨Softmax
        ZL = self.params[f'W{self.L}'] @ A + self.params[f'b{self.L}']
        AL = self.softmax(ZL)
        self.cache[f'Z{self.L}'] = ZL
        self.cache[f'A{self.L}'] = AL

        return AL

    def compute_loss(self, AL, Y):
        """äº¤å‰ç†µæŸå¤±"""
        m = Y.shape[1]
        epsilon = 1e-15
        loss = -np.sum(Y * np.log(AL + epsilon)) / m
        return loss

    def backward(self, Y):
        """åå‘ä¼ æ’­"""
        m = Y.shape[1]
        grads = {}

        # è¾“å‡ºå±‚æ¢¯åº¦
        dZL = self.cache[f'A{self.L}'] - Y
        grads[f'dW{self.L}'] = (1/m) * dZL @ self.cache[f'A{self.L-1}'].T
        grads[f'db{self.L}'] = (1/m) * np.sum(dZL, axis=1, keepdims=True)

        dA = self.params[f'W{self.L}'].T @ dZL

        # éšè—å±‚æ¢¯åº¦
        for l in reversed(range(1, self.L)):
            dZ = dA * self.relu_derivative(self.cache[f'Z{l}'])
            grads[f'dW{l}'] = (1/m) * dZ @ self.cache[f'A{l-1}'].T
            grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)
            dA = self.params[f'W{l}'].T @ dZ

        return grads

    def update_params(self, grads, learning_rate):
        """æ›´æ–°å‚æ•°"""
        for l in range(1, self.L + 1):
            self.params[f'W{l}'] -= learning_rate * grads[f'dW{l}']
            self.params[f'b{l}'] -= learning_rate * grads[f'db{l}']

    def train(self, X, Y, epochs, learning_rate, print_cost=True):
        """è®­ç»ƒæ¨¡å‹"""
        costs = []

        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            AL = self.forward(X)

            # è®¡ç®—æŸå¤±
            cost = self.compute_loss(AL, Y)
            costs.append(cost)

            # åå‘ä¼ æ’­
            grads = self.backward(Y)

            # æ›´æ–°å‚æ•°
            self.update_params(grads, learning_rate)

            if print_cost and epoch % 100 == 0:
                print(f"Epoch {epoch}: Loss = {cost:.4f}")

        return costs

    def predict(self, X):
        """é¢„æµ‹"""
        AL = self.forward(X)
        return np.argmax(AL, axis=0)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    X = np.random.randn(784, 1000)  # 1000ä¸ªæ ·æœ¬ï¼Œ784ç»´ç‰¹å¾
    Y = np.eye(10)[:, np.random.randint(0, 10, 1000)]  # one-hotç¼–ç 

    # åˆ›å»ºå¹¶è®­ç»ƒç½‘ç»œ
    nn = NeuralNetwork([784, 128, 64, 10])
    costs = nn.train(X, Y, epochs=1000, learning_rate=0.1)

    # é¢„æµ‹
    predictions = nn.predict(X)
    accuracy = np.mean(predictions == np.argmax(Y, axis=0))
    print(f"Training Accuracy: {accuracy * 100:.2f}%")
```

### 7.2 æ•°å­¦å…¬å¼é€ŸæŸ¥è¡¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æ·±åº¦å­¦ä¹ æ•°å­¦å…¬å¼é€ŸæŸ¥è¡¨                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ã€çº¿æ€§ä»£æ•°ã€‘                                                             â”‚
â”‚   çŸ©é˜µä¹˜æ³•:    (AB)áµ¢â±¼ = Î£â‚– Aáµ¢â‚–Bâ‚–â±¼                                       â”‚
â”‚   è½¬ç½®:       (AB)áµ€ = Báµ€Aáµ€                                              â”‚
â”‚   é€†çŸ©é˜µ:     (AB)â»Â¹ = Bâ»Â¹Aâ»Â¹                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ã€å¾®ç§¯åˆ†ã€‘                                                               â”‚
â”‚   é“¾å¼æ³•åˆ™:   âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚g Â· âˆ‚g/âˆ‚x                                     â”‚
â”‚   Softmax:   Ïƒáµ¢ = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)                                 â”‚
â”‚   Sigmoid:   Ïƒ(x) = 1/(1+eâ»Ë£), Ïƒ'(x) = Ïƒ(x)(1-Ïƒ(x))                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ã€æ¦‚ç‡è®ºã€‘                                                               â”‚
â”‚   è´å¶æ–¯:     P(A|B) = P(B|A)P(A) / P(B)                                â”‚
â”‚   é«˜æ–¯åˆ†å¸ƒ:   N(Î¼,ÏƒÂ²) = (1/âˆš(2Ï€ÏƒÂ²)) exp(-(x-Î¼)Â²/(2ÏƒÂ²))                  â”‚
â”‚   æœŸæœ›:       E[X] = Î£ xáµ¢P(xáµ¢)                                          â”‚
â”‚   æ–¹å·®:       Var(X) = E[XÂ²] - E[X]Â²                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ã€ä¼˜åŒ–ã€‘                                                                 â”‚
â”‚   æ¢¯åº¦ä¸‹é™:   Î¸ â† Î¸ - Î·âˆ‡L(Î¸)                                            â”‚
â”‚   åŠ¨é‡:       v â† Î³v + Î·âˆ‡L, Î¸ â† Î¸ - v                                   â”‚
â”‚   Adam:      m â† Î²â‚m + (1-Î²â‚)g, v â† Î²â‚‚v + (1-Î²â‚‚)gÂ²                      â”‚
â”‚              Î¸ â† Î¸ - Î·Â·mÌ‚/(âˆšvÌ‚ + Îµ)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ã€ä¿¡æ¯è®ºã€‘                                                               â”‚
â”‚   ç†µ:        H(X) = -Î£ P(x)log P(x)                                     â”‚
â”‚   äº¤å‰ç†µ:    H(P,Q) = -Î£ P(x)log Q(x)                                   â”‚
â”‚   KLæ•£åº¦:    D_KL(P||Q) = Î£ P(x)log(P(x)/Q(x))                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 å­¦ä¹ è·¯å¾„å»ºè®®

```
                    æ·±åº¦å­¦ä¹ æ•°å­¦å­¦ä¹ è·¯å¾„

    ç¬¬1é˜¶æ®µ (1-2å‘¨)              ç¬¬2é˜¶æ®µ (2-3å‘¨)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  çº¿æ€§ä»£æ•°    â”‚             â”‚   å¾®ç§¯åˆ†     â”‚
    â”‚             â”‚             â”‚             â”‚
    â”‚ â€¢ å‘é‡è¿ç®—   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚ â€¢ å¯¼æ•°       â”‚
    â”‚ â€¢ çŸ©é˜µè¿ç®—   â”‚             â”‚ â€¢ é“¾å¼æ³•åˆ™   â”‚
    â”‚ â€¢ ç‰¹å¾åˆ†è§£   â”‚             â”‚ â€¢ æ¢¯åº¦       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
           â†“                          â†“
    ç¬¬3é˜¶æ®µ (2-3å‘¨)              ç¬¬4é˜¶æ®µ (2-3å‘¨)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   æ¦‚ç‡è®º     â”‚             â”‚  ä¼˜åŒ–ç†è®º    â”‚
    â”‚             â”‚             â”‚             â”‚
    â”‚ â€¢ æ¦‚ç‡åˆ†å¸ƒ   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚ â€¢ æ¢¯åº¦ä¸‹é™   â”‚
    â”‚ â€¢ æœŸæœ›æ–¹å·®   â”‚             â”‚ â€¢ Adamç­‰    â”‚
    â”‚ â€¢ MLE       â”‚             â”‚ â€¢ æ­£åˆ™åŒ–     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
                  ç¬¬5é˜¶æ®µ (å®è·µ)
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  é¡¹ç›®å®æˆ˜    â”‚
                  â”‚             â”‚
                  â”‚ â€¢ æ‰‹å†™NN    â”‚
                  â”‚ â€¢ è¯»è®ºæ–‡    â”‚
                  â”‚ â€¢ å¤ç°æ¨¡å‹  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å‚è€ƒèµ„æº

### æ¨èä¹¦ç±

| ä¹¦å                   | ä½œè€…          | é€‚åˆç¨‹åº¦      |
| ---------------------- | ------------- | ------------- |
| ã€Šæ·±åº¦å­¦ä¹ ã€‹(èŠ±ä¹¦)     | Goodfellow ç­‰ | â­â­â­ è¿›é˜¶   |
| ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ | é‚±é”¡é¹        | â­â­ å…¥é—¨     |
| ã€Šçº¿æ€§ä»£æ•°åº”è¯¥è¿™æ ·å­¦ã€‹ | Axler         | â­â­ æ•°å­¦åŸºç¡€ |
| ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹   | é™ˆå¸Œå­º        | â­â­ æ•°å­¦åŸºç¡€ |

### åœ¨çº¿èµ„æº

- **3Blue1Brown**: çº¿æ€§ä»£æ•°å’Œå¾®ç§¯åˆ†çš„å¯è§†åŒ–
- **Stanford CS229**: æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€
- **MIT 18.06**: Gilbert Strang çš„çº¿æ€§ä»£æ•°è¯¾ç¨‹
- **Khan Academy**: åŸºç¡€æ•°å­¦å¤ä¹ 

---

> ğŸ“ **æ€»ç»“**ï¼šæ·±åº¦å­¦ä¹ çš„æ•°å­¦åŸºç¡€ä¸»è¦åŒ…æ‹¬**çº¿æ€§ä»£æ•°**ï¼ˆæ•°æ®è¡¨ç¤ºï¼‰ã€**å¾®ç§¯åˆ†**ï¼ˆæ¨¡å‹ä¼˜åŒ–ï¼‰ã€**æ¦‚ç‡è®º**ï¼ˆä¸ç¡®å®šæ€§å»ºæ¨¡ï¼‰å’Œ**ä¼˜åŒ–ç†è®º**ï¼ˆå‚æ•°å­¦ä¹ ï¼‰ã€‚æŒæ¡è¿™äº›çŸ¥è¯†åï¼Œä½ å°†èƒ½å¤Ÿç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æœ¬è´¨åŸç†ï¼Œè€Œä¸ä»…ä»…æ˜¯è°ƒç”¨ APIã€‚

---

_æ–‡æ¡£ç‰ˆæœ¬ï¼š1.0 | æœ€åæ›´æ–°ï¼š2024 å¹´_
