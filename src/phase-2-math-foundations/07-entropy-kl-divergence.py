"""
07-entropy-kl-divergence.py
Phase 2: æ·±åº¦å­¦ä¹ æ•°å­¦åŸºç¡€

ç†µä¸KLæ•£åº¦ - ä¿¡æ¯è®ºä¸æŸå¤±å‡½æ•°

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£ä¿¡æ¯ç†µçš„æ¦‚å¿µ
2. æŒæ¡äº¤å‰ç†µæŸå¤±çš„åŸç†
3. ç†è§£KLæ•£åº¦åŠå…¶åº”ç”¨
4. äº†è§£è¿™äº›æ¦‚å¿µåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨
"""

import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 60)
print("æ·±åº¦å­¦ä¹ æ•°å­¦åŸºç¡€ - ç†µä¸ KL æ•£åº¦")
print("=" * 60)

# =============================================================================
# 1. ä¿¡æ¯è®ºåŸºç¡€
# =============================================================================
print("\nã€1. ä¿¡æ¯è®ºåŸºç¡€ã€‘")

print("""
ä¿¡æ¯é‡ï¼š
    I(x) = -logâ‚‚(p(x))
    
- æ¦‚ç‡è¶Šå°çš„äº‹ä»¶ï¼Œä¿¡æ¯é‡è¶Šå¤§
- ç¡®å®šäº‹ä»¶ï¼ˆp=1ï¼‰ä¿¡æ¯é‡ä¸º 0
- ä¸å¯èƒ½äº‹ä»¶ï¼ˆpâ†’0ï¼‰ä¿¡æ¯é‡ä¸º âˆ

ä¾‹å¦‚ï¼š
- "å¤ªé˜³ä»ä¸œè¾¹å‡èµ·"ï¼šæ¦‚ç‡é«˜ï¼Œä¿¡æ¯é‡ä½
- "ä»Šå¤©å½©ç¥¨ä¸­å¥–"ï¼šæ¦‚ç‡ä½ï¼Œä¿¡æ¯é‡é«˜
""")

# ä¿¡æ¯é‡ç¤ºä¾‹
probs = [0.9999, 0.5, 0.1, 0.01, 0.001]
for p in probs:
    info = -np.log2(p)
    print(f"  P(x)={p:.4f}, ä¿¡æ¯é‡ I(x) = {info:.4f} bits")

# =============================================================================
# 2. ç†µ (Entropy)
# =============================================================================
print("\n" + "=" * 60)
print("ã€2. ç†µ (Entropy)ã€‘")

print("""
ç†µï¼šéšæœºå˜é‡çš„å¹³å‡ä¿¡æ¯é‡ï¼ˆä¸ç¡®å®šæ€§åº¦é‡ï¼‰

    H(X) = -Î£ p(x) Ã— logâ‚‚(p(x))

æ€§è´¨ï¼š
- ç†µè¶Šé«˜ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§
- å‡åŒ€åˆ†å¸ƒç†µæœ€å¤§
- ç¡®å®šæ€§åˆ†å¸ƒï¼ˆæŸç±»åˆ«æ¦‚ç‡ä¸º1ï¼‰ç†µä¸º 0
""")

def entropy(probs):
    """è®¡ç®—ç¦»æ•£åˆ†å¸ƒçš„ç†µ"""
    probs = np.array(probs)
    probs = probs[probs > 0]  # é¿å… log(0)
    return -np.sum(probs * np.log2(probs))

# ä¸åŒåˆ†å¸ƒçš„ç†µ
distributions = [
    ([1.0], "ç¡®å®šæ€§ [1.0]"),
    ([0.5, 0.5], "å‡åŒ€ [0.5, 0.5]"),
    ([0.9, 0.1], "åæ–œ [0.9, 0.1]"),
    ([0.25, 0.25, 0.25, 0.25], "å‡åŒ€ 4ç±» [0.25Ã—4]"),
    ([0.7, 0.1, 0.1, 0.1], "åæ–œ 4ç±» [0.7, 0.1Ã—3]"),
]

print("\nä¸åŒåˆ†å¸ƒçš„ç†µï¼š")
for probs, name in distributions:
    h = entropy(probs)
    print(f"  {name}: H = {h:.4f} bits")

# å¯è§†åŒ–ç†µä¸æ¦‚ç‡çš„å…³ç³»ï¼ˆäºŒå…ƒæƒ…å†µï¼‰
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# äºŒå…ƒç†µ
p = np.linspace(0.001, 0.999, 100)
h = -p * np.log2(p) - (1-p) * np.log2(1-p)
axes[0].plot(p, h, 'b-', linewidth=2)
axes[0].set_xlabel('P(X=1)')
axes[0].set_ylabel('ç†µ H(X)')
axes[0].set_title('äºŒå…ƒç†µå‡½æ•°', fontsize=12, fontweight='bold')
axes[0].axhline(y=1, color='r', linestyle='--', label='æœ€å¤§ç†µ=1 @ p=0.5')
axes[0].axvline(x=0.5, color='r', linestyle='--')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ä¸åŒç±»åˆ«æ•°çš„æœ€å¤§ç†µ
n_classes = np.arange(2, 20)
max_entropy = np.log2(n_classes)
axes[1].plot(n_classes, max_entropy, 'bo-', linewidth=2, markersize=8)
axes[1].set_xlabel('ç±»åˆ«æ•°')
axes[1].set_ylabel('æœ€å¤§ç†µ')
axes[1].set_title('å‡åŒ€åˆ†å¸ƒçš„æœ€å¤§ç†µ = logâ‚‚(n)', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('outputs/07_entropy.png', dpi=150, bbox_inches='tight')
plt.close()
print("\nå·²ä¿å­˜: outputs/07_entropy.png")

# =============================================================================
# 3. äº¤å‰ç†µ (Cross Entropy)
# =============================================================================
print("\n" + "=" * 60)
print("ã€3. äº¤å‰ç†µ (Cross Entropy)ã€‘")

print("""
äº¤å‰ç†µï¼šç”¨åˆ†å¸ƒ q æ¥ç¼–ç æ¥è‡ªåˆ†å¸ƒ p çš„æ•°æ®çš„å¹³å‡æ¯”ç‰¹æ•°

    H(p, q) = -Î£ p(x) Ã— log(q(x))

æ·±åº¦å­¦ä¹ ä¸­ï¼š
- p: çœŸå®æ ‡ç­¾åˆ†å¸ƒï¼ˆone-hotï¼‰
- q: æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆsoftmaxè¾“å‡ºï¼‰

å¯¹äºåˆ†ç±»é—®é¢˜ï¼š
    L = -Î£ y_true Ã— log(y_pred)
    
å¯¹äºäºŒåˆ†ç±»ï¼š
    L = -[yÃ—log(p) + (1-y)Ã—log(1-p)]  (Binary Cross Entropy)
""")

def cross_entropy(p_true, q_pred):
    """äº¤å‰ç†µ"""
    q_pred = np.clip(q_pred, 1e-15, 1)  # é¿å… log(0)
    return -np.sum(p_true * np.log(q_pred))

# ç¤ºä¾‹
y_true = np.array([1, 0, 0])  # çœŸå®æ ‡ç­¾ï¼ˆç±»åˆ«0ï¼‰

predictions = [
    [0.9, 0.05, 0.05],   # å¥½çš„é¢„æµ‹
    [0.7, 0.2, 0.1],     # ä¸€èˆ¬çš„é¢„æµ‹
    [0.4, 0.3, 0.3],     # å·®çš„é¢„æµ‹
    [0.1, 0.8, 0.1],     # é”™è¯¯çš„é¢„æµ‹
]

print("\nçœŸå®æ ‡ç­¾: [1, 0, 0] (ç±»åˆ«0)")
print("ä¸åŒé¢„æµ‹çš„äº¤å‰ç†µæŸå¤±ï¼š")
for pred in predictions:
    ce = cross_entropy(y_true, pred)
    print(f"  é¢„æµ‹ {pred} â†’ äº¤å‰ç†µ = {ce:.4f}")

# =============================================================================
# 4. KL æ•£åº¦ (KL Divergence)
# =============================================================================
print("\n" + "=" * 60)
print("ã€4. KL æ•£åº¦ (KL Divergence)ã€‘")

print("""
KLæ•£åº¦ï¼šè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„"è·ç¦»"

    D_KL(p || q) = Î£ p(x) Ã— log(p(x) / q(x))
                 = H(p, q) - H(p)
                 = äº¤å‰ç†µ - ç†µ

æ€§è´¨ï¼š
1. D_KL(p || q) â‰¥ 0ï¼Œå½“ä¸”ä»…å½“ p = q æ—¶ç­‰äº 0
2. ä¸å¯¹ç§°ï¼šD_KL(p || q) â‰  D_KL(q || p)
3. ä¸æ»¡è¶³ä¸‰è§’ä¸ç­‰å¼ï¼Œæ‰€ä»¥ä¸æ˜¯çœŸæ­£çš„"è·ç¦»"

æ·±åº¦å­¦ä¹ åº”ç”¨ï¼š
- VAE çš„ ELBO æŸå¤±ä¸­çš„æ­£åˆ™é¡¹
- çŸ¥è¯†è’¸é¦
- ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆPPOï¼‰
""")

def kl_divergence(p, q):
    """KLæ•£åº¦ D_KL(p || q)"""
    p = np.array(p)
    q = np.array(q)
    p = np.clip(p, 1e-15, 1)
    q = np.clip(q, 1e-15, 1)
    return np.sum(p * np.log(p / q))

# ç¤ºä¾‹
p = np.array([0.4, 0.3, 0.3])
distributions_q = [
    ([0.4, 0.3, 0.3], "q = p"),
    ([0.35, 0.35, 0.3], "q ç•¥æœ‰ä¸åŒ"),
    ([0.5, 0.25, 0.25], "q æœ‰å·®å¼‚"),
    ([0.8, 0.1, 0.1], "q å·®å¼‚è¾ƒå¤§"),
]

print(f"\nçœŸå®åˆ†å¸ƒ p = {p}")
print("ä¸åŒ q åˆ†å¸ƒçš„ KL æ•£åº¦ï¼š")
for q, name in distributions_q:
    kl = kl_divergence(p, q)
    print(f"  {name}: D_KL(p||q) = {kl:.6f}")

# KL æ•£åº¦çš„ä¸å¯¹ç§°æ€§
q = np.array([0.6, 0.2, 0.2])
print(f"\np = {list(p)}, q = {list(q)}")
print(f"D_KL(p || q) = {kl_divergence(p, q):.4f}")
print(f"D_KL(q || p) = {kl_divergence(q, p):.4f}")
print("æ³¨æ„ï¼šKL æ•£åº¦æ˜¯ä¸å¯¹ç§°çš„ï¼")

# =============================================================================
# 5. äº¤å‰ç†µ vs KL æ•£åº¦
# =============================================================================
print("\n" + "=" * 60)
print("ã€5. äº¤å‰ç†µ vs KL æ•£åº¦ã€‘")

print("""
å…³ç³»ï¼š
    H(p, q) = H(p) + D_KL(p || q)
    äº¤å‰ç†µ = ç†µ + KLæ•£åº¦

ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ ç”¨äº¤å‰ç†µè€Œä¸æ˜¯ KL æ•£åº¦ï¼Ÿ
- å½“ p æ˜¯çœŸå®æ ‡ç­¾ï¼ˆå›ºå®šçš„ï¼‰ï¼ŒH(p) æ˜¯å¸¸æ•°
- æœ€å°åŒ–äº¤å‰ç†µ = æœ€å°åŒ– KL æ•£åº¦
- äº¤å‰ç†µè®¡ç®—æ›´ç®€å•ï¼ˆä¸éœ€è¦è®¡ç®— H(p)ï¼‰
""")

# éªŒè¯å…³ç³»
p = np.array([0.7, 0.2, 0.1])
q = np.array([0.6, 0.3, 0.1])

h_p = entropy(p)
h_p_q = cross_entropy(p, q)
kl_p_q = kl_divergence(p, q)

print(f"\np = {list(p)}, q = {list(q)}")
print(f"H(p) = {h_p:.4f}")
print(f"H(p, q) = {h_p_q:.4f}")
print(f"D_KL(p || q) = {kl_p_q:.4f}")
print(f"H(p) + D_KL(p||q) = {h_p + kl_p_q:.4f}")

# =============================================================================
# 6. å¯è§†åŒ– KL æ•£åº¦
# =============================================================================
print("\n" + "=" * 60)
print("ã€6. å¯è§†åŒ– KL æ•£åº¦ã€‘")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ KL æ•£åº¦
from scipy import stats

mu_p, sigma_p = 0, 1
x = np.linspace(-5, 5, 1000)
p_pdf = stats.norm.pdf(x, mu_p, sigma_p)

mus_q = [0, 0.5, 1, 2]
ax = axes[0]
ax.plot(x, p_pdf, 'k-', linewidth=2, label='p ~ N(0, 1)')

for mu_q in mus_q:
    sigma_q = 1
    q_pdf = stats.norm.pdf(x, mu_q, sigma_q)
    # é«˜æ–¯åˆ†å¸ƒçš„ KL æ•£åº¦æœ‰è§£æè§£
    kl = 0.5 * ((sigma_p/sigma_q)**2 + (mu_q - mu_p)**2 / sigma_q**2 - 1 + 2*np.log(sigma_q/sigma_p))
    ax.plot(x, q_pdf, '--', linewidth=2, label=f'q ~ N({mu_q}, 1), KL={kl:.4f}')

ax.set_xlabel('x')
ax.set_ylabel('æ¦‚ç‡å¯†åº¦')
ax.set_title('KLæ•£åº¦ï¼šå‡å€¼å˜åŒ–çš„å½±å“', fontsize=12, fontweight='bold')
ax.legend()

sigmas_q = [0.5, 1, 1.5, 2]
ax = axes[1]
ax.plot(x, p_pdf, 'k-', linewidth=2, label='p ~ N(0, 1)')

for sigma_q in sigmas_q:
    mu_q = 0
    q_pdf = stats.norm.pdf(x, mu_q, sigma_q)
    kl = 0.5 * ((sigma_p/sigma_q)**2 + (mu_q - mu_p)**2 / sigma_q**2 - 1 + 2*np.log(sigma_q/sigma_p))
    ax.plot(x, q_pdf, '--', linewidth=2, label=f'q ~ N(0, {sigma_q}Â²), KL={kl:.4f}')

ax.set_xlabel('x')
ax.set_ylabel('æ¦‚ç‡å¯†åº¦')
ax.set_title('KLæ•£åº¦ï¼šæ–¹å·®å˜åŒ–çš„å½±å“', fontsize=12, fontweight='bold')
ax.legend()

plt.tight_layout()
plt.savefig('outputs/07_kl_divergence.png', dpi=150, bbox_inches='tight')
plt.close()
print("å·²ä¿å­˜: outputs/07_kl_divergence.png")

# =============================================================================
# 7. æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨
# =============================================================================
print("\n" + "=" * 60)
print("ã€7. æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ã€‘")

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ä¿¡æ¯è®ºåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  äº¤å‰ç†µæŸå¤±    â•‘  åˆ†ç±»ä»»åŠ¡çš„æ ‡å‡†æŸå¤±å‡½æ•°                        â•‘
â•‘  BCELoss       â•‘  äºŒåˆ†ç±»ï¼š-[yÃ—log(p) + (1-y)Ã—log(1-p)]         â•‘
â•‘  CELoss        â•‘  å¤šåˆ†ç±»ï¼š-Î£y_iÃ—log(p_i)                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  VAE           â•‘  ELBO = E[log p(x|z)] - D_KL(q(z|x) || p(z)) â•‘
â•‘  çŸ¥è¯†è’¸é¦      â•‘  ç”¨ KL æ•£åº¦è®©å­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹              â•‘
â•‘  PPO           â•‘  é™åˆ¶ç­–ç•¥æ›´æ–°ï¼šD_KL(Ï€_new || Ï€_old) < Îµ       â•‘
â•‘  Focal Loss    â•‘  ä¿®æ”¹äº¤å‰ç†µè§£å†³ç±»åˆ«ä¸å¹³è¡¡                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# å®ç°äº¤å‰ç†µæŸå¤±
print("\nå®ç°åˆ†ç±»æŸå¤±ï¼š")

def binary_cross_entropy(y_true, y_pred):
    """äºŒåˆ†ç±»äº¤å‰ç†µ"""
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def categorical_cross_entropy(y_true, y_pred):
    """å¤šåˆ†ç±»äº¤å‰ç†µï¼ˆy_true æ˜¯ one-hotï¼‰"""
    y_pred = np.clip(y_pred, 1e-15, 1)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

# äºŒåˆ†ç±»ç¤ºä¾‹
y_true_binary = np.array([1, 0, 1, 1, 0])
y_pred_binary = np.array([0.9, 0.2, 0.8, 0.7, 0.3])
bce = binary_cross_entropy(y_true_binary, y_pred_binary)
print(f"äºŒåˆ†ç±»: y_true={y_true_binary}, y_pred={y_pred_binary}")
print(f"BCELoss = {bce:.4f}")

# å¤šåˆ†ç±»ç¤ºä¾‹
y_true_multi = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
y_pred_multi = np.array([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7]])
cce = categorical_cross_entropy(y_true_multi, y_pred_multi)
print(f"\nå¤šåˆ†ç±» y_true (one-hot):\n{y_true_multi}")
print(f"y_pred:\n{y_pred_multi}")
print(f"CELoss = {cce:.4f}")

# =============================================================================
# 8. VAE ä¸­çš„ KL æ•£åº¦
# =============================================================================
print("\n" + "=" * 60)
print("ã€8. VAE ä¸­çš„ KL æ•£åº¦ã€‘")

print("""
VAE æŸå¤± = é‡æ„æŸå¤± + KL æ­£åˆ™é¡¹

KL æ­£åˆ™é¡¹ï¼ˆå‡è®¾ p(z) = N(0, I)ï¼‰ï¼š
    D_KL(q(z|x) || p(z)) = -0.5 Ã— Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)

ä½œç”¨ï¼šä½¿ç¼–ç å™¨è¾“å‡ºçš„åˆ†å¸ƒæ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ
""")

def vae_kl_loss(mu, log_var):
    """VAE çš„ KL æŸå¤±é¡¹
    
    mu: ç¼–ç å™¨è¾“å‡ºçš„å‡å€¼
    log_var: ç¼–ç å™¨è¾“å‡ºçš„å¯¹æ•°æ–¹å·®
    """
    return -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var))

# ç¤ºä¾‹
mu = np.array([0.5, -0.3, 0.2])
log_var = np.array([-0.5, 0.1, -0.2])  # log(ÏƒÂ²)

kl = vae_kl_loss(mu, log_var)
print(f"Î¼ = {mu}")
print(f"log(ÏƒÂ²) = {log_var}")
print(f"KL Loss = {kl:.4f}")

# å½“ Î¼=0, Ïƒ=1 æ—¶ KL åº”è¯¥æ¥è¿‘ 0
mu_zero = np.zeros(3)
log_var_zero = np.zeros(3)  # log(1) = 0
kl_zero = vae_kl_loss(mu_zero, log_var_zero)
print(f"\nå½“ Î¼=0, Ïƒ=1 æ—¶: KL Loss = {kl_zero:.4f} (åº”è¯¥æ¥è¿‘ 0)")

# =============================================================================
# 9. ç»ƒä¹ é¢˜
# =============================================================================
print("\n" + "=" * 60)
print("ã€ç»ƒä¹ é¢˜ã€‘")
print("=" * 60)

print("""
1. è®¡ç®—åˆ†å¸ƒ [0.5, 0.3, 0.2] çš„ç†µ

2. ç»™å®šçœŸå®æ ‡ç­¾ [0, 1, 0] å’Œé¢„æµ‹ [0.1, 0.7, 0.2]ï¼Œè®¡ç®—äº¤å‰ç†µ

3. è®¡ç®—ä»¥ä¸‹ä¸¤ä¸ªåˆ†å¸ƒçš„ KL æ•£åº¦ï¼š
   p = [0.3, 0.4, 0.3]
   q = [0.25, 0.5, 0.25]

4. å®ç° Label Smoothingï¼šå°† one-hot æ ‡ç­¾ [1, 0, 0] å¹³æ»‘ä¸º [0.9, 0.05, 0.05]

5. å®ç° Focal Lossï¼šFL = -(1-p)^Î³ Ã— log(p)ï¼Œå…¶ä¸­ Î³=2
""")

# === ç»ƒä¹ ä»£ç  ===
# ç»ƒä¹  1: è®¡ç®—ç†µ
# p = np.array([0.5, 0.3, 0.2])
# H = -np.sum(p * np.log2(p))
# print(f"H([0.5, 0.3, 0.2]) = {H:.4f} bits")

# ç»ƒä¹  2: äº¤å‰ç†µ
# y_true = np.array([0, 1, 0])
# y_pred = np.array([0.1, 0.7, 0.2])
# CE = -np.sum(y_true * np.log(y_pred))
# print(f"äº¤å‰ç†µ = {CE:.4f}")

# ç»ƒä¹  3: KL æ•£åº¦
# p = np.array([0.3, 0.4, 0.3])
# q = np.array([0.25, 0.5, 0.25])
# KL = np.sum(p * np.log(p / q))
# print(f"D_KL(p||q) = {KL:.6f}")

# ç»ƒä¹  4: Label Smoothing
# def label_smoothing(one_hot, epsilon=0.1):
#     n_classes = len(one_hot)
#     return one_hot * (1 - epsilon) + epsilon / n_classes
# y = np.array([1, 0, 0])
# y_smooth = label_smoothing(y, epsilon=0.1)
# print(f"å¹³æ»‘å: {y_smooth}")  # [0.9, 0.05, 0.05]

# ç»ƒä¹  5: Focal Loss
# def focal_loss(p, gamma=2):
#     return -((1 - p)**gamma) * np.log(p)
# for p in [0.1, 0.5, 0.9]:
#     ce = -np.log(p)
#     fl = focal_loss(p)
#     print(f"p={p}: CE={ce:.4f}, Focal={fl:.4f}")

print("\nâœ… ç†µä¸KLæ•£åº¦å®Œæˆï¼")
print("ğŸ‰ Phase 2 å…¨éƒ¨æ¨¡å—å·²å®Œæˆï¼")
print("\nä¸‹ä¸€æ­¥ï¼šè¿›å…¥ Phase 3 - PyTorch æ ¸å¿ƒæŠ€èƒ½")
