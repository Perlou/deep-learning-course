"""
02-eigenvalue-svd.py
Phase 2: æ·±åº¦å­¦ä¹ æ•°å­¦åŸºç¡€

ç‰¹å¾åˆ†è§£ä¸å¥‡å¼‚å€¼åˆ†è§£ (SVD)

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„å‡ ä½•æ„ä¹‰
2. æŒæ¡ SVD åˆ†è§£åŠå…¶åº”ç”¨
3. ç†è§£ PCA çš„æ•°å­¦åŸç†
4. äº†è§£ä½ç§©è¿‘ä¼¼åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ï¼ˆå¦‚ LoRAï¼‰
"""

import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 60)
print("æ·±åº¦å­¦ä¹ æ•°å­¦åŸºç¡€ - ç‰¹å¾åˆ†è§£ä¸ SVD")
print("=" * 60)

# =============================================================================
# 1. ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡
# =============================================================================
print("\nã€1. ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡ã€‘")

print("""
å®šä¹‰ï¼šå¯¹äºæ–¹é˜µ Aï¼Œå¦‚æœå­˜åœ¨éé›¶å‘é‡ v å’Œæ ‡é‡ Î»ï¼Œä½¿å¾—ï¼š
      A @ v = Î» Ã— v
åˆ™ Î» æ˜¯ç‰¹å¾å€¼ï¼Œv æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚

å‡ ä½•æ„ä¹‰ï¼š
- ç‰¹å¾å‘é‡æ˜¯åœ¨å˜æ¢ A ä½œç”¨ä¸‹åªå‘ç”Ÿç¼©æ”¾ï¼ˆä¸æ”¹å˜æ–¹å‘ï¼‰çš„å‘é‡
- ç‰¹å¾å€¼æ˜¯ç¼©æ”¾çš„å€æ•°
""")

# ç¤ºä¾‹
A = np.array([[4, 2],
              [1, 3]])
print(f"çŸ©é˜µ A:\n{A}")

eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"\nç‰¹å¾å€¼: {eigenvalues}")
print(f"ç‰¹å¾å‘é‡:\n{eigenvectors}")

# éªŒè¯ A @ v = Î» Ã— v
print("\néªŒè¯ A @ v = Î» Ã— v:")
for i in range(len(eigenvalues)):
    v = eigenvectors[:, i]
    Î» = eigenvalues[i]
    Av = A @ v
    Î»v = Î» * v
    print(f"  Î»_{i+1} = {Î»:.2f}")
    print(f"  A @ v_{i+1} = {Av}")
    print(f"  Î» Ã— v_{i+1} = {Î»v}")
    print()

# å¯è§†åŒ–ç‰¹å¾å‘é‡
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# åŸå§‹ç©ºé—´
ax = axes[0]
# ç»˜åˆ¶ç½‘æ ¼
for i in np.linspace(-2, 2, 9):
    ax.axhline(y=i, color='lightgray', linewidth=0.5)
    ax.axvline(x=i, color='lightgray', linewidth=0.5)
# ç»˜åˆ¶ç‰¹å¾å‘é‡
colors = ['red', 'blue']
for i in range(2):
    v = eigenvectors[:, i]
    ax.arrow(0, 0, v[0], v[1], head_width=0.1, head_length=0.05, 
             fc=colors[i], ec=colors[i], label=f'v_{i+1} (Î»={eigenvalues[i]:.2f})')
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_aspect('equal')
ax.set_title('åŸå§‹ç‰¹å¾å‘é‡', fontsize=12, fontweight='bold')
ax.legend()

# å˜æ¢å
ax = axes[1]
for i in np.linspace(-2, 2, 9):
    ax.axhline(y=i, color='lightgray', linewidth=0.5)
    ax.axvline(x=i, color='lightgray', linewidth=0.5)
for i in range(2):
    v = eigenvectors[:, i]
    v_transformed = A @ v
    ax.arrow(0, 0, v_transformed[0], v_transformed[1], head_width=0.1, head_length=0.05,
             fc=colors[i], ec=colors[i], label=f'A @ v_{i+1}')
ax.set_xlim(-6, 6)
ax.set_ylim(-6, 6)
ax.set_aspect('equal')
ax.set_title('å˜æ¢åçš„ç‰¹å¾å‘é‡ï¼ˆåªç¼©æ”¾ï¼Œä¸æ—‹è½¬ï¼‰', fontsize=12, fontweight='bold')
ax.legend()

plt.tight_layout()
plt.savefig('outputs/02_eigenvectors.png', dpi=150, bbox_inches='tight')
plt.close()
print("å·²ä¿å­˜: outputs/02_eigenvectors.png")

# =============================================================================
# 2. å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾åˆ†è§£
# =============================================================================
print("\n" + "=" * 60)
print("ã€2. å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾åˆ†è§£ã€‘")

# å¯¹ç§°çŸ©é˜µçš„ç‰¹æ®Šæ€§è´¨
A = np.array([[4, 2],
              [2, 3]])
print(f"å¯¹ç§°çŸ©é˜µ A:\n{A}")

eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"\nç‰¹å¾å€¼: {eigenvalues}")
print(f"ç‰¹å¾å‘é‡:\n{eigenvectors}")

# å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ­£äº¤
print(f"\nç‰¹å¾å‘é‡ç‚¹ç§¯ï¼ˆéªŒè¯æ­£äº¤æ€§ï¼‰: {eigenvectors[:, 0] @ eigenvectors[:, 1]:.6f}")

# ç‰¹å¾åˆ†è§£: A = Q Î› Q^T
Q = eigenvectors
Lambda = np.diag(eigenvalues)
A_reconstructed = Q @ Lambda @ Q.T

print(f"\nç‰¹å¾åˆ†è§£ A = Q Î› Q^T:")
print(f"Q:\n{Q}")
print(f"Î›:\n{Lambda}")
print(f"é‡æ„ A:\n{A_reconstructed}")

print("""
ğŸ’¡ æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ï¼š
   - åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ â†’ ç‰¹å¾å‘é‡æ­£äº¤
   - PCA å°±æ˜¯å¯¹åæ–¹å·®çŸ©é˜µåšç‰¹å¾åˆ†è§£
   - Hessian çŸ©é˜µçš„ç‰¹å¾å€¼åæ˜ æŸå¤±å‡½æ•°çš„æ›²ç‡
""")

# =============================================================================
# 3. å¥‡å¼‚å€¼åˆ†è§£ (SVD)
# =============================================================================
print("\n" + "=" * 60)
print("ã€3. å¥‡å¼‚å€¼åˆ†è§£ (SVD)ã€‘")

print("""
SVD å°†ä»»æ„çŸ©é˜µ A (m Ã— n) åˆ†è§£ä¸ºï¼š
    A = U @ Î£ @ V^T

å…¶ä¸­ï¼š
- U: å·¦å¥‡å¼‚å‘é‡çŸ©é˜µ (m Ã— m)ï¼Œåˆ—å‘é‡æ­£äº¤
- Î£: å¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µ (m Ã— n)ï¼Œå¯¹è§’çº¿å…ƒç´ éè´Ÿé€’å‡
- V^T: å³å¥‡å¼‚å‘é‡çŸ©é˜µ (n Ã— n)ï¼Œè¡Œå‘é‡æ­£äº¤
""")

# ç¤ºä¾‹
A = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9],
              [10, 11, 12]])
print(f"çŸ©é˜µ A ({A.shape[0]}Ã—{A.shape[1]}):\n{A}")

U, s, Vt = np.linalg.svd(A)
print(f"\nU ({U.shape[0]}Ã—{U.shape[1]}):\n{U}")
print(f"\nå¥‡å¼‚å€¼ Ïƒ: {s}")
print(f"\nV^T ({Vt.shape[0]}Ã—{Vt.shape[1]}):\n{Vt}")

# é‡æ„çŸ©é˜µ
Sigma = np.zeros_like(A, dtype=float)
np.fill_diagonal(Sigma, s)
A_reconstructed = U @ Sigma @ Vt
print(f"\né‡æ„ A = U @ Î£ @ V^T:\n{A_reconstructed}")

# =============================================================================
# 4. ä½ç§©è¿‘ä¼¼
# =============================================================================
print("\n" + "=" * 60)
print("ã€4. ä½ç§©è¿‘ä¼¼ã€‘")

print("""
SVD çš„é‡è¦åº”ç”¨ï¼šç”¨å‰ k ä¸ªå¥‡å¼‚å€¼è¿‘ä¼¼åŸçŸ©é˜µ
    A â‰ˆ U_k @ Î£_k @ V_k^T

è¿™æ˜¯æœ€ä¼˜çš„ç§©-k è¿‘ä¼¼ï¼ˆæœ€å°åŒ– Frobenius èŒƒæ•°è¯¯å·®ï¼‰
""")

# åˆ›å»ºä¸€ä¸ªæœ‰æ„ä¹‰çš„çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼
np.random.seed(42)
# çœŸå®æ•°æ®æ˜¯ä½ç§©çš„ï¼ˆç§©ä¸º 2ï¼‰åŠ å™ªå£°
true_rank = 2
A = np.random.randn(10, 2) @ np.random.randn(2, 8) + 0.1 * np.random.randn(10, 8)
print(f"åŸå§‹çŸ©é˜µå½¢çŠ¶: {A.shape}")
print(f"åŸå§‹çŸ©é˜µç§©: {np.linalg.matrix_rank(A)}")

U, s, Vt = np.linalg.svd(A, full_matrices=False)

# è®¡ç®—ä¸åŒç§©çš„è¿‘ä¼¼è¯¯å·®
errors = []
for k in range(1, len(s) + 1):
    A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    error = np.linalg.norm(A - A_k, 'fro') / np.linalg.norm(A, 'fro')
    errors.append(error)
    if k <= 3:
        print(f"ç§©-{k} è¿‘ä¼¼ç›¸å¯¹è¯¯å·®: {error:.4f}")

# å¯è§†åŒ–å¥‡å¼‚å€¼å’Œè¯¯å·®
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å¥‡å¼‚å€¼
axes[0].bar(range(1, len(s) + 1), s, color='steelblue', edgecolor='black')
axes[0].set_xlabel('å¥‡å¼‚å€¼ç´¢å¼•')
axes[0].set_ylabel('å¥‡å¼‚å€¼å¤§å°')
axes[0].set_title('å¥‡å¼‚å€¼åˆ†å¸ƒ', fontsize=12, fontweight='bold')

# ç´¯ç§¯æ–¹å·®è§£é‡Š
cumulative_var = np.cumsum(s**2) / np.sum(s**2)
axes[1].plot(range(1, len(s) + 1), cumulative_var, 'bo-', linewidth=2, markersize=8)
axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% æ–¹å·®')
axes[1].set_xlabel('ä¿ç•™çš„å¥‡å¼‚å€¼æ•°é‡')
axes[1].set_ylabel('ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹')
axes[1].set_title('ä½ç§©è¿‘ä¼¼çš„æ–¹å·®è§£é‡Š', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('outputs/02_svd_lowrank.png', dpi=150, bbox_inches='tight')
plt.close()
print("\nå·²ä¿å­˜: outputs/02_svd_lowrank.png")

# =============================================================================
# 5. PCA é™ç»´
# =============================================================================
print("\n" + "=" * 60)
print("ã€5. PCA é™ç»´ã€‘")

print("""
PCA (ä¸»æˆåˆ†åˆ†æ) æ­¥éª¤ï¼š
1. æ•°æ®ä¸­å¿ƒåŒ–ï¼šXÌƒ = X - mean(X)
2. è®¡ç®—åæ–¹å·®çŸ©é˜µï¼šC = XÌƒ^T @ XÌƒ / (n-1)
3. å¯¹ C åšç‰¹å¾åˆ†è§£ï¼ˆæˆ–å¯¹ XÌƒ åš SVDï¼‰
4. é€‰æ‹©å‰ k ä¸ªä¸»æˆåˆ†
5. æŠ•å½±ï¼šZ = XÌƒ @ V_k
""")

# ç”Ÿæˆ 2D æ•°æ®
np.random.seed(42)
n_samples = 200
# åˆ›å»ºæœ‰ç›¸å…³æ€§çš„æ•°æ®
mean = [3, 5]
cov = [[2, 1.5], [1.5, 1.5]]
X = np.random.multivariate_normal(mean, cov, n_samples)

print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")

# PCA å®ç°
# 1. ä¸­å¿ƒåŒ–
X_centered = X - X.mean(axis=0)

# 2. SVDï¼ˆè®¡ç®—ä¸»æˆåˆ†ï¼‰
U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)
V = Vt.T  # ä¸»æˆåˆ†æ–¹å‘

print(f"å¥‡å¼‚å€¼: {s}")
print(f"æ–¹å·®è§£é‡Šæ¯”ä¾‹: {s**2 / np.sum(s**2)}")
print(f"ç¬¬ä¸€ä¸»æˆåˆ†æ–¹å‘: {V[:, 0]}")

# 3. æŠ•å½±åˆ°ç¬¬ä¸€ä¸»æˆåˆ†
X_1d = X_centered @ V[:, 0]
print(f"é™ç»´åæ•°æ®å½¢çŠ¶: {X_1d.shape}")

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# åŸå§‹æ•°æ®å’Œä¸»æˆåˆ†
ax = axes[0]
ax.scatter(X[:, 0], X[:, 1], alpha=0.5, c='steelblue')
# ç»˜åˆ¶ä¸»æˆåˆ†æ–¹å‘
origin = X.mean(axis=0)
for i in range(2):
    ax.arrow(origin[0], origin[1], V[0, i]*s[i]/10, V[1, i]*s[i]/10,
             head_width=0.1, head_length=0.05, fc=['red', 'green'][i], ec=['red', 'green'][i],
             label=f'PC{i+1} ({s[i]**2/np.sum(s**2)*100:.1f}%)')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_title('åŸå§‹æ•°æ®ä¸ä¸»æˆåˆ†æ–¹å‘', fontsize=12, fontweight='bold')
ax.legend()
ax.set_aspect('equal')

# æŠ•å½±åˆ°ç¬¬ä¸€ä¸»æˆåˆ†
ax = axes[1]
# é‡æ„ç‚¹
X_reconstructed = X_centered @ V[:, :1] @ V[:, :1].T + X.mean(axis=0)
ax.scatter(X[:, 0], X[:, 1], alpha=0.3, c='steelblue', label='åŸå§‹')
ax.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.5, c='red', s=10, label='æŠ•å½±')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_title('æŠ•å½±åˆ°ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12, fontweight='bold')
ax.legend()
ax.set_aspect('equal')

# é™ç»´åçš„åˆ†å¸ƒ
ax = axes[2]
ax.hist(X_1d, bins=30, color='coral', edgecolor='white', alpha=0.7)
ax.set_xlabel('PC1')
ax.set_ylabel('é¢‘æ¬¡')
ax.set_title('é™ç»´åçš„ä¸€ç»´åˆ†å¸ƒ', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('outputs/02_pca.png', dpi=150, bbox_inches='tight')
plt.close()
print("å·²ä¿å­˜: outputs/02_pca.png")

# =============================================================================
# 6. LoRA: æ·±åº¦å­¦ä¹ ä¸­çš„ä½ç§©é€‚é…
# =============================================================================
print("\n" + "=" * 60)
print("ã€6. LoRA: æ·±åº¦å­¦ä¹ ä¸­çš„ä½ç§©é€‚é…ã€‘")

print("""
LoRA (Low-Rank Adaptation) æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼š
- åŸå§‹æƒé‡: W (d Ã— k)
- å¾®è°ƒæ—¶ä¸ç›´æ¥æ›´æ–° Wï¼Œè€Œæ˜¯å­¦ä¹ ä½ç§©åˆ†è§£: Î”W = A @ B
  - A: (d Ã— r)
  - B: (r Ã— k)
  - r << min(d, k)
  
ä¼˜åŠ¿ï¼š
- å‚æ•°é‡ä» dÃ—k é™åˆ° dÃ—r + rÃ—k
- ä¾‹å¦‚ d=4096, k=4096, r=8
  - åŸå§‹: 16M å‚æ•°
  - LoRA: 65K å‚æ•° (å‡å°‘ 250 å€!)
""")

# æ¨¡æ‹Ÿ LoRA
d, k = 1024, 512
r = 4  # ä½ç§©

# åŸå§‹æƒé‡
W_original = np.random.randn(d, k) * 0.01

# LoRA å¢é‡
A = np.random.randn(d, r) * 0.01
B = np.random.randn(r, k) * 0.01
delta_W = A @ B

# å¾®è°ƒåçš„æƒé‡
W_finetuned = W_original + delta_W

print(f"åŸå§‹æƒé‡ W: {d}Ã—{k} = {d*k:,} å‚æ•°")
print(f"LoRA å‚æ•°: A({d}Ã—{r}) + B({r}Ã—{k}) = {d*r + r*k:,} å‚æ•°")
print(f"å‚æ•°å‹ç¼©æ¯”: {d*k / (d*r + r*k):.1f}x")

# éªŒè¯ delta_W çš„ç§©
print(f"\nÎ”W çš„ç§©: {np.linalg.matrix_rank(delta_W)}")
print(f"Î”W çš„å½¢çŠ¶: {delta_W.shape}")

# =============================================================================
# 7. ç»ƒä¹ é¢˜
# =============================================================================
print("\n" + "=" * 60)
print("ã€ç»ƒä¹ é¢˜ã€‘")
print("=" * 60)

print("""
1. å¯¹çŸ©é˜µ A = [[3, 1], [1, 3]] è¿›è¡Œç‰¹å¾åˆ†è§£ï¼š
   - è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
   - éªŒè¯ A = Q Î› Q^T
   
2. ä½¿ç”¨ SVD å¯¹ä»¥ä¸‹çŸ©é˜µè¿›è¡Œç§©-2 è¿‘ä¼¼ï¼š
   A = [[1, 2, 3, 4],
        [2, 4, 6, 8],
        [3, 6, 9, 12],
        [1, 1, 1, 1]]
   
3. å®ç°ä¸€ä¸ªç®€åŒ–çš„ PCAï¼š
   - ç”Ÿæˆ 100 ä¸ª 5 ç»´æ•°æ®ç‚¹
   - é™ç»´åˆ° 2 ç»´
   - è®¡ç®—ä¿ç•™äº†å¤šå°‘æ–¹å·®

4. è®¡ç®— LoRA å‚æ•°é‡ï¼š
   - åŸå§‹æƒé‡: 4096 Ã— 4096
   - LoRA ç§©: r = 16
   - è®¡ç®—å‚æ•°å‹ç¼©æ¯”
""")

# === ç»ƒä¹ ä»£ç  ===
# ç»ƒä¹  1: å¯¹ç§°çŸ©é˜µç‰¹å¾åˆ†è§£
# A = np.array([[3, 1], [1, 3]])
# eigenvalues, eigenvectors = np.linalg.eig(A)
# print(f"çŸ©é˜µ A:\n{A}")
# print(f"ç‰¹å¾å€¼: {eigenvalues}")
# print(f"ç‰¹å¾å‘é‡:\n{eigenvectors}")
# # éªŒè¯ A = Q Î› Q^T
# Q = eigenvectors
# Lambda = np.diag(eigenvalues)
# A_reconstructed = Q @ Lambda @ Q.T
# print(f"éªŒè¯ A = Q @ Î› @ Q^T:\n{np.round(A_reconstructed, 6)}")

# ç»ƒä¹  2: SVD ç§©-2 è¿‘ä¼¼
# A = np.array([[1, 2, 3, 4],
#               [2, 4, 6, 8],
#               [3, 6, 9, 12],
#               [1, 1, 1, 1]])
# U, s, Vt = np.linalg.svd(A, full_matrices=False)
# print(f"å¥‡å¼‚å€¼: {s}")
# # ç§©-2 è¿‘ä¼¼
# k = 2
# A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
# print(f"ç§©-2 è¿‘ä¼¼:\n{np.round(A_k, 4)}")
# print(f"ç›¸å¯¹è¯¯å·®: {np.linalg.norm(A - A_k) / np.linalg.norm(A):.6f}")

# ç»ƒä¹  3: PCA é™ç»´
# np.random.seed(42)
# X = np.random.randn(100, 5)  # 100 samples, 5 features
# X_centered = X - X.mean(axis=0)
# U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)
# V = Vt.T
# # é™åˆ° 2 ç»´
# k = 2
# X_2d = X_centered @ V[:, :k]
# print(f"åŸå§‹æ•°æ®: {X.shape}")
# print(f"é™ç»´å: {X_2d.shape}")
# var_explained = np.sum(s[:k]**2) / np.sum(s**2)
# print(f"ä¿ç•™æ–¹å·®æ¯”ä¾‹: {var_explained:.4f} ({var_explained*100:.2f}%)")

# ç»ƒä¹  4: LoRA å‚æ•°é‡
# d, k = 4096, 4096
# r = 16
# original_params = d * k
# lora_params = d * r + r * k
# compression = original_params / lora_params
# print(f"åŸå§‹æƒé‡: {d}Ã—{k} = {original_params:,} å‚æ•°")
# print(f"LoRA å‚æ•°: A({d}Ã—{r}) + B({r}Ã—{k}) = {lora_params:,} å‚æ•°")
# print(f"å‹ç¼©æ¯”: {compression:.1f}x")

print("\nâœ… ç‰¹å¾åˆ†è§£ä¸ SVD å®Œæˆï¼")
print("ä¸‹ä¸€æ­¥ï¼š03-derivatives-gradients.py - åå¯¼æ•°ä¸æ¢¯åº¦")
