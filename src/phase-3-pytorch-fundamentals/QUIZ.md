# Phase 3 æµ‹è¯•é¢˜ï¼šPyTorch æ ¸å¿ƒæŠ€èƒ½

## ä¸€ã€é€‰æ‹©é¢˜ (æ¯é¢˜ 2 åˆ†ï¼Œå…± 30 åˆ†)

### 1. ä»¥ä¸‹å“ªä¸ªæ–¹æ³•åˆ›å»ºçš„ Tensor ä¸ NumPy æ•°ç»„å…±äº«å†…å­˜ï¼Ÿ

A. `torch.tensor(np_array)`  
B. `torch.from_numpy(np_array)`  
C. `torch.as_tensor(np_array).clone()`  
D. `torch.Tensor(np_array)`

### 2. å…³äº `requires_grad`ï¼Œä»¥ä¸‹è¯´æ³•æ­£ç¡®çš„æ˜¯ï¼Ÿ

A. åªæœ‰æ ‡é‡æ‰èƒ½è®¾ç½® `requires_grad=True`  
B. `backward()` åæ¢¯åº¦ä¼šè‡ªåŠ¨æ¸…é›¶  
C. é»˜è®¤æƒ…å†µä¸‹æ–°åˆ›å»ºçš„ Tensor `requires_grad=False`  
D. `detach()` ä¼šåŸåœ°ä¿®æ”¹ Tensor çš„ `requires_grad`

### 3. çŸ©é˜µ A (32, 64) ä¸ B (64, 128) ç›¸ä¹˜ï¼Œç»“æœå½¢çŠ¶æ˜¯ï¼Ÿ

A. (32, 64)  
B. (64, 128)  
C. (32, 128)  
D. (64, 64)

### 4. ä»¥ä¸‹å“ªä¸ªæ“ä½œä¸ä¼šæ”¹å˜ Tensor çš„å½¢çŠ¶ï¼Ÿ

A. `squeeze()`  
B. `unsqueeze(0)`  
C. `contiguous()`  
D. `permute(1, 0)`

### 5. åœ¨è‡ªå®šä¹‰ `nn.Module` æ—¶ï¼Œå¿…é¡»å®ç°çš„æ–¹æ³•æ˜¯ï¼Ÿ

A. `__init__` å’Œ `forward`  
B. `__init__` å’Œ `backward`  
C. `forward` å’Œ `backward`  
D. åªæœ‰ `forward`

### 6. `nn.CrossEntropyLoss` çš„è¾“å…¥åº”è¯¥æ˜¯ï¼Ÿ

A. ç»è¿‡ Softmax çš„æ¦‚ç‡  
B. ç»è¿‡ LogSoftmax çš„å¯¹æ•°æ¦‚ç‡  
C. æœªç»å¤„ç†çš„ logits  
D. one-hot ç¼–ç çš„æ ‡ç­¾

### 7. å…³äº `model.train()` å’Œ `model.eval()` çš„åŒºåˆ«ï¼Œé”™è¯¯çš„æ˜¯ï¼Ÿ

A. å½±å“ Dropout å±‚çš„è¡Œä¸º  
B. å½±å“ BatchNorm å±‚çš„è¡Œä¸º  
C. `eval()` æ¨¡å¼ä¸‹ä¸è®¡ç®—æ¢¯åº¦  
D. éœ€è¦åœ¨è®­ç»ƒå’ŒéªŒè¯æ—¶åˆ†åˆ«è®¾ç½®

### 8. ä¼˜åŒ–å™¨çš„ `zero_grad()` åº”è¯¥åœ¨ä»€ä¹ˆæ—¶å€™è°ƒç”¨ï¼Ÿ

A. `backward()` ä¹‹å  
B. `backward()` ä¹‹å‰  
C. `step()` ä¹‹å  
D. æ¯ä¸ª epoch å¼€å§‹æ—¶

### 9. ä»¥ä¸‹å“ªä¸ªä¸æ˜¯ DataLoader çš„å‚æ•°ï¼Ÿ

A. `batch_size`  
B. `shuffle`  
C. `learning_rate`  
D. `num_workers`

### 10. å…³äºæ•°æ®å¢å¼ºï¼Œæ­£ç¡®çš„è¯´æ³•æ˜¯ï¼Ÿ

A. éªŒè¯é›†ä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®å¢å¼º  
B. å¢å¼ºå¼ºåº¦è¶Šå¤§è¶Šå¥½  
C. MixUp é€‚åˆåœ¨ DataLoader å¤–éƒ¨å®ç°  
D. æ‰€æœ‰å¢å¼ºéƒ½åº”è¯¥åœ¨ GPU ä¸Šè¿›è¡Œ

### 11. ä¿å­˜æ¨¡å‹æ—¶ï¼Œæ¨èä½¿ç”¨å“ªç§æ–¹å¼ï¼Ÿ

A. `torch.save(model, 'model.pth')`  
B. `torch.save(model.state_dict(), 'model.pth')`  
C. `pickle.dump(model, file)`  
D. `model.save('model.pth')`

### 12. ä»¥ä¸‹å“ªä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨éœ€è¦ä¼ å…¥éªŒè¯æŒ‡æ ‡ï¼Ÿ

A. `StepLR`  
B. `CosineAnnealingLR`  
C. `ReduceLROnPlateau`  
D. `ExponentialLR`

### 13. `torch.no_grad()` çš„ä½œç”¨æ˜¯ï¼Ÿ

A. ç¦æ­¢æ¨¡å‹æ›´æ–°å‚æ•°  
B. ç¦æ­¢è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜  
C. å°†æ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼  
D. æ¸…é›¶æ‰€æœ‰æ¢¯åº¦

### 14. æ‰¹é‡çŸ©é˜µä¹˜æ³•åº”è¯¥ä½¿ç”¨å“ªä¸ªå‡½æ•°ï¼Ÿ

A. `torch.mm`  
B. `torch.matmul`  
C. `torch.bmm`  
D. `torch.dot`

### 15. å…³äºæ¢¯åº¦è£å‰ªï¼Œæ­£ç¡®çš„æ˜¯ï¼Ÿ

A. åªèƒ½æŒ‰èŒƒæ•°è£å‰ª  
B. åº”è¯¥åœ¨ `backward()` ä¹‹å‰è°ƒç”¨  
C. å¯ä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸  
D. ä¼šæ”¹å˜æ¨¡å‹çš„å‚æ•°

---

## äºŒã€å¡«ç©ºé¢˜ (æ¯é¢˜ 3 åˆ†ï¼Œå…± 24 åˆ†)

### 1. PyTorch çš„åŠ¨æ€è®¡ç®—å›¾åœ¨æ¯æ¬¡**\_\_**ä¼ æ’­æ—¶åˆ›å»ºï¼Œåœ¨**\_\_**ä¼ æ’­åé»˜è®¤é‡Šæ”¾ã€‚

### 2. å°† Tensor ç§»åŠ¨åˆ° GPU çš„ä¸¤ç§æ–¹æ³•ï¼š`x.______` å’Œ `x.to(______)`

### 3. è‡ªå®šä¹‰ Dataset å¿…é¡»å®ç° `__len__` å’Œ**\_\_**æ–¹æ³•ã€‚

### 4. è®­ç»ƒå¾ªç¯çš„ä¸‰æ­¥ï¼š`optimizer.______()`ã€`loss.______()`ã€`optimizer.______()`

### 5. `nn.BCEWithLogitsLoss` å†…éƒ¨åŒ…å«äº†**\_\_**å‡½æ•°ã€‚

### 6. åŠ è½½æ¨¡å‹å‚æ•°æ—¶ï¼Œå¦‚æœæŸäº›é”®ä¸åŒ¹é…ï¼Œå¯ä»¥è®¾ç½®**\_\_**=False è·³è¿‡ã€‚

### 7. ä½¿ç”¨ `torch.cat` æ²¿ç°æœ‰ç»´åº¦æ‹¼æ¥ï¼Œä½¿ç”¨ `torch.______` æ²¿æ–°ç»´åº¦å †å ã€‚

### 8. AdamW ç›¸æ¯” Adam çš„æ”¹è¿›æ˜¯**\_\_**æƒé‡è¡°å‡ã€‚

---

## ä¸‰ã€è®¡ç®—é¢˜ (æ¯é¢˜ 8 åˆ†ï¼Œå…± 24 åˆ†)

### 1. Tensor å½¢çŠ¶æ¨æ–­

ç»™å®šä»¥ä¸‹ä»£ç ï¼Œå†™å‡ºæ¯æ­¥çš„è¾“å‡ºå½¢çŠ¶ï¼š

```python
x = torch.randn(4, 3, 32, 32)  # è¾“å…¥
conv = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)
pool = nn.MaxPool2d(2)
fc = nn.Linear(64 * 4 * 4, 10)

a = conv(x)    # a çš„å½¢çŠ¶ï¼Ÿ
b = pool(a)    # b çš„å½¢çŠ¶ï¼Ÿ
c = b.flatten(1)  # c çš„å½¢çŠ¶ï¼Ÿ
d = fc(c)      # d çš„å½¢çŠ¶ï¼Ÿ
```

### 2. æ¢¯åº¦è®¡ç®—

```python
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x[0] ** 2 + 2 * x[1]
y.backward()
```

é—®ï¼š`x.grad` çš„å€¼æ˜¯å¤šå°‘ï¼Ÿå†™å‡ºè®¡ç®—è¿‡ç¨‹ã€‚

### 3. è®­ç»ƒä»£ç è¡¥å…¨

è¡¥å…¨ä»¥ä¸‹è®­ç»ƒå¾ªç¯ä¸­çš„ç¼ºå¤±éƒ¨åˆ†ï¼š

```python
model = MyModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    model.______()  # (a)
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)

        ______.zero_grad()  # (b)
        output = model(x)
        loss = ______(output, y)  # (c)
        loss.______()  # (d)
        optimizer.______()  # (e)

    model.______()  # (f)
    with torch.______():  # (g)
        for x, y in val_loader:
            # éªŒè¯...
```

---

## å››ã€ç®€ç­”é¢˜ (æ¯é¢˜ 6 åˆ†ï¼Œå…± 12 åˆ†)

### 1. è§£é‡Š `model.train()` å’Œ `model.eval()` çš„åŒºåˆ«ï¼Œä»¥åŠä¸ºä»€ä¹ˆåœ¨éªŒè¯æ—¶éœ€è¦é…åˆ `torch.no_grad()` ä½¿ç”¨ï¼Ÿ

### 2. æ¯”è¾ƒ `torch.save(model, ...)` å’Œ `torch.save(model.state_dict(), ...)` ä¸¤ç§ä¿å­˜æ–¹å¼çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆæ¨èåè€…ï¼Ÿ

---

## äº”ã€ç¼–ç¨‹é¢˜ (10 åˆ†)

å®ç°ä¸€ä¸ªç®€å•çš„ CNN åˆ†ç±»å™¨ï¼Œè¦æ±‚ï¼š

- è¾“å…¥ï¼š(batch, 1, 28, 28) çš„ç°åº¦å›¾åƒ
- ç½‘ç»œç»“æ„ï¼š
  - Conv2d(1, 32, 3, padding=1) + ReLU + MaxPool2d(2)
  - Conv2d(32, 64, 3, padding=1) + ReLU + MaxPool2d(2)
  - Flatten + Linear(64*7*7, 128) + ReLU + Dropout(0.5)
  - Linear(128, 10)
- è¾“å‡ºï¼š10 ä¸ªç±»åˆ«çš„ logits

```python
import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # TODO: å®ç°ç½‘ç»œç»“æ„
        pass

    def forward(self, x):
        # TODO: å®ç°å‰å‘ä¼ æ’­
        pass
```

---

# å‚è€ƒç­”æ¡ˆ

## ä¸€ã€é€‰æ‹©é¢˜ç­”æ¡ˆ

| é¢˜å· | ç­”æ¡ˆ | è§£æ                                                |
| ---- | ---- | --------------------------------------------------- |
| 1    | B    | `from_numpy` ä¸ NumPy å…±äº«å†…å­˜ï¼Œå…¶ä»–ä¼šå¤åˆ¶æ•°æ®      |
| 2    | C    | æ–° Tensor é»˜è®¤ä¸éœ€è¦æ¢¯åº¦ï¼Œæ¢¯åº¦ä¸ä¼šè‡ªåŠ¨æ¸…é›¶          |
| 3    | C    | (32, 64) @ (64, 128) = (32, 128)                    |
| 4    | C    | `contiguous()` åªæ”¹å˜å†…å­˜å¸ƒå±€ï¼Œä¸æ”¹å˜å½¢çŠ¶           |
| 5    | A    | å¿…é¡»å®ç° `__init__` å’Œ `forward`ï¼Œbackward è‡ªåŠ¨ç”Ÿæˆ |
| 6    | C    | CrossEntropyLoss å†…éƒ¨åŒ…å« Softmaxï¼Œè¾“å…¥åº”æ˜¯ logits  |
| 7    | C    | `eval()` ä¸å½±å“æ¢¯åº¦è®¡ç®—ï¼Œéœ€è¦é¢å¤–ç”¨ `no_grad()`     |
| 8    | B    | åº”åœ¨ `backward()` ä¹‹å‰æ¸…é›¶ï¼Œå¦åˆ™æ¢¯åº¦ä¼šç´¯åŠ           |
| 9    | C    | `learning_rate` æ˜¯ä¼˜åŒ–å™¨å‚æ•°ï¼Œä¸æ˜¯ DataLoader å‚æ•°  |
| 10   | C    | MixUp éœ€è¦åœ¨ DataLoader å¤–éƒ¨å¯¹æ ‡ç­¾è¿›è¡Œæ··åˆ          |
| 11   | B    | åªä¿å­˜ state_dict æ›´çµæ´»ã€æ›´å°ã€æ›´å…¼å®¹              |
| 12   | C    | ReduceLROnPlateau éœ€è¦ä¼ å…¥éªŒè¯ loss æˆ– acc          |
| 13   | B    | ç¦æ­¢æ¢¯åº¦è®¡ç®—ï¼Œä¸æ„å»ºè®¡ç®—å›¾ï¼ŒèŠ‚çœå†…å­˜                |
| 14   | C    | `bmm` ä¸“é—¨ç”¨äºæ‰¹é‡çŸ©é˜µä¹˜æ³• (3D tensors)             |
| 15   | C    | æ¢¯åº¦è£å‰ªåœ¨ backward åã€step å‰è°ƒç”¨ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸   |

---

## äºŒã€å¡«ç©ºé¢˜ç­”æ¡ˆ

1. **å‰å‘**ï¼Œ**åå‘**
2. **cuda()**ï¼Œ**'cuda'** æˆ– **device**
3. **`__getitem__`**
4. **zero_grad**ï¼Œ**backward**ï¼Œ**step**
5. **Sigmoid**
6. **strict**
7. **stack**
8. **è§£è€¦** (decoupled)

---

## ä¸‰ã€è®¡ç®—é¢˜ç­”æ¡ˆ

### 1. Tensor å½¢çŠ¶æ¨æ–­

```
è¾“å…¥ x: (4, 3, 32, 32)

a = conv(x):
  H_out = (32 + 2*1 - 3) / 2 + 1 = 16
  å½¢çŠ¶: (4, 64, 16, 16)

b = pool(a):
  å½¢çŠ¶: (4, 64, 8, 8)

c = b.flatten(1):
  å½¢çŠ¶: (4, 64*8*8) = (4, 4096)

d = fc(c):
  ä½† fc æœŸæœ›è¾“å…¥ 64*4*4=1024ï¼Œä¸ 4096 ä¸åŒ¹é…ï¼
  å¦‚æœç½‘ç»œè®¾è®¡æ­£ç¡®åº”è¯¥æ˜¯ nn.Linear(64*8*8, 10)
  ä¿®æ­£å: (4, 10)
```

### 2. æ¢¯åº¦è®¡ç®—

```
y = x[0]Â² + 2*x[1]
y = (2.0)Â² + 2*(3.0) = 4 + 6 = 10

âˆ‚y/âˆ‚x[0] = 2*x[0] = 2*2.0 = 4.0
âˆ‚y/âˆ‚x[1] = 2

x.grad = tensor([4.0, 2.0])
```

### 3. è®­ç»ƒä»£ç è¡¥å…¨

```python
(a) train        # è®¾ç½®è®­ç»ƒæ¨¡å¼
(b) optimizer    # æ¸…é›¶æ¢¯åº¦
(c) criterion    # è®¡ç®—æŸå¤±
(d) backward     # åå‘ä¼ æ’­
(e) step         # æ›´æ–°å‚æ•°
(f) eval         # è®¾ç½®è¯„ä¼°æ¨¡å¼
(g) no_grad      # ç¦ç”¨æ¢¯åº¦è®¡ç®—
```

---

## å››ã€ç®€ç­”é¢˜ç­”æ¡ˆ

### 1. train() å’Œ eval() çš„åŒºåˆ«

**åŒºåˆ«ï¼š**

- `train()`: å¯ç”¨ Dropoutï¼ˆéšæœºç½®é›¶ï¼‰å’Œ BatchNormï¼ˆä½¿ç”¨ batch ç»Ÿè®¡é‡ï¼‰
- `eval()`: ç¦ç”¨ Dropoutï¼ŒBatchNorm ä½¿ç”¨è¿è¡Œæ—¶ç»Ÿè®¡é‡

**ä¸ºä»€ä¹ˆé…åˆ no_grad()ï¼š**

- `eval()` åªæ”¹å˜æŸäº›å±‚çš„è¡Œä¸ºï¼Œä¸å½±å“æ¢¯åº¦è®¡ç®—
- `no_grad()` ç¦æ­¢æ„å»ºè®¡ç®—å›¾ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—
- éªŒè¯æ—¶ä¸éœ€è¦åå‘ä¼ æ’­ï¼Œä½¿ç”¨ `no_grad()` æ›´é«˜æ•ˆ
- ä¸¤è€…ä½œç”¨ä¸åŒï¼Œéœ€è¦åŒæ—¶ä½¿ç”¨

### 2. ä¸¤ç§ä¿å­˜æ–¹å¼å¯¹æ¯”

**ä¿å­˜æ•´ä¸ªæ¨¡å‹ `torch.save(model, ...)`ï¼š**

- ä¼˜ç‚¹ï¼šç®€å•ï¼Œä¸€è¡Œä»£ç 
- ç¼ºç‚¹ï¼š
  - ä¿å­˜äº†ç±»å®šä¹‰ï¼Œä¾èµ–ç‰¹å®šè·¯å¾„
  - åŠ è½½æ—¶éœ€è¦èƒ½è®¿é—®åŸå§‹ç±»å®šä¹‰
  - æ–‡ä»¶æ›´å¤§ï¼Œå…¼å®¹æ€§å·®

**åªä¿å­˜å‚æ•° `torch.save(model.state_dict(), ...)`ï¼š**

- ä¼˜ç‚¹ï¼š
  - æ–‡ä»¶æ›´å°
  - æ›´çµæ´»ï¼Œå¯ç”¨äºä¸åŒæ¨¡å‹ç»“æ„
  - æ›´å¥½çš„è·¨ç‰ˆæœ¬å…¼å®¹æ€§
  - æ›´é€‚åˆè¿ç§»å­¦ä¹ ï¼ˆéƒ¨åˆ†åŠ è½½ï¼‰
- ç¼ºç‚¹ï¼šåŠ è½½æ—¶éœ€è¦å…ˆåˆ›å»ºæ¨¡å‹å®ä¾‹

**æ¨èåè€…çš„åŸå› ï¼š** æ›´çµæ´»ã€æ›´å°ã€æ›´å…¼å®¹ï¼Œç¬¦åˆæœ€ä½³å®è·µã€‚

---

## äº”ã€ç¼–ç¨‹é¢˜ç­”æ¡ˆ

```python
import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)

        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(2)

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

        # Dropout
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—: (B, 1, 28, 28) -> (B, 32, 14, 14)
        x = self.pool(torch.relu(self.conv1(x)))

        # ç¬¬äºŒä¸ªå·ç§¯å—: (B, 32, 14, 14) -> (B, 64, 7, 7)
        x = self.pool(torch.relu(self.conv2(x)))

        # å±•å¹³: (B, 64, 7, 7) -> (B, 64*7*7)
        x = x.flatten(1)

        # å…¨è¿æ¥å±‚
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

# æµ‹è¯•
model = SimpleCNN()
x = torch.randn(4, 1, 28, 28)
y = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")  # torch.Size([4, 10])
```

---

## ğŸ“Š è¯„åˆ†æ ‡å‡†

| éƒ¨åˆ†     | åˆ†å€¼       | åŠæ ¼çº¿    |
| -------- | ---------- | --------- |
| é€‰æ‹©é¢˜   | 30 åˆ†      | 18 åˆ†     |
| å¡«ç©ºé¢˜   | 24 åˆ†      | 15 åˆ†     |
| è®¡ç®—é¢˜   | 24 åˆ†      | 12 åˆ†     |
| ç®€ç­”é¢˜   | 12 åˆ†      | 6 åˆ†      |
| ç¼–ç¨‹é¢˜   | 10 åˆ†      | 5 åˆ†      |
| **æ€»åˆ†** | **100 åˆ†** | **60 åˆ†** |
