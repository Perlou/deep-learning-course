# PyTorch æ·±åº¦è§£æï¼šä»é›¶å¼€å§‹çš„å®Œæ•´æŒ‡å—

---

## ğŸ“‘ ç›®å½•

1. [PyTorch ç®€ä»‹](#1-pytorch-ç®€ä»‹)
2. [å®‰è£…ä¸ç¯å¢ƒé…ç½®](#2-å®‰è£…ä¸ç¯å¢ƒé…ç½®)
3. [å¼ é‡(Tensor)è¯¦è§£](#3-å¼ é‡tensorè¯¦è§£)
4. [è‡ªåŠ¨æ±‚å¯¼(Autograd)æœºåˆ¶](#4-è‡ªåŠ¨æ±‚å¯¼autogradæœºåˆ¶)
5. [ç¥ç»ç½‘ç»œæ¨¡å—(nn.Module)](#5-ç¥ç»ç½‘ç»œæ¨¡å—nnmodule)
6. [æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨](#6-æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨)
7. [æ•°æ®åŠ è½½ä¸å¤„ç†](#7-æ•°æ®åŠ è½½ä¸å¤„ç†)
8. [å®Œæ•´è®­ç»ƒæµç¨‹](#8-å®Œæ•´è®­ç»ƒæµç¨‹)
9. [æ¨¡å‹ä¿å­˜ä¸åŠ è½½](#9-æ¨¡å‹ä¿å­˜ä¸åŠ è½½)
10. [GPU åŠ é€Ÿ](#10-gpu-åŠ é€Ÿ)
11. [é«˜çº§ä¸»é¢˜](#11-é«˜çº§ä¸»é¢˜)
12. [å®æˆ˜æ¡ˆä¾‹](#12-å®æˆ˜æ¡ˆä¾‹)
13. [æœ€ä½³å®è·µä¸è°ƒè¯•æŠ€å·§](#13-æœ€ä½³å®è·µä¸è°ƒè¯•æŠ€å·§)

---

## 1. PyTorch ç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯ PyTorch

PyTorch æ˜¯ä¸€ä¸ªå¼€æºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”± Facebookï¼ˆç° Metaï¼‰çš„ AI ç ”ç©¶å›¢é˜Ÿå¼€å‘ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PyTorch æ¶æ„                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ torchvision â”‚  â”‚  torchaudio â”‚  â”‚     torchtext       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    torch.nn                            â”‚  â”‚
â”‚  â”‚    (ç¥ç»ç½‘ç»œå±‚ã€æŸå¤±å‡½æ•°ã€å®¹å™¨)                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  torch.autograd                        â”‚  â”‚
â”‚  â”‚              (è‡ªåŠ¨å¾®åˆ†å¼•æ“)                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   torch.Tensor                         â”‚  â”‚
â”‚  â”‚              (å¤šç»´æ•°ç»„/å¼ é‡)                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              CUDA / CPU Backend                        â”‚  â”‚
â”‚  â”‚            (åº•å±‚è®¡ç®—å¼•æ“)                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 PyTorch vs TensorFlow å¯¹æ¯”

| ç‰¹æ€§     | PyTorch                  | TensorFlow                   |
| -------- | ------------------------ | ---------------------------- |
| è®¡ç®—å›¾   | åŠ¨æ€å›¾ï¼ˆé»˜è®¤ï¼‰           | é™æ€å›¾ï¼ˆ1.xï¼‰/ åŠ¨æ€å›¾ï¼ˆ2.xï¼‰ |
| è°ƒè¯•     | ç®€å•ï¼Œå¯ç”¨ Python è°ƒè¯•å™¨ | ç›¸å¯¹å¤æ‚                     |
| å­¦ä¹ æ›²çº¿ | è¾ƒå¹³ç¼“                   | è¾ƒé™¡å³­                       |
| éƒ¨ç½²     | TorchScript / ONNX       | TensorFlow Serving / TFLite  |
| ç¤¾åŒº     | å­¦æœ¯ç•Œä¸»æµ               | å·¥ä¸šç•Œå¹¿æ³›                   |

### 1.3 æ ¸å¿ƒç‰¹æ€§

```python
# PyTorch çš„æ ¸å¿ƒç‰¹æ€§æ¼”ç¤º
import torch

# 1. åŠ¨æ€è®¡ç®—å›¾ - æ¯æ¬¡å‰å‘ä¼ æ’­æ„å»ºæ–°å›¾
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x ** 2  # è®¡ç®—å›¾åœ¨æ­¤åŠ¨æ€æ„å»º
y.sum().backward()

# 2. Pythonic - ä¸ Python ç”Ÿæ€æ— ç¼é›†æˆ
import numpy as np
numpy_array = np.array([1, 2, 3])
tensor = torch.from_numpy(numpy_array)

# 3. GPU åŠ é€Ÿ - ç®€å•çš„è®¾å¤‡åˆ‡æ¢
if torch.cuda.is_available():
    tensor = tensor.cuda()
```

---

## 2. å®‰è£…ä¸ç¯å¢ƒé…ç½®

### 2.1 å®‰è£…æ–¹æ³•

```bash
# CPU ç‰ˆæœ¬
pip install torch torchvision torchaudio

# CUDA 11.8 ç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1 ç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Conda å®‰è£…
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
```

### 2.2 éªŒè¯å®‰è£…

```python
import torch

# åŸºæœ¬ä¿¡æ¯
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
print(f"GPU æ•°é‡: {torch.cuda.device_count()}")

if torch.cuda.is_available():
    print(f"å½“å‰ GPU: {torch.cuda.get_device_name(0)}")
```

---

## 3. å¼ é‡(Tensor)è¯¦è§£

### 3.1 å¼ é‡åŸºç¡€

å¼ é‡æ˜¯ PyTorch ä¸­æœ€æ ¸å¿ƒçš„æ•°æ®ç»“æ„ï¼Œæ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å¼ é‡ç»´åº¦ç¤ºæ„                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   æ ‡é‡ (0-D)      å‘é‡ (1-D)      çŸ©é˜µ (2-D)      3-Då¼ é‡    â”‚
â”‚                                                             â”‚
â”‚      5            [1,2,3]        [[1,2],         [[[1,2],   â”‚
â”‚                                   [3,4]]           [3,4]],  â”‚
â”‚                                                   [[5,6],   â”‚
â”‚     ( )           â”€â”€â”€â”€â”€          â”Œâ”€â”€â”€â”            [7,8]]]   â”‚
â”‚                                  â”‚   â”‚                      â”‚
â”‚                                  â””â”€â”€â”€â”˜           â”Œâ”€â”€â”€â”€â”€â”    â”‚
â”‚                                                  â”‚â”Œâ”€â”€â”€â”â”‚    â”‚
â”‚                                                  â”‚â”‚   â”‚â”‚    â”‚
â”‚                                                  â”‚â””â”€â”€â”€â”˜â”‚    â”‚
â”‚                                                  â””â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 åˆ›å»ºå¼ é‡

```python
import torch

# ==================== 1. ä»æ•°æ®åˆ›å»º ====================
# ä»åˆ—è¡¨åˆ›å»º
t1 = torch.tensor([1, 2, 3])
t2 = torch.tensor([[1, 2], [3, 4]])

# ä» NumPy åˆ›å»ºï¼ˆå…±äº«å†…å­˜ï¼‰
import numpy as np
np_array = np.array([1, 2, 3])
t3 = torch.from_numpy(np_array)

# ä»å¦ä¸€ä¸ªå¼ é‡åˆ›å»º
t4 = torch.tensor(t1)           # å¤åˆ¶æ•°æ®
t5 = t1.clone()                 # å…‹éš†
t6 = t1.detach()                # åˆ†ç¦»è®¡ç®—å›¾

# ==================== 2. ç‰¹æ®Šå¼ é‡ ====================
# å…¨é›¶/å…¨ä¸€
zeros = torch.zeros(3, 4)       # 3x4 å…¨é›¶çŸ©é˜µ
ones = torch.ones(3, 4)         # 3x4 å…¨ä¸€çŸ©é˜µ
full = torch.full((3, 4), 7)    # 3x4 å…¨7çŸ©é˜µ

# å•ä½çŸ©é˜µ
eye = torch.eye(4)              # 4x4 å•ä½çŸ©é˜µ

# æœªåˆå§‹åŒ–ï¼ˆéšæœºå€¼ï¼Œé€Ÿåº¦å¿«ï¼‰
empty = torch.empty(3, 4)

# ==================== 3. éšæœºå¼ é‡ ====================
# å‡åŒ€åˆ†å¸ƒ [0, 1)
rand = torch.rand(3, 4)

# æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0, 1)
randn = torch.randn(3, 4)

# æŒ‡å®šèŒƒå›´çš„éšæœºæ•´æ•°
randint = torch.randint(0, 10, (3, 4))  # [0, 10) çš„æ•´æ•°

# æŒ‡å®šæ­£æ€åˆ†å¸ƒ
normal = torch.normal(mean=0, std=1, size=(3, 4))

# ==================== 4. åºåˆ—å¼ é‡ ====================
# ç­‰å·®åºåˆ—
arange = torch.arange(0, 10, 2)         # [0, 2, 4, 6, 8]
linspace = torch.linspace(0, 1, 5)      # [0, 0.25, 0.5, 0.75, 1]
logspace = torch.logspace(0, 2, 3)      # [1, 10, 100]

# ==================== 5. ç±»ä¼¼å½¢çŠ¶çš„å¼ é‡ ====================
x = torch.tensor([[1, 2], [3, 4]])
zeros_like = torch.zeros_like(x)        # ç›¸åŒå½¢çŠ¶çš„å…¨é›¶
ones_like = torch.ones_like(x)          # ç›¸åŒå½¢çŠ¶çš„å…¨ä¸€
rand_like = torch.rand_like(x.float())  # ç›¸åŒå½¢çŠ¶çš„éšæœº
```

### 3.3 å¼ é‡å±æ€§

```python
import torch

t = torch.randn(3, 4, 5)

# åŸºæœ¬å±æ€§
print(f"å½¢çŠ¶: {t.shape}")           # torch.Size([3, 4, 5])
print(f"ç»´åº¦: {t.dim()}")           # 3
print(f"å…ƒç´ æ€»æ•°: {t.numel()}")     # 60
print(f"æ•°æ®ç±»å‹: {t.dtype}")       # torch.float32
print(f"è®¾å¤‡: {t.device}")          # cpu
print(f"æ˜¯å¦éœ€è¦æ¢¯åº¦: {t.requires_grad}")  # False

# æ­¥é•¿ï¼ˆå†…å­˜å¸ƒå±€ï¼‰
print(f"æ­¥é•¿: {t.stride()}")        # (20, 5, 1)
print(f"æ˜¯å¦è¿ç»­: {t.is_contiguous()}")  # True
```

### 3.4 æ•°æ®ç±»å‹

```python
# PyTorch æ•°æ®ç±»å‹
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ•°æ®ç±»å‹å¯¹ç…§è¡¨                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    PyTorch       â”‚    Python/NumPy  â”‚        è¯´æ˜            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ torch.float16    â”‚ np.float16      â”‚ åŠç²¾åº¦æµ®ç‚¹             â”‚
â”‚ torch.float32    â”‚ np.float32      â”‚ å•ç²¾åº¦æµ®ç‚¹ï¼ˆé»˜è®¤ï¼‰      â”‚
â”‚ torch.float64    â”‚ np.float64      â”‚ åŒç²¾åº¦æµ®ç‚¹             â”‚
â”‚ torch.int8       â”‚ np.int8         â”‚ 8ä½æœ‰ç¬¦å·æ•´æ•°           â”‚
â”‚ torch.int16      â”‚ np.int16        â”‚ 16ä½æœ‰ç¬¦å·æ•´æ•°          â”‚
â”‚ torch.int32      â”‚ np.int32        â”‚ 32ä½æœ‰ç¬¦å·æ•´æ•°          â”‚
â”‚ torch.int64      â”‚ np.int64        â”‚ 64ä½æœ‰ç¬¦å·æ•´æ•°ï¼ˆé»˜è®¤ï¼‰   â”‚
â”‚ torch.bool       â”‚ np.bool_        â”‚ å¸ƒå°”ç±»å‹               â”‚
â”‚ torch.complex64  â”‚ np.complex64    â”‚ å¤æ•°ï¼ˆå®è™šéƒ¨å„32ä½ï¼‰    â”‚
â”‚ torch.complex128 â”‚ np.complex128   â”‚ å¤æ•°ï¼ˆå®è™šéƒ¨å„64ä½ï¼‰    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# ç±»å‹è½¬æ¢
t = torch.tensor([1, 2, 3])
t_float = t.float()               # è½¬æ¢ä¸º float32
t_double = t.double()             # è½¬æ¢ä¸º float64
t_int = t.int()                   # è½¬æ¢ä¸º int32
t_long = t.long()                 # è½¬æ¢ä¸º int64
t_bool = t.bool()                 # è½¬æ¢ä¸º bool

# ä½¿ç”¨ to() æ–¹æ³•
t_float16 = t.to(torch.float16)
t_cuda = t.to('cuda')             # ç§»åŠ¨åˆ° GPU
t_cpu = t.to('cpu')               # ç§»åŠ¨åˆ° CPU
```

### 3.5 å¼ é‡æ“ä½œ

#### 3.5.1 ç´¢å¼•ä¸åˆ‡ç‰‡

```python
import torch

t = torch.arange(12).reshape(3, 4)
print(t)
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])

# åŸºæœ¬ç´¢å¼•
print(t[0])           # ç¬¬ä¸€è¡Œ: tensor([0, 1, 2, 3])
print(t[0, 1])        # ç¬¬ä¸€è¡Œç¬¬äºŒåˆ—: tensor(1)
print(t[:, 0])        # ç¬¬ä¸€åˆ—: tensor([0, 4, 8])
print(t[1:, :2])      # ç¬¬2è¡Œèµ·ï¼Œå‰2åˆ—

# é«˜çº§ç´¢å¼•
indices = torch.tensor([0, 2])
print(t[indices])     # ç¬¬1ã€3è¡Œ

# å¸ƒå°”ç´¢å¼•
mask = t > 5
print(t[mask])        # tensor([ 6,  7,  8,  9, 10, 11])

# ä¿®æ”¹å€¼
t[0, 0] = 100
t[:, -1] = 0          # æœ€åä¸€åˆ—ç½®0
```

#### 3.5.2 å½¢çŠ¶æ“ä½œ

```python
import torch

t = torch.arange(12)

# reshapeï¼šæ”¹å˜å½¢çŠ¶ï¼ˆå¯èƒ½è¿”å›è§†å›¾æˆ–å‰¯æœ¬ï¼‰
t1 = t.reshape(3, 4)
t2 = t.reshape(2, -1)     # -1 è‡ªåŠ¨è®¡ç®—

# viewï¼šæ”¹å˜å½¢çŠ¶ï¼ˆå¿…é¡»è¿ç»­ï¼Œè¿”å›è§†å›¾ï¼‰
t3 = t.view(3, 4)

# flattenï¼šå±•å¹³
t4 = t1.flatten()         # å®Œå…¨å±•å¹³
t5 = t1.flatten(0, 1)     # å±•å¹³æŒ‡å®šç»´åº¦

# squeeze/unsqueezeï¼šå‹ç¼©/æ‰©å±•ç»´åº¦
t6 = torch.zeros(1, 3, 1, 4)
print(t6.squeeze().shape)        # torch.Size([3, 4])
print(t6.squeeze(0).shape)       # torch.Size([3, 1, 4])

t7 = torch.zeros(3, 4)
print(t7.unsqueeze(0).shape)     # torch.Size([1, 3, 4])
print(t7.unsqueeze(-1).shape)    # torch.Size([3, 4, 1])

# transpose/permuteï¼šè½¬ç½®
t8 = torch.randn(2, 3, 4)
print(t8.transpose(0, 2).shape)  # torch.Size([4, 3, 2])
print(t8.permute(2, 0, 1).shape) # torch.Size([4, 2, 3])
```

#### 3.5.3 æ•°å­¦è¿ç®—

```python
import torch

a = torch.tensor([1., 2., 3.])
b = torch.tensor([4., 5., 6.])

# ============ åŸºæœ¬è¿ç®— ============
print(a + b)          # åŠ æ³•
print(a - b)          # å‡æ³•
print(a * b)          # é€å…ƒç´ ä¹˜æ³•
print(a / b)          # é™¤æ³•
print(a ** 2)         # å¹‚è¿ç®—
print(a % 2)          # å–æ¨¡

# ============ çŸ©é˜µè¿ç®— ============
m1 = torch.randn(2, 3)
m2 = torch.randn(3, 4)

# çŸ©é˜µä¹˜æ³•
result = m1 @ m2                    # æ¨è
result = torch.mm(m1, m2)           # 2DçŸ©é˜µ
result = torch.matmul(m1, m2)       # é€šç”¨

# æ‰¹é‡çŸ©é˜µä¹˜æ³•
batch_m1 = torch.randn(10, 2, 3)
batch_m2 = torch.randn(10, 3, 4)
result = torch.bmm(batch_m1, batch_m2)  # (10, 2, 4)

# ç‚¹ç§¯
dot = torch.dot(a, b)

# ============ èšåˆè¿ç®— ============
t = torch.tensor([[1., 2., 3.], [4., 5., 6.]])

print(t.sum())            # æ‰€æœ‰å…ƒç´ æ±‚å’Œ: 21
print(t.sum(dim=0))       # æŒ‰åˆ—æ±‚å’Œ: [5, 7, 9]
print(t.sum(dim=1))       # æŒ‰è¡Œæ±‚å’Œ: [6, 15]
print(t.mean())           # å‡å€¼
print(t.std())            # æ ‡å‡†å·®
print(t.var())            # æ–¹å·®
print(t.max())            # æœ€å¤§å€¼
print(t.min())            # æœ€å°å€¼
print(t.argmax())         # æœ€å¤§å€¼ç´¢å¼•
print(t.argmin())         # æœ€å°å€¼ç´¢å¼•

# ============ æ•°å­¦å‡½æ•° ============
x = torch.tensor([1., 2., 3.])
print(torch.exp(x))       # æŒ‡æ•°
print(torch.log(x))       # å¯¹æ•°
print(torch.sqrt(x))      # å¹³æ–¹æ ¹
print(torch.abs(x))       # ç»å¯¹å€¼
print(torch.sin(x))       # æ­£å¼¦
print(torch.cos(x))       # ä½™å¼¦
print(torch.tanh(x))      # åŒæ›²æ­£åˆ‡

# ============ æ¯”è¾ƒè¿ç®— ============
print(a > b)              # é€å…ƒç´ æ¯”è¾ƒ
print(a == b)
print(torch.eq(a, b))     # ç›¸ç­‰
print(torch.gt(a, b))     # å¤§äº
print(torch.lt(a, b))     # å°äº
print(torch.all(a > 0))   # å…¨éƒ¨æ»¡è¶³
print(torch.any(a > 2))   # å­˜åœ¨æ»¡è¶³
```

#### 3.5.4 å¹¿æ’­æœºåˆ¶

```python
import torch

"""
å¹¿æ’­è§„åˆ™ï¼š
1. å¦‚æœä¸¤ä¸ªå¼ é‡ç»´åº¦ä¸åŒï¼Œåœ¨ç»´åº¦å°‘çš„å¼ é‡å‰é¢è¡¥1
2. ä»å³å‘å·¦æ¯”è¾ƒå„ç»´åº¦ï¼Œç»´åº¦ç›¸åŒæˆ–å…¶ä¸­ä¸€ä¸ªä¸º1æ—¶å¯ä»¥å¹¿æ’­
3. ä¸º1çš„ç»´åº¦ä¼šæ‰©å±•æˆä¸å¦ä¸€ä¸ªå¼ é‡ç›¸åŒ

ç¤ºä¾‹:
    a: (3, 4)
    b: (   4)  â†’ è¡¥é½ä¸º (1, 4) â†’ å¹¿æ’­ä¸º (3, 4)
    ç»“æœ: (3, 4)
"""

# ç¤ºä¾‹1ï¼šå‘é‡ä¸çŸ©é˜µ
a = torch.ones(3, 4)
b = torch.tensor([1, 2, 3, 4])
print((a + b).shape)  # torch.Size([3, 4])

# ç¤ºä¾‹2ï¼šåˆ—å‘é‡ä¸è¡Œå‘é‡
row = torch.tensor([[1, 2, 3]])       # (1, 3)
col = torch.tensor([[1], [2], [3]])   # (3, 1)
print((row + col).shape)  # torch.Size([3, 3])
# ç»“æœ:
# [[2, 3, 4],
#  [3, 4, 5],
#  [4, 5, 6]]

# ç¤ºä¾‹3ï¼š3Då¹¿æ’­
a = torch.randn(2, 3, 4)
b = torch.randn(   3, 1)
print((a + b).shape)  # torch.Size([2, 3, 4])
```

#### 3.5.5 æ‹¼æ¥ä¸åˆ†å‰²

```python
import torch

a = torch.tensor([[1, 2], [3, 4]])
b = torch.tensor([[5, 6], [7, 8]])

# ============ æ‹¼æ¥ ============
# cat: æ²¿ç°æœ‰ç»´åº¦æ‹¼æ¥
cat_0 = torch.cat([a, b], dim=0)  # (4, 2) å‚ç›´æ‹¼æ¥
cat_1 = torch.cat([a, b], dim=1)  # (2, 4) æ°´å¹³æ‹¼æ¥

# stack: æ²¿æ–°ç»´åº¦å †å 
stack = torch.stack([a, b], dim=0)  # (2, 2, 2)

# ============ åˆ†å‰² ============
t = torch.arange(12).reshape(4, 3)

# split: æŒ‰å¤§å°åˆ†å‰²
parts = torch.split(t, 2, dim=0)     # æ¯ä»½2è¡Œ
parts = torch.split(t, [1, 3], dim=0) # ç¬¬1ä»½1è¡Œï¼Œç¬¬2ä»½3è¡Œ

# chunk: åˆ†æˆnä»½
chunks = torch.chunk(t, 2, dim=0)    # åˆ†æˆ2ä»½

# unbind: æ²¿ç»´åº¦è§£å¼€ï¼ˆç§»é™¤è¯¥ç»´åº¦ï¼‰
rows = torch.unbind(t, dim=0)        # è¿”å›4ä¸ª1Då¼ é‡
```

---

## 4. è‡ªåŠ¨æ±‚å¯¼(Autograd)æœºåˆ¶

### 4.1 è®¡ç®—å›¾åŸç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è®¡ç®—å›¾ç¤ºæ„                                  â”‚
â”‚                                                                 â”‚
â”‚    å‰å‘ä¼ æ’­ (Forward Pass)                                       â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                                       â”‚
â”‚                                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ x â”‚â”€â”€â”€â”€â”€â–ºâ”‚  y=   â”‚â”€â”€â”€â”€â”€â–ºâ”‚  z=   â”‚â”€â”€â”€â”€â”€â–ºâ”‚  L=   â”‚          â”‚
â”‚    â”‚   â”‚      â”‚ x*w+b â”‚      â”‚ ReLU  â”‚      â”‚ loss  â”‚          â”‚
â”‚    â””â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚      â–²            â”‚              â”‚              â”‚               â”‚
â”‚      â”‚            â–¼              â–¼              â–¼               â”‚
â”‚    â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚âˆ‚L/â”‚â—„â”€â”€â”€â”€â”€â”‚ âˆ‚L/âˆ‚y â”‚â—„â”€â”€â”€â”€â”€â”‚ âˆ‚L/âˆ‚z â”‚â—„â”€â”€â”€â”€â”€â”‚ âˆ‚L/âˆ‚L â”‚          â”‚
â”‚    â”‚âˆ‚x â”‚      â”‚       â”‚      â”‚       â”‚      â”‚  = 1  â”‚          â”‚
â”‚    â””â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚    â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚    åå‘ä¼ æ’­ (Backward Pass)                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 è‡ªåŠ¨æ±‚å¯¼åŸºç¡€

```python
import torch

# ============ åŸºæœ¬ç”¨æ³• ============
# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = torch.tensor([4., 5., 6.], requires_grad=True)

# è®¡ç®—
z = x * y
out = z.sum()

# åå‘ä¼ æ’­
out.backward()

# æŸ¥çœ‹æ¢¯åº¦
print(f"x.grad: {x.grad}")  # tensor([4., 5., 6.])
print(f"y.grad: {y.grad}")  # tensor([1., 2., 3.])

# ============ æ¢¯åº¦ç´¯ç§¯ ============
# é»˜è®¤æƒ…å†µä¸‹æ¢¯åº¦ä¼šç´¯ç§¯ï¼Œéœ€è¦æ‰‹åŠ¨æ¸…é›¶
x = torch.tensor([1.], requires_grad=True)

for i in range(3):
    y = x ** 2
    y.backward()
    print(f"ç¬¬{i+1}æ¬¡: x.grad = {x.grad}")
    # ç¬¬1æ¬¡: 2, ç¬¬2æ¬¡: 4, ç¬¬3æ¬¡: 6 (ç´¯ç§¯!)

# æ­£ç¡®åšæ³•ï¼šæ¸…é›¶æ¢¯åº¦
x = torch.tensor([1.], requires_grad=True)
for i in range(3):
    if x.grad is not None:
        x.grad.zero_()  # æ¸…é›¶
    y = x ** 2
    y.backward()
    print(f"ç¬¬{i+1}æ¬¡: x.grad = {x.grad}")  # æ¯æ¬¡éƒ½æ˜¯2
```

### 4.3 æ§åˆ¶æ¢¯åº¦è®¡ç®—

```python
import torch

# ============ ç¦ç”¨æ¢¯åº¦è¿½è¸ª ============
x = torch.tensor([1., 2., 3.], requires_grad=True)

# æ–¹æ³•1ï¼šwith torch.no_grad()
with torch.no_grad():
    y = x * 2
    print(y.requires_grad)  # False

# æ–¹æ³•2ï¼šè£…é¥°å™¨
@torch.no_grad()
def inference(x):
    return x * 2

# æ–¹æ³•3ï¼šdetach()
y = x.detach()
print(y.requires_grad)  # False

# ============ æ¨ç†æ¨¡å¼ï¼ˆæ›´é«˜æ•ˆï¼‰============
with torch.inference_mode():
    y = x * 2  # æ¯” no_grad æ›´å¿«

# ============ å¯ç”¨/ç¦ç”¨å…¨å±€æ¢¯åº¦ ============
torch.set_grad_enabled(False)  # ç¦ç”¨
torch.set_grad_enabled(True)   # å¯ç”¨

# ============ å†»ç»“å‚æ•° ============
# å¸¸ç”¨äºè¿ç§»å­¦ä¹ 
model = torch.nn.Linear(10, 2)
for param in model.parameters():
    param.requires_grad = False
```

### 4.4 é«˜çº§è‡ªåŠ¨æ±‚å¯¼

```python
import torch

# ============ è®¡ç®—é«˜é˜¶å¯¼æ•° ============
x = torch.tensor([1.], requires_grad=True)
y = x ** 3

# ä¸€é˜¶å¯¼æ•°
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]
print(f"dy/dx = {dy_dx}")  # 3

# äºŒé˜¶å¯¼æ•°
d2y_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]
print(f"dÂ²y/dxÂ² = {d2y_dx2}")  # 6

# ============ é›…å¯æ¯”çŸ©é˜µ ============
x = torch.randn(3, requires_grad=True)
y = x ** 2

# è®¡ç®—é›…å¯æ¯”çŸ©é˜µ
jacobian = torch.autograd.functional.jacobian(lambda x: x**2, x)
print(jacobian)  # å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ ä¸º 2*x

# ============ æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰============
parameters = [torch.randn(10, requires_grad=True) for _ in range(3)]
for p in parameters:
    p.grad = torch.randn_like(p) * 100  # æ¨¡æ‹Ÿå¤§æ¢¯åº¦

# æŒ‰èŒƒæ•°è£å‰ª
torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0)

# æŒ‰å€¼è£å‰ª
torch.nn.utils.clip_grad_value_(parameters, clip_value=0.5)
```

### 4.5 è‡ªå®šä¹‰æ¢¯åº¦

```python
import torch

# ============ è‡ªå®šä¹‰ autograd Function ============
class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        """
        ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œç”¨äºä¿å­˜åå‘ä¼ æ’­æ‰€éœ€çš„ä¿¡æ¯
        """
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        """
        grad_output: ä¸Šæ¸¸ä¼ æ¥çš„æ¢¯åº¦
        """
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input

# ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
x = torch.randn(5, requires_grad=True)
y = MyReLU.apply(x)
y.sum().backward()
print(x.grad)

# ============ ä½¿ç”¨è£…é¥°å™¨ç®€åŒ– ============
@torch.no_grad()
def custom_backward_hook(grad):
    """æ¢¯åº¦é’©å­"""
    return grad * 0.1  # ç¼©æ”¾æ¢¯åº¦

x = torch.randn(3, requires_grad=True)
x.register_hook(custom_backward_hook)
y = x ** 2
y.sum().backward()
print(x.grad)  # æ¢¯åº¦è¢«ç¼©æ”¾äº†
```

---

## 5. ç¥ç»ç½‘ç»œæ¨¡å—(nn.Module)

### 5.1 nn.Module åŸºç¡€

```python
import torch
import torch.nn as nn

# ============ æœ€ç®€å•çš„ç½‘ç»œ ============
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()  # å¿…é¡»è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# ä½¿ç”¨
model = SimpleNet(784, 256, 10)
x = torch.randn(32, 784)  # batch_size=32
output = model(x)
print(output.shape)  # torch.Size([32, 10])
```

### 5.2 å¸¸ç”¨ç½‘ç»œå±‚

```python
import torch.nn as nn

# ============ çº¿æ€§å±‚ ============
linear = nn.Linear(in_features=100, out_features=50, bias=True)

# ============ å·ç§¯å±‚ ============
# 1Då·ç§¯ (åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€éŸ³é¢‘)
conv1d = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3,
                   stride=1, padding=1)

# 2Då·ç§¯ (å›¾åƒ)
conv2d = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,
                   stride=1, padding=1, bias=True)

# 3Då·ç§¯ (è§†é¢‘)
conv3d = nn.Conv3d(in_channels=3, out_channels=64, kernel_size=3)

# ============ è½¬ç½®å·ç§¯ï¼ˆä¸Šé‡‡æ ·ï¼‰============
deconv = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)

# ============ æ± åŒ–å±‚ ============
maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
avgpool = nn.AvgPool2d(kernel_size=2, stride=2)
adaptivepool = nn.AdaptiveAvgPool2d(output_size=(1, 1))  # å…¨å±€å¹³å‡æ± åŒ–

# ============ å½’ä¸€åŒ–å±‚ ============
batchnorm1d = nn.BatchNorm1d(num_features=100)
batchnorm2d = nn.BatchNorm2d(num_features=64)
layernorm = nn.LayerNorm(normalized_shape=[64, 32, 32])
instancenorm = nn.InstanceNorm2d(num_features=64)
groupnorm = nn.GroupNorm(num_groups=8, num_channels=64)

# ============ Dropout ============
dropout = nn.Dropout(p=0.5)
dropout2d = nn.Dropout2d(p=0.5)  # ç©ºé—´Dropout

# ============ å¾ªç¯å±‚ ============
rnn = nn.RNN(input_size=100, hidden_size=256, num_layers=2,
             batch_first=True, bidirectional=True)
lstm = nn.LSTM(input_size=100, hidden_size=256, num_layers=2,
               batch_first=True, bidirectional=True)
gru = nn.GRU(input_size=100, hidden_size=256, num_layers=2,
             batch_first=True)

# ============ Transformerå±‚ ============
transformer = nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6,
                             num_decoder_layers=6)
encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)

# ============ Embedding ============
embedding = nn.Embedding(num_embeddings=10000, embedding_dim=256)
```

### 5.3 æ¿€æ´»å‡½æ•°

```python
import torch.nn as nn
import torch.nn.functional as F

# ============ å¸¸ç”¨æ¿€æ´»å‡½æ•° ============
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ¿€æ´»å‡½æ•°å¯¹æ¯”                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    å‡½æ•°       â”‚       å…¬å¼           â”‚         ç‰¹ç‚¹            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ReLU         â”‚  max(0, x)          â”‚ ç®€å•é«˜æ•ˆï¼Œå¯èƒ½æ­»äº¡ç¥ç»å…ƒ â”‚
â”‚  LeakyReLU    â”‚  max(Î±x, x)         â”‚ è§£å†³æ­»äº¡ç¥ç»å…ƒé—®é¢˜       â”‚
â”‚  PReLU        â”‚  max(Î±x, x), Î±å¯å­¦ä¹ â”‚ è‡ªé€‚åº”è´Ÿæ–œç‡            â”‚
â”‚  ELU          â”‚  x if x>0 else Î±(eË£-1)â”‚ å¹³æ»‘ï¼Œå‡å€¼æ¥è¿‘0        â”‚
â”‚  GELU         â”‚  xÂ·Î¦(x)             â”‚ Transformerå¸¸ç”¨         â”‚
â”‚  Sigmoid      â”‚  1/(1+eâ»Ë£)          â”‚ è¾“å‡º(0,1)ï¼Œæ¢¯åº¦æ¶ˆå¤±     â”‚
â”‚  Tanh         â”‚  (eË£-eâ»Ë£)/(eË£+eâ»Ë£)  â”‚ è¾“å‡º(-1,1)ï¼Œé›¶ä¸­å¿ƒ      â”‚
â”‚  Softmax      â”‚  eË£â±/Î£eË£Ê²           â”‚ å¤šåˆ†ç±»è¾“å‡ºå±‚            â”‚
â”‚  SiLU/Swish   â”‚  xÂ·Ïƒ(x)             â”‚ ç°ä»£ç½‘ç»œå¸¸ç”¨            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# ä½œä¸ºæ¨¡å—ä½¿ç”¨
relu = nn.ReLU()
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
gelu = nn.GELU()
sigmoid = nn.Sigmoid()
tanh = nn.Tanh()
softmax = nn.Softmax(dim=-1)
silu = nn.SiLU()

# ä½œä¸ºå‡½æ•°ä½¿ç”¨
x = torch.randn(10)
y = F.relu(x)
y = F.leaky_relu(x, negative_slope=0.01)
y = F.gelu(x)
y = F.sigmoid(x)
y = F.softmax(x, dim=-1)
```

### 5.4 å®¹å™¨æ¨¡å—

```python
import torch.nn as nn

# ============ Sequentialï¼šé¡ºåºå®¹å™¨ ============
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Dropout(0.5),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

# å¸¦åç§°çš„Sequential
model = nn.Sequential(OrderedDict([
    ('fc1', nn.Linear(784, 256)),
    ('relu1', nn.ReLU()),
    ('fc2', nn.Linear(256, 10))
]))

# ============ ModuleListï¼šæ¨¡å—åˆ—è¡¨ ============
class MyModel(nn.Module):
    def __init__(self, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(100, 100) for _ in range(num_layers)
        ])

    def forward(self, x):
        for layer in self.layers:
            x = F.relu(layer(x))
        return x

# ============ ModuleDictï¼šæ¨¡å—å­—å…¸ ============
class MultiTaskModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.shared = nn.Linear(100, 64)
        self.heads = nn.ModuleDict({
            'classification': nn.Linear(64, 10),
            'regression': nn.Linear(64, 1)
        })

    def forward(self, x, task):
        x = F.relu(self.shared(x))
        return self.heads[task](x)
```

### 5.5 å‚æ•°ç®¡ç†

```python
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)

# ============ æŸ¥çœ‹å‚æ•° ============
# æ‰€æœ‰å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: shape={param.shape}, requires_grad={param.requires_grad}")

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"æ€»å‚æ•°é‡: {total_params:,}")
print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")

# ============ å‚æ•°åˆå§‹åŒ– ============
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

model.apply(init_weights)

# å¸¸ç”¨åˆå§‹åŒ–æ–¹æ³•
"""
nn.init.xavier_uniform_(tensor)      # Xavierå‡åŒ€åˆ†å¸ƒ
nn.init.xavier_normal_(tensor)       # Xavieræ­£æ€åˆ†å¸ƒ
nn.init.kaiming_uniform_(tensor)     # Kaimingå‡åŒ€åˆ†å¸ƒ
nn.init.kaiming_normal_(tensor)      # Kaimingæ­£æ€åˆ†å¸ƒ
nn.init.zeros_(tensor)               # å…¨é›¶
nn.init.ones_(tensor)                # å…¨ä¸€
nn.init.constant_(tensor, val)       # å¸¸æ•°
nn.init.normal_(tensor, mean, std)   # æ­£æ€åˆ†å¸ƒ
nn.init.uniform_(tensor, a, b)       # å‡åŒ€åˆ†å¸ƒ
nn.init.orthogonal_(tensor)          # æ­£äº¤åˆå§‹åŒ–
"""

# ============ å†»ç»“/è§£å†»å‚æ•° ============
# å†»ç»“æ‰€æœ‰å‚æ•°
for param in model.parameters():
    param.requires_grad = False

# åªè§£å†»æœ€åä¸€å±‚
for param in model[-1].parameters():
    param.requires_grad = True
```

---

## 6. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨

### 6.1 å¸¸ç”¨æŸå¤±å‡½æ•°

```python
import torch
import torch.nn as nn

# ============ åˆ†ç±»æŸå¤± ============
# äº¤å‰ç†µæŸå¤±ï¼ˆå¤šåˆ†ç±»ï¼Œå†…ç½®Softmaxï¼‰
criterion = nn.CrossEntropyLoss()
logits = torch.randn(3, 5)  # 3ä¸ªæ ·æœ¬ï¼Œ5ä¸ªç±»åˆ«
targets = torch.tensor([1, 0, 4])  # çœŸå®æ ‡ç­¾
loss = criterion(logits, targets)

# äºŒå…ƒäº¤å‰ç†µï¼ˆäºŒåˆ†ç±»ï¼Œéœ€è¦å…ˆSigmoidï¼‰
criterion = nn.BCELoss()
probs = torch.sigmoid(torch.randn(3))
targets = torch.tensor([1., 0., 1.])
loss = criterion(probs, targets)

# BCEWithLogitsLossï¼ˆå†…ç½®Sigmoidï¼Œæ›´ç¨³å®šï¼‰
criterion = nn.BCEWithLogitsLoss()
logits = torch.randn(3)
loss = criterion(logits, targets)

# ============ å›å½’æŸå¤± ============
# MSEæŸå¤±
criterion = nn.MSELoss()
predictions = torch.randn(3)
targets = torch.randn(3)
loss = criterion(predictions, targets)

# L1æŸå¤±ï¼ˆMAEï¼‰
criterion = nn.L1Loss()
loss = criterion(predictions, targets)

# Smooth L1æŸå¤±ï¼ˆHuber Lossï¼‰
criterion = nn.SmoothL1Loss()
loss = criterion(predictions, targets)

# ============ å…¶ä»–æŸå¤± ============
# è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆNLLï¼‰
criterion = nn.NLLLoss()
log_probs = F.log_softmax(logits, dim=1)
loss = criterion(log_probs, targets)

# KLæ•£åº¦
criterion = nn.KLDivLoss(reduction='batchmean')
log_probs = F.log_softmax(logits, dim=1)
target_probs = F.softmax(torch.randn(3, 5), dim=1)
loss = criterion(log_probs, target_probs)

# ä½™å¼¦åµŒå…¥æŸå¤±
criterion = nn.CosineEmbeddingLoss()
x1 = torch.randn(3, 10)
x2 = torch.randn(3, 10)
y = torch.tensor([1, -1, 1])  # 1è¡¨ç¤ºç›¸ä¼¼ï¼Œ-1è¡¨ç¤ºä¸ç›¸ä¼¼
loss = criterion(x1, x2, y)

# TripletæŸå¤±
criterion = nn.TripletMarginLoss(margin=1.0)
anchor = torch.randn(3, 10)
positive = torch.randn(3, 10)
negative = torch.randn(3, 10)
loss = criterion(anchor, positive, negative)
```

### 6.2 è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
import torch
import torch.nn as nn

# æ–¹æ³•1ï¼šç»§æ‰¿ nn.Module
class FocalLoss(nn.Module):
    """Focal Loss - è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜"""
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

# æ–¹æ³•2ï¼šç›´æ¥å®šä¹‰å‡½æ•°
def dice_loss(pred, target, smooth=1e-6):
    """Dice Loss - å¸¸ç”¨äºåˆ†å‰²ä»»åŠ¡"""
    pred = torch.sigmoid(pred)
    intersection = (pred * target).sum()
    union = pred.sum() + target.sum()
    dice = (2. * intersection + smooth) / (union + smooth)
    return 1 - dice

# æ–¹æ³•3ï¼šç»„åˆæŸå¤±
class CombinedLoss(nn.Module):
    def __init__(self, alpha=0.5):
        super().__init__()
        self.alpha = alpha
        self.ce = nn.CrossEntropyLoss()

    def forward(self, pred, target):
        ce_loss = self.ce(pred, target)
        dice = dice_loss(pred, target)
        return self.alpha * ce_loss + (1 - self.alpha) * dice
```

### 6.3 ä¼˜åŒ–å™¨

```python
import torch.optim as optim

model = nn.Linear(10, 2)

# ============ å¸¸ç”¨ä¼˜åŒ–å™¨ ============
# SGD
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9,
                      weight_decay=1e-4, nesterov=True)

# Adamï¼ˆæœ€å¸¸ç”¨ï¼‰
optimizer = optim.Adam(model.parameters(), lr=0.001,
                       betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4)

# AdamWï¼ˆè§£è€¦æƒé‡è¡°å‡ï¼‰
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# RMSprop
optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)

# Adagrad
optimizer = optim.Adagrad(model.parameters(), lr=0.01)

# ============ åˆ†ç»„å‚æ•°ï¼ˆä¸åŒå±‚ä¸åŒå­¦ä¹ ç‡ï¼‰============
optimizer = optim.Adam([
    {'params': model.base.parameters(), 'lr': 1e-5},
    {'params': model.classifier.parameters(), 'lr': 1e-3}
], lr=1e-4)  # é»˜è®¤å­¦ä¹ ç‡

# ============ ä¼˜åŒ–å™¨åŸºæœ¬æ“ä½œ ============
optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
loss.backward()        # è®¡ç®—æ¢¯åº¦
optimizer.step()       # æ›´æ–°å‚æ•°

# æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡
for param_group in optimizer.param_groups:
    param_group['lr'] = new_lr
```

### 6.4 å­¦ä¹ ç‡è°ƒåº¦å™¨

```python
import torch.optim.lr_scheduler as lr_scheduler

optimizer = optim.Adam(model.parameters(), lr=0.001)

# ============ å¸¸ç”¨è°ƒåº¦å™¨ ============
# é˜¶æ¢¯è¡°å‡
scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# å¤šé˜¶æ®µè¡°å‡
scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)

# æŒ‡æ•°è¡°å‡
scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

# ä½™å¼¦é€€ç«
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)

# å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«
scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)

# æŒ‰éªŒè¯æŒ‡æ ‡è°ƒæ•´ï¼ˆplateauæ—¶é™ä½ï¼‰
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',
                                           factor=0.1, patience=10)

# çº¿æ€§é¢„çƒ­
def warmup_lambda(epoch):
    if epoch < warmup_epochs:
        return epoch / warmup_epochs
    return 1.0
scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)

# ============ ä½¿ç”¨è°ƒåº¦å™¨ ============
for epoch in range(num_epochs):
    train(...)
    val_loss = validate(...)

    # å¸¸è§„è°ƒåº¦å™¨
    scheduler.step()

    # ReduceLROnPlateauéœ€è¦ä¼ å…¥æŒ‡æ ‡
    # scheduler.step(val_loss)

    print(f"å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()}")
```

---

## 7. æ•°æ®åŠ è½½ä¸å¤„ç†

### 7.1 Dataset ç±»

```python
from torch.utils.data import Dataset, DataLoader
import torch

# ============ è‡ªå®šä¹‰Dataset ============
class CustomDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data)

    def __getitem__(self, idx):
        """è¿”å›å•ä¸ªæ ·æœ¬"""
        sample = self.data[idx]
        label = self.labels[idx]

        if self.transform:
            sample = self.transform(sample)

        return sample, label

# ä½¿ç”¨ç¤ºä¾‹
data = torch.randn(1000, 28, 28)
labels = torch.randint(0, 10, (1000,))
dataset = CustomDataset(data, labels)

# ============ å¸¸ç”¨å†…ç½®Dataset ============
from torchvision import datasets
from torchvision.transforms import ToTensor

# MNIST
mnist_train = datasets.MNIST(root='./data', train=True,
                             download=True, transform=ToTensor())

# CIFAR-10
cifar_train = datasets.CIFAR10(root='./data', train=True,
                               download=True, transform=ToTensor())

# ImageFolderï¼ˆè‡ªå®šä¹‰å›¾ç‰‡æ•°æ®é›†ï¼‰
# ç›®å½•ç»“æ„: root/class1/xxx.png, root/class2/xxx.png, ...
from torchvision.datasets import ImageFolder
dataset = ImageFolder(root='./data/train', transform=ToTensor())
```

### 7.2 DataLoader

```python
from torch.utils.data import DataLoader

# åˆ›å»ºDataLoader
dataloader = DataLoader(
    dataset,
    batch_size=32,           # æ‰¹å¤§å°
    shuffle=True,            # æ˜¯å¦æ‰“ä¹±
    num_workers=4,           # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True,         # åŠ é€ŸGPUä¼ è¾“
    drop_last=False,         # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batch
    collate_fn=None,         # è‡ªå®šä¹‰batchæ•´ç†å‡½æ•°
    sampler=None,            # è‡ªå®šä¹‰é‡‡æ ·å™¨
    persistent_workers=True  # ä¿æŒworkerè¿›ç¨‹
)

# éå†æ•°æ®
for batch_idx, (data, targets) in enumerate(dataloader):
    print(f"Batch {batch_idx}: data shape = {data.shape}")
    break

# ============ è‡ªå®šä¹‰ collate_fn ============
def custom_collate_fn(batch):
    """å¤„ç†å˜é•¿åºåˆ—"""
    data = [item[0] for item in batch]
    labels = [item[1] for item in batch]

    # å¡«å……åˆ°ç›¸åŒé•¿åº¦
    data_padded = nn.utils.rnn.pad_sequence(data, batch_first=True)
    labels = torch.stack(labels)

    return data_padded, labels

# ============ è‡ªå®šä¹‰é‡‡æ ·å™¨ ============
from torch.utils.data import WeightedRandomSampler

# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
class_weights = [1.0, 2.0, 0.5]  # å„ç±»åˆ«æƒé‡
sample_weights = [class_weights[label] for label in dataset.labels]
sampler = WeightedRandomSampler(sample_weights, num_samples=len(dataset))

dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)
```

### 7.3 æ•°æ®å˜æ¢

```python
from torchvision import transforms

# ============ å¸¸ç”¨å˜æ¢ ============
transform = transforms.Compose([
    # å‡ ä½•å˜æ¢
    transforms.Resize((256, 256)),           # è°ƒæ•´å¤§å°
    transforms.CenterCrop(224),              # ä¸­å¿ƒè£å‰ª
    transforms.RandomCrop(224),              # éšæœºè£å‰ª
    transforms.RandomResizedCrop(224),       # éšæœºè£å‰ª+ç¼©æ”¾
    transforms.RandomHorizontalFlip(p=0.5),  # æ°´å¹³ç¿»è½¬
    transforms.RandomVerticalFlip(p=0.5),    # å‚ç›´ç¿»è½¬
    transforms.RandomRotation(degrees=15),   # éšæœºæ—‹è½¬

    # é¢œè‰²å˜æ¢
    transforms.ColorJitter(brightness=0.2, contrast=0.2,
                          saturation=0.2, hue=0.1),
    transforms.RandomGrayscale(p=0.1),       # éšæœºç°åº¦åŒ–

    # è½¬æ¢
    transforms.ToTensor(),                   # PIL/numpy -> Tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # æ ‡å‡†åŒ–
                        std=[0.229, 0.224, 0.225]),

    # æ•°æ®å¢å¼º
    transforms.RandomErasing(p=0.5),         # éšæœºæ“¦é™¤
])

# ============ è®­ç»ƒ/éªŒè¯ä¸åŒå˜æ¢ ============
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.AutoAugment(),  # è‡ªåŠ¨æ•°æ®å¢å¼º
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# ============ è‡ªå®šä¹‰å˜æ¢ ============
class AddGaussianNoise:
    def __init__(self, mean=0., std=1.):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        noise = torch.randn(tensor.size()) * self.std + self.mean
        return tensor + noise
```

---

## 8. å®Œæ•´è®­ç»ƒæµç¨‹

### 8.1 è®­ç»ƒæ¡†æ¶

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

class Trainer:
    def __init__(self, model, train_loader, val_loader, criterion,
                 optimizer, scheduler=None, device='cuda'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device

        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')

    def train_epoch(self):
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        pbar = tqdm(self.train_loader, desc='Training')
        for data, targets in pbar:
            data, targets = data.to(self.device), targets.to(self.device)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            # æ›´æ–°å‚æ•°
            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*correct/total:.2f}%'
            })

        return total_loss / len(self.train_loader), correct / total

    @torch.no_grad()
    def validate(self):
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        for data, targets in self.val_loader:
            data, targets = data.to(self.device), targets.to(self.device)
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        return total_loss / len(self.val_loader), correct / total

    def fit(self, epochs, save_path='best_model.pth'):
        for epoch in range(epochs):
            print(f'\nEpoch {epoch+1}/{epochs}')

            train_loss, train_acc = self.train_epoch()
            val_loss, val_acc = self.validate()

            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)

            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%')

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                torch.save(self.model.state_dict(), save_path)
                print(f'Saved best model with val_loss: {val_loss:.4f}')

# ============ ä½¿ç”¨ç¤ºä¾‹ ============
# æ¨¡å‹
model = SimpleNet(784, 256, 10)

# æ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# è®­ç»ƒ
trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, scheduler)
trainer.fit(epochs=100)
```

### 8.2 æ—©åœæœºåˆ¶

```python
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_weights = model.state_dict().copy()
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_loss = val_loss
            self.best_weights = model.state_dict().copy()
            self.counter = 0
        return False

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=10)
for epoch in range(max_epochs):
    train(...)
    val_loss = validate(...)
    if early_stopping(val_loss, model):
        print("Early stopping triggered")
        break
```

---

## 9. æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### 9.1 ä¿å­˜ä¸åŠ è½½æ–¹æ³•

```python
import torch

# ============ æ–¹æ³•1ï¼šåªä¿å­˜å‚æ•°ï¼ˆæ¨èï¼‰============
# ä¿å­˜
torch.save(model.state_dict(), 'model_weights.pth')

# åŠ è½½
model = MyModel()  # éœ€è¦å…ˆåˆ›å»ºæ¨¡å‹ç»“æ„
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()  # æ¨ç†æ¨¡å¼

# ============ æ–¹æ³•2ï¼šä¿å­˜æ•´ä¸ªæ¨¡å‹ ============
# ä¿å­˜
torch.save(model, 'model_complete.pth')

# åŠ è½½
model = torch.load('model_complete.pth')

# ============ æ–¹æ³•3ï¼šä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆè®­ç»ƒæ¢å¤ï¼‰============
# ä¿å­˜
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'train_loss': train_loss,
    'val_loss': val_loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# åŠ è½½
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
start_epoch = checkpoint['epoch'] + 1

# ============ è·¨è®¾å¤‡åŠ è½½ ============
# GPUæ¨¡å‹åŠ è½½åˆ°CPU
model.load_state_dict(torch.load('model.pth', map_location='cpu'))

# CPUæ¨¡å‹åŠ è½½åˆ°GPU
model.load_state_dict(torch.load('model.pth', map_location='cuda:0'))

# ============ éƒ¨åˆ†åŠ è½½ ============
pretrained_dict = torch.load('pretrained.pth')
model_dict = model.state_dict()

# è¿‡æ»¤ä¸åŒ¹é…çš„é”®
pretrained_dict = {k: v for k, v in pretrained_dict.items()
                   if k in model_dict and v.shape == model_dict[k].shape}
model_dict.update(pretrained_dict)
model.load_state_dict(model_dict)
```

### 9.2 å¯¼å‡ºä¸º ONNX

```python
import torch

# å¯¼å‡º
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }
)

# éªŒè¯
import onnx
onnx_model = onnx.load("model.onnx")
onnx.checker.check_model(onnx_model)

# æ¨ç†
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
outputs = session.run(None, {'input': input_data.numpy()})
```

---

## 10. GPU åŠ é€Ÿ

### 10.1 åŸºæœ¬ GPU æ“ä½œ

```python
import torch

# ============ æ£€æŸ¥CUDA ============
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
print(f"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")

# ============ è®¾å¤‡è®¾ç½® ============
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# æˆ–æŒ‡å®šå…·ä½“GPU
device = torch.device('cuda:0')

# ============ æ•°æ®ç§»åŠ¨ ============
# å¼ é‡
tensor = torch.randn(3, 4)
tensor = tensor.to(device)
tensor = tensor.cuda()
tensor = tensor.cpu()

# æ¨¡å‹
model = model.to(device)
model = model.cuda()

# ============ å†…å­˜ç®¡ç† ============
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
print(f"å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
print(f"å·²ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")

# æ¸…ç©ºç¼“å­˜
torch.cuda.empty_cache()

# åŒæ­¥
torch.cuda.synchronize()
```

### 10.2 å¤š GPU è®­ç»ƒ

```python
import torch
import torch.nn as nn

# ============ DataParallelï¼ˆç®€å•ä½†æ•ˆç‡è¾ƒä½ï¼‰============
model = MyModel()
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
model = model.to(device)

# ============ DistributedDataParallelï¼ˆæ¨èï¼‰============
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def train(rank, world_size):
    setup(rank, world_size)

    # åˆ›å»ºæ¨¡å‹
    model = MyModel().to(rank)
    model = DDP(model, device_ids=[rank])

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)

    # è®­ç»ƒ
    for epoch in range(epochs):
        sampler.set_epoch(epoch)  # ç¡®ä¿æ¯ä¸ªepochæ‰“ä¹±ä¸åŒ
        for data, target in dataloader:
            data, target = data.to(rank), target.to(rank)
            # ... è®­ç»ƒä»£ç 

    cleanup()

# å¯åŠ¨
if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size)
```

### 10.3 æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

# åˆ›å»ºGradScaler
scaler = GradScaler()

model = model.to(device)
optimizer = optim.Adam(model.parameters())

for data, target in dataloader:
    data, target = data.to(device), target.to(device)

    optimizer.zero_grad()

    # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
    with autocast():
        output = model(data)
        loss = criterion(output, target)

    # ç¼©æ”¾ååå‘ä¼ æ’­
    scaler.scale(loss).backward()

    # æ›´æ–°å‚æ•°
    scaler.step(optimizer)
    scaler.update()
```

---

## 11. é«˜çº§ä¸»é¢˜

### 11.1 è‡ªå®šä¹‰å±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SEBlock(nn.Module):
    """Squeeze-and-Excitation Block"""
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.squeeze(x).view(b, c)
        y = self.excitation(y).view(b, c, 1, 1)
        return x * y.expand_as(x)


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_k = d_model // num_heads
        self.num_heads = num_heads

        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, q, k, v, mask=None):
        bs = q.size(0)

        # çº¿æ€§æŠ•å½±
        q = self.q_linear(q).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        output = torch.matmul(attn, v)
        output = output.transpose(1, 2).contiguous().view(bs, -1, self.num_heads * self.d_k)

        return self.out_linear(output)
```

### 11.2 é’©å­(Hooks)

```python
import torch
import torch.nn as nn

# ============ å‰å‘é’©å­ ============
activations = {}

def get_activation(name):
    def hook(model, input, output):
        activations[name] = output.detach()
    return hook

model = models.resnet18(pretrained=True)
model.layer1.register_forward_hook(get_activation('layer1'))
model.layer2.register_forward_hook(get_activation('layer2'))

# è¿è¡Œæ¨¡å‹
x = torch.randn(1, 3, 224, 224)
output = model(x)

# è·å–ä¸­é—´ç‰¹å¾
print(activations['layer1'].shape)
print(activations['layer2'].shape)

# ============ åå‘é’©å­ ============
gradients = {}

def get_gradient(name):
    def hook(model, grad_input, grad_output):
        gradients[name] = grad_output[0].detach()
    return hook

model.layer4.register_full_backward_hook(get_gradient('layer4'))

output = model(x)
output.sum().backward()
print(gradients['layer4'].shape)

# ============ ä½¿ç”¨é’©å­è¿›è¡Œç‰¹å¾å¯è§†åŒ–ï¼ˆCAMï¼‰============
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None

        target_layer.register_forward_hook(self._save_activation)
        target_layer.register_full_backward_hook(self._save_gradient)

    def _save_activation(self, module, input, output):
        self.activations = output.detach()

    def _save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()

    def generate(self, x, target_class):
        output = self.model(x)

        self.model.zero_grad()
        output[0, target_class].backward()

        weights = self.gradients.mean(dim=[2, 3], keepdim=True)
        cam = (weights * self.activations).sum(dim=1, keepdim=True)
        cam = F.relu(cam)
        cam = F.interpolate(cam, x.shape[2:], mode='bilinear', align_corners=False)
        cam = cam - cam.min()
        cam = cam / cam.max()

        return cam
```

### 11.3 TorchScriptï¼ˆæ¨¡å‹ç¼–è¯‘ï¼‰

```python
import torch

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 5)

    def forward(self, x):
        return torch.relu(self.linear(x))

model = MyModel()
model.eval()

# ============ Tracingï¼ˆè¿½è¸ªï¼‰============
# é€‚åˆæ²¡æœ‰æ§åˆ¶æµçš„æ¨¡å‹
example_input = torch.randn(1, 10)
traced_model = torch.jit.trace(model, example_input)
traced_model.save("traced_model.pt")

# ============ Scriptingï¼ˆè„šæœ¬åŒ–ï¼‰============
# é€‚åˆæœ‰æ§åˆ¶æµçš„æ¨¡å‹
class ModelWithControl(nn.Module):
    def forward(self, x):
        if x.sum() > 0:
            return x * 2
        else:
            return x * 3

scripted_model = torch.jit.script(ModelWithControl())
scripted_model.save("scripted_model.pt")

# ============ åŠ è½½å’Œä½¿ç”¨ ============
loaded_model = torch.jit.load("traced_model.pt")
output = loaded_model(example_input)
```

---

## 12. å®æˆ˜æ¡ˆä¾‹

### 12.1 å›¾åƒåˆ†ç±»ï¼ˆResNetï¼‰

```python
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# ============ æ•°æ®å‡†å¤‡ ============
transform_train = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

transform_test = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)

# ============ æ¨¡å‹ï¼ˆè¿ç§»å­¦ä¹ ï¼‰============
model = torchvision.models.resnet18(pretrained=True)

# å†»ç»“ç‰¹å¾æå–å±‚
for param in model.parameters():
    param.requires_grad = False

# æ›¿æ¢åˆ†ç±»å¤´
model.fc = nn.Linear(model.fc.in_features, 10)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# ============ è®­ç»ƒè®¾ç½® ============
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

# ============ è®­ç»ƒå¾ªç¯ ============
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    scheduler.step()

    # éªŒè¯
    model.eval()
    test_correct = 0
    test_total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            test_total += labels.size(0)
            test_correct += predicted.eq(labels).sum().item()

    print(f'Epoch [{epoch+1}/{num_epochs}], '
          f'Loss: {running_loss/len(train_loader):.4f}, '
          f'Train Acc: {100*correct/total:.2f}%, '
          f'Test Acc: {100*test_correct/test_total:.2f}%')
```

### 12.2 æ–‡æœ¬åˆ†ç±»ï¼ˆTransformerï¼‰

```python
import torch
import torch.nn as nn

class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, num_layers,
                 num_classes, max_len=512, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_embedding = nn.Embedding(max_len, embed_dim)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim*4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        self.classifier = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, num_classes)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        seq_len = x.size(1)
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)

        x = self.embedding(x) + self.pos_embedding(positions)
        x = self.dropout(x)

        if mask is not None:
            # åˆ›å»º attention mask (True è¡¨ç¤ºå±è”½)
            padding_mask = (mask == 0)
        else:
            padding_mask = None

        x = self.transformer(x, src_key_padding_mask=padding_mask)

        # ä½¿ç”¨ [CLS] token æˆ–å¹³å‡æ± åŒ–
        x = x.mean(dim=1)  # å¹³å‡æ± åŒ–

        return self.classifier(x)

# ä½¿ç”¨
model = TextClassifier(
    vocab_size=30000,
    embed_dim=256,
    num_heads=8,
    num_layers=4,
    num_classes=10
)

# ç¤ºä¾‹è¾“å…¥
batch_size = 16
seq_len = 128
input_ids = torch.randint(0, 30000, (batch_size, seq_len))
attention_mask = torch.ones(batch_size, seq_len)

output = model(input_ids, attention_mask)
print(output.shape)  # torch.Size([16, 10])
```

### 12.3 ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰

```python
import torch
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_channels=1, feature_dim=64):
        super().__init__()
        self.main = nn.Sequential(
            # è¾“å…¥: (batch, latent_dim, 1, 1)
            nn.ConvTranspose2d(latent_dim, feature_dim*8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(feature_dim*8),
            nn.ReLU(True),
            # (batch, feature_dim*8, 4, 4)

            nn.ConvTranspose2d(feature_dim*8, feature_dim*4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim*4),
            nn.ReLU(True),
            # (batch, feature_dim*4, 8, 8)

            nn.ConvTranspose2d(feature_dim*4, feature_dim*2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim*2),
            nn.ReLU(True),
            # (batch, feature_dim*2, 16, 16)

            nn.ConvTranspose2d(feature_dim*2, feature_dim, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(True),
            # (batch, feature_dim, 32, 32)

            nn.ConvTranspose2d(feature_dim, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # (batch, img_channels, 64, 64)
        )

    def forward(self, z):
        return self.main(z)


class Discriminator(nn.Module):
    def __init__(self, img_channels=1, feature_dim=64):
        super().__init__()
        self.main = nn.Sequential(
            # è¾“å…¥: (batch, img_channels, 64, 64)
            nn.Conv2d(img_channels, feature_dim, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim, feature_dim*2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim*2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim*2, feature_dim*4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim*4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim*4, feature_dim*8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim*8),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim*8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, img):
        return self.main(img).view(-1, 1)


# ============ è®­ç»ƒ GAN ============
def train_gan(generator, discriminator, dataloader, num_epochs, device):
    criterion = nn.BCELoss()

    optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    latent_dim = 100

    for epoch in range(num_epochs):
        for real_imgs, _ in dataloader:
            batch_size = real_imgs.size(0)
            real_imgs = real_imgs.to(device)

            # çœŸå®å’Œå‡æ ‡ç­¾
            real_labels = torch.ones(batch_size, 1).to(device)
            fake_labels = torch.zeros(batch_size, 1).to(device)

            # ============ è®­ç»ƒåˆ¤åˆ«å™¨ ============
            optimizer_D.zero_grad()

            # çœŸå®å›¾ç‰‡
            outputs = discriminator(real_imgs)
            d_loss_real = criterion(outputs, real_labels)

            # å‡å›¾ç‰‡
            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)
            fake_imgs = generator(z)
            outputs = discriminator(fake_imgs.detach())
            d_loss_fake = criterion(outputs, fake_labels)

            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            optimizer_D.step()

            # ============ è®­ç»ƒç”Ÿæˆå™¨ ============
            optimizer_G.zero_grad()

            outputs = discriminator(fake_imgs)
            g_loss = criterion(outputs, real_labels)

            g_loss.backward()
            optimizer_G.step()

        print(f'Epoch [{epoch+1}/{num_epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')
```

---

## 13. æœ€ä½³å®è·µä¸è°ƒè¯•æŠ€å·§

### 13.1 ä»£ç è§„èŒƒ

```python
# ============ é¡¹ç›®ç»“æ„ ============
"""
project/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml         # é…ç½®æ–‡ä»¶
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py          # æ•°æ®é›†ç±»
â”‚   â””â”€â”€ transforms.py       # æ•°æ®å˜æ¢
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ backbone.py         # ä¸»å¹²ç½‘ç»œ
â”‚   â””â”€â”€ head.py             # ä»»åŠ¡å¤´
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py           # æ—¥å¿—
â”‚   â”œâ”€â”€ metrics.py          # è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ visualization.py    # å¯è§†åŒ–
â”œâ”€â”€ train.py                # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py             # è¯„ä¼°è„šæœ¬
â””â”€â”€ requirements.txt        # ä¾èµ–
"""

# ============ è®¾ç½®éšæœºç§å­ï¼ˆå¯å¤ç°æ€§ï¼‰============
import random
import numpy as np
import torch

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ============ è®¾å¤‡æ— å…³ä»£ç  ============
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
def get_device():
    if torch.cuda.is_available():
        return torch.device('cuda')
    elif torch.backends.mps.is_available():  # Apple Silicon
        return torch.device('mps')
    return torch.device('cpu')
```

### 13.2 è°ƒè¯•æŠ€å·§

```python
# ============ å½¢çŠ¶æ£€æŸ¥ ============
def debug_shapes(model, input_shape):
    """æ‰“å°æ¯ä¸€å±‚çš„è¾“å‡ºå½¢çŠ¶"""
    x = torch.randn(*input_shape)
    for name, layer in model.named_children():
        x = layer(x)
        print(f"{name}: {x.shape}")

# ============ æ¢¯åº¦æ£€æŸ¥ ============
def check_gradients(model):
    """æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸"""
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            if grad_norm == 0:
                print(f"Warning: {name} æ¢¯åº¦ä¸ºé›¶")
            elif grad_norm > 1000:
                print(f"Warning: {name} æ¢¯åº¦è¿‡å¤§: {grad_norm}")
            elif torch.isnan(param.grad).any():
                print(f"Error: {name} æ¢¯åº¦ä¸ºNaN")

# ============ å¼‚å¸¸æ£€æµ‹ ============
torch.autograd.set_detect_anomaly(True)  # å¼€å¯å¼‚å¸¸æ£€æµ‹

# ============ å†…å­˜åˆ†æ ============
def memory_stats():
    if torch.cuda.is_available():
        print(f"å·²åˆ†é…: {torch.cuda.memory_allocated()/1024**2:.2f} MB")
        print(f"æœ€å¤§åˆ†é…: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB")
        print(f"å·²ç¼“å­˜: {torch.cuda.memory_reserved()/1024**2:.2f} MB")

# ============ æ€§èƒ½åˆ†æ ============
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    record_shapes=True,
    profile_memory=True,
) as prof:
    model(input)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

### 13.3 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

```python
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        é—®é¢˜            â”‚              è§£å†³æ–¹æ¡ˆ                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA out of memory     â”‚ - å‡å° batch_size                         â”‚
â”‚                        â”‚ - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯                             â”‚
â”‚                        â”‚ - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ                         â”‚
â”‚                        â”‚ - ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Loss ä¸º NaN            â”‚ - é™ä½å­¦ä¹ ç‡                               â”‚
â”‚                        â”‚ - æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰ NaN                       â”‚
â”‚                        â”‚ - æ·»åŠ æ¢¯åº¦è£å‰ª                             â”‚
â”‚                        â”‚ - æ£€æŸ¥é™¤é›¶æ“ä½œ                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹ä¸æ”¶æ•›             â”‚ - æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®                      â”‚
â”‚                        â”‚ - è°ƒæ•´å­¦ä¹ ç‡                               â”‚
â”‚                        â”‚ - æ£€æŸ¥æ ‡ç­¾æ˜¯å¦æ­£ç¡®                         â”‚
â”‚                        â”‚ - ç®€åŒ–æ¨¡å‹éªŒè¯åŸºæœ¬åŠŸèƒ½                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿‡æ‹Ÿåˆ                 â”‚ - å¢åŠ æ•°æ®å¢å¼º                             â”‚
â”‚                        â”‚ - æ·»åŠ  Dropout                            â”‚
â”‚                        â”‚ - æ·»åŠ æ­£åˆ™åŒ– (weight_decay)               â”‚
â”‚                        â”‚ - æ—©åœ                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è®­ç»ƒé€Ÿåº¦æ…¢             â”‚ - å¢åŠ  num_workers                         â”‚
â”‚                        â”‚ - ä½¿ç”¨ pin_memory=True                    â”‚
â”‚                        â”‚ - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ                         â”‚
â”‚                        â”‚ - æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„è®¡ç®—                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# ============ æ¢¯åº¦ç´¯ç§¯ï¼ˆå†…å­˜ä¸è¶³æ—¶ï¼‰============
accumulation_steps = 4
optimizer.zero_grad()

for i, (data, target) in enumerate(dataloader):
    output = model(data)
    loss = criterion(output, target)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# ============ æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœå†…å­˜ï¼‰============
from torch.utils.checkpoint import checkpoint

class LargeModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(1000, 1000)
        self.layer2 = nn.Linear(1000, 1000)
        self.layer3 = nn.Linear(1000, 1000)

    def forward(self, x):
        x = checkpoint(self.layer1, x)  # ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼
        x = checkpoint(self.layer2, x)
        x = self.layer3(x)
        return x
```

---

## ğŸ“š é™„å½•

### å¸¸ç”¨èµ„æº

| èµ„æº             | é“¾æ¥                               |
| ---------------- | ---------------------------------- |
| PyTorch å®˜æ–¹æ–‡æ¡£ | https://pytorch.org/docs/          |
| PyTorch æ•™ç¨‹     | https://pytorch.org/tutorials/     |
| PyTorch Hub      | https://pytorch.org/hub/           |
| PyTorch è®ºå›     | https://discuss.pytorch.org/       |
| PyTorch GitHub   | https://github.com/pytorch/pytorch |

### ç‰ˆæœ¬ä¿¡æ¯

```python
# æ£€æŸ¥å®Œæ•´ç¯å¢ƒä¿¡æ¯
import torch
print(torch.__config__.show())
```

---

> ğŸ“ **æ–‡æ¡£ä¿¡æ¯**
>
> - ç‰ˆæœ¬ï¼š1.0
> - æœ€åæ›´æ–°ï¼š2024 å¹´
> - é€‚ç”¨ PyTorch ç‰ˆæœ¬ï¼š2.0+
