# Phase 3 å­¦ä¹ æ€»ç»“ï¼šPyTorch æ ¸å¿ƒæŠ€èƒ½

## ğŸ“š æ¨¡å—æ¦‚è§ˆ

| æ¨¡å— | ä¸»é¢˜        | æ ¸å¿ƒæ¦‚å¿µ               |
| ---- | ----------- | ---------------------- |
| 01   | Tensor åŸºç¡€ | åˆ›å»ºã€å±æ€§ã€è®¾å¤‡ç®¡ç†   |
| 02   | Tensor è¿ç®— | æ•°å­¦è¿ç®—ã€å¹¿æ’­ã€ç´¢å¼•   |
| 03   | è‡ªåŠ¨å¾®åˆ†    | Autogradã€è®¡ç®—å›¾       |
| 04   | nn.Module   | æ¨¡å‹æ„å»ºã€å‚æ•°ç®¡ç†     |
| 05   | æŸå¤±å‡½æ•°    | åˆ†ç±»ã€å›å½’æŸå¤±         |
| 06   | ä¼˜åŒ–å™¨      | SGDã€Adamã€å­¦ä¹ ç‡è°ƒåº¦  |
| 07   | æ•°æ®åŠ è½½    | Datasetã€DataLoader    |
| 08   | æ•°æ®å¢å¼º    | å›¾åƒã€æ–‡æœ¬å¢å¼º         |
| 09   | è®­ç»ƒå¾ªç¯    | å®Œæ•´è®­ç»ƒæµç¨‹           |
| 10   | æ¨¡å‹ä¿å­˜    | state_dictã€checkpoint |

---

## 1ï¸âƒ£ Tensor åŸºç¡€

### åˆ›å»ºæ–¹å¼

```python
torch.tensor([1, 2, 3])           # ä»åˆ—è¡¨
torch.zeros(3, 4)                 # å…¨é›¶
torch.ones(3, 4)                  # å…¨ä¸€
torch.randn(3, 4)                 # æ ‡å‡†æ­£æ€
torch.arange(0, 10, 2)            # ç­‰å·®åºåˆ—
torch.eye(3)                      # å•ä½çŸ©é˜µ
torch.from_numpy(np_array)        # ä» NumPy
```

### å…³é”®å±æ€§

```python
x.shape          # å½¢çŠ¶
x.dtype          # æ•°æ®ç±»å‹
x.device         # è®¾å¤‡ (cpu/cuda)
x.requires_grad  # æ˜¯å¦éœ€è¦æ¢¯åº¦
x.numel()        # å…ƒç´ æ€»æ•°
```

### è®¾å¤‡ç®¡ç†

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = x.to(device)
model = model.to(device)
```

---

## 2ï¸âƒ£ Tensor è¿ç®—

### åŸºæœ¬è¿ç®—

```python
a + b, a - b, a * b, a / b    # å…ƒç´ çº§è¿ç®—
a @ b                          # çŸ©é˜µä¹˜æ³•
torch.mm(A, B)                 # 2D çŸ©é˜µä¹˜æ³•
torch.bmm(A, B)                # æ‰¹é‡çŸ©é˜µä¹˜æ³•
torch.dot(a, b)                # å‘é‡ç‚¹ç§¯
```

### å¹¿æ’­è§„åˆ™

```
ä»åå¾€å‰å¯¹é½ç»´åº¦ï¼Œç»´åº¦ä¸º 1 çš„å¯ä»¥å¹¿æ’­
(3, 4) + (4,) = (3, 4)
(3, 4) + (3, 1) = (3, 4)
```

### å½¢çŠ¶å˜æ¢

```python
x.view(3, 4)           # æ”¹å˜å½¢çŠ¶ï¼ˆè¦æ±‚è¿ç»­ï¼‰
x.reshape(3, 4)        # æ”¹å˜å½¢çŠ¶ï¼ˆæ›´çµæ´»ï¼‰
x.squeeze()            # ç§»é™¤ç»´åº¦=1 çš„ç»´
x.unsqueeze(0)         # å¢åŠ ç»´åº¦
x.permute(2, 0, 1)     # ç»´åº¦é‡æ’
torch.cat([a, b], dim=0)  # æ‹¼æ¥
torch.stack([a, b])       # å †å 
```

---

## 3ï¸âƒ£ è‡ªåŠ¨å¾®åˆ† (Autograd)

### æ ¸å¿ƒæ¦‚å¿µ

```python
x = torch.tensor([1.0], requires_grad=True)
y = x ** 2
y.backward()      # åå‘ä¼ æ’­
print(x.grad)     # æŸ¥çœ‹æ¢¯åº¦
```

### å…³é”®ç‚¹

- **æ¢¯åº¦ç´¯ç§¯**ï¼šæ¯æ¬¡ `backward()` æ¢¯åº¦ä¼šç´¯åŠ ï¼Œéœ€è¦ `zero_grad()`
- **è®¡ç®—å›¾**ï¼šåŠ¨æ€åˆ›å»ºï¼Œ`backward()` åé‡Šæ”¾
- **ç¦ç”¨æ¢¯åº¦**ï¼š`with torch.no_grad():` æˆ– `.detach()`

### å¸¸ç”¨æ¨¡å¼

```python
# è®­ç»ƒ
loss.backward()
optimizer.step()
optimizer.zero_grad()

# æ¨ç†
model.eval()
with torch.no_grad():
    output = model(x)
```

---

## 4ï¸âƒ£ nn.Module

### è‡ªå®šä¹‰æ¨¡å‹

```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 5)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)
```

### å¸¸ç”¨å±‚

| å±‚               | ç”¨é€”     |
| ---------------- | -------- |
| `nn.Linear`      | å…¨è¿æ¥å±‚ |
| `nn.Conv2d`      | 2D å·ç§¯  |
| `nn.BatchNorm2d` | æ‰¹å½’ä¸€åŒ– |
| `nn.Dropout`     | Dropout  |
| `nn.LSTM`        | LSTM     |
| `nn.Embedding`   | è¯åµŒå…¥   |

### å‚æ•°ç®¡ç†

```python
model.parameters()           # æ‰€æœ‰å‚æ•°
model.named_parameters()     # å¸¦åç§°çš„å‚æ•°
model.train()               # è®­ç»ƒæ¨¡å¼
model.eval()                # è¯„ä¼°æ¨¡å¼
```

---

## 5ï¸âƒ£ æŸå¤±å‡½æ•°

| æŸå¤±å‡½æ•°               | ç”¨é€”     | è¾“å…¥   |
| ---------------------- | -------- | ------ |
| `nn.CrossEntropyLoss`  | å¤šåˆ†ç±»   | logits |
| `nn.BCEWithLogitsLoss` | äºŒåˆ†ç±»   | logits |
| `nn.MSELoss`           | å›å½’     | é¢„æµ‹å€¼ |
| `nn.L1Loss`            | é²æ£’å›å½’ | é¢„æµ‹å€¼ |

### å…¸å‹ç”¨æ³•

```python
# å¤šåˆ†ç±»
criterion = nn.CrossEntropyLoss()
loss = criterion(logits, labels)  # labels æ˜¯ç±»åˆ«ç´¢å¼•

# äºŒåˆ†ç±»
criterion = nn.BCEWithLogitsLoss()
loss = criterion(logits, targets.float())
```

---

## 6ï¸âƒ£ ä¼˜åŒ–å™¨

### å¸¸ç”¨ä¼˜åŒ–å™¨

```python
optim.SGD(params, lr=0.01, momentum=0.9)
optim.Adam(params, lr=0.001)
optim.AdamW(params, lr=0.001, weight_decay=0.01)
```

### å­¦ä¹ ç‡è°ƒåº¦

```python
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# æ¯ä¸ª epoch è°ƒç”¨
scheduler.step()
```

### æ¢¯åº¦è£å‰ª

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## 7ï¸âƒ£ æ•°æ®åŠ è½½

### è‡ªå®šä¹‰ Dataset

```python
class MyDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]
```

### DataLoader

```python
loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)
```

---

## 8ï¸âƒ£ æ•°æ®å¢å¼º

### å›¾åƒå¢å¼º

```python
transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.2, 0.2, 0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])
```

### é«˜çº§å¢å¼º

- **MixUp**: æ··åˆä¸¤å¼ å›¾ç‰‡
- **CutOut**: éšæœºé®æŒ¡
- **CutMix**: å‰ªåˆ‡ç²˜è´´

---

## 9ï¸âƒ£ è®­ç»ƒå¾ªç¯

### å®Œæ•´æµç¨‹

```python
for epoch in range(n_epochs):
    model.train()
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        for x, y in val_loader:
            # éªŒè¯...

    scheduler.step()
```

---

## ğŸ”Ÿ æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### æ¨èæ–¹å¼

```python
# ä¿å­˜
torch.save(model.state_dict(), 'model.pth')

# åŠ è½½
model = MyModel()
model.load_state_dict(torch.load('model.pth'))
```

### å®Œæ•´ Checkpoint

```python
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')
```

---

## ğŸ”— æ ¸å¿ƒæµç¨‹å›¾

```
æ•°æ®å‡†å¤‡                   æ¨¡å‹æ„å»º                   è®­ç»ƒ
   â”‚                         â”‚                        â”‚
Dataset â”€â”€â†’ DataLoader   nn.Module â”€â”€â†’ forward()   è®­ç»ƒå¾ªç¯
   â”‚                         â”‚                        â”‚
transform                   å±‚ç»„åˆ                 optimizer.zero_grad()
   â”‚                         â”‚                        â”‚
collate_fn                å‚æ•°åˆå§‹åŒ–              loss.backward()
                             â”‚                        â”‚
                          to(device)             optimizer.step()
                                                      â”‚
                                              scheduler.step()
                                                      â”‚
                                              ä¿å­˜ checkpoint
```

---

## âœ… è‡ªæ£€æ¸…å•

- [ ] èƒ½åˆ›å»ºå’Œæ“ä½œ Tensorï¼Œç†è§£è®¾å¤‡ç®¡ç†
- [ ] ç†è§£ requires_grad å’Œ backward()
- [ ] èƒ½è‡ªå®šä¹‰ nn.Module æ„å»ºæ¨¡å‹
- [ ] çŸ¥é“å¸¸ç”¨æŸå¤±å‡½æ•°çš„é€‰æ‹©
- [ ] èƒ½é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
- [ ] èƒ½å®ç°è‡ªå®šä¹‰ Dataset å’Œ DataLoader
- [ ] ç†è§£å®Œæ•´è®­ç»ƒå¾ªç¯çš„å„ä¸ªæ­¥éª¤
- [ ] èƒ½ä¿å­˜å’ŒåŠ è½½æ¨¡å‹

---

## ğŸ“– æ¨èèµ„æº

1. [PyTorch å®˜æ–¹æ•™ç¨‹](https://pytorch.org/tutorials/)
2. [PyTorch é€ŸæŸ¥æ‰‹å†Œ](docs/PYTORCH_HANDBOOK.md)
3. [Deep Learning with PyTorch](https://pytorch.org/deep-learning-with-pytorch)
