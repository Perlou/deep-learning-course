"""
RLHF åŸç†
=========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£RLHFçš„ä¸‰é˜¶æ®µæµç¨‹
    2. äº†è§£å¥–åŠ±æ¨¡å‹çš„ä½œç”¨
    3. æŒæ¡DPOç­‰æ›¿ä»£æ–¹æ³•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šRLHFæ¦‚è¿° ====================


def introduction():
    """RLHFä»‹ç»"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šRLHFæ¦‚è¿°")
    print("=" * 60)

    print("""
RLHF: Reinforcement Learning from Human Feedback

ä¸ºä»€ä¹ˆéœ€è¦RLHFï¼Ÿ
    é¢„è®­ç»ƒ: å­¦ä¼šè¯­è¨€èƒ½åŠ›
    SFT: å­¦ä¼šéµå¾ªæŒ‡ä»¤
    RLHF: å­¦ä¼š"æ›´å¥½çš„"å›å¤ï¼ˆå¯¹é½äººç±»åå¥½ï¼‰

RLHFä¸‰é˜¶æ®µï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: SFT (ç›‘ç£å¾®è°ƒ)                                 â”‚
â”‚  ä½¿ç”¨é«˜è´¨é‡å¯¹è¯æ•°æ®å¾®è°ƒåŸºåº§æ¨¡å‹                           â”‚
â”‚                                                          â”‚
â”‚  Stage 2: å¥–åŠ±æ¨¡å‹è®­ç»ƒ (Reward Model)                    â”‚
â”‚  æ”¶é›†äººç±»åå¥½æ•°æ®ï¼šå¯¹åŒä¸€é—®é¢˜çš„å¤šä¸ªå›å¤æ’åº               â”‚
â”‚  è®­ç»ƒRMé¢„æµ‹äººç±»åå¥½                                       â”‚
â”‚                                                          â”‚
â”‚  Stage 3: PPOå¼ºåŒ–å­¦ä¹                                     â”‚
â”‚  ç”¨RMä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡PPOä¼˜åŒ–ç­–ç•¥æ¨¡å‹                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¯¹é½ç›®æ ‡ (HHH):
    - Helpful: æœ‰å¸®åŠ©
    - Honest: è¯šå®
    - Harmless: æ— å®³
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå¥–åŠ±æ¨¡å‹ ====================


class SimpleRewardModel(nn.Module):
    """ç®€åŒ–çš„å¥–åŠ±æ¨¡å‹"""

    def __init__(self, hidden_size):
        super().__init__()
        self.score_head = nn.Linear(hidden_size, 1)

    def forward(self, hidden_states):
        # å–æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        last_hidden = hidden_states[:, -1, :]
        reward = self.score_head(last_hidden)
        return reward.squeeze(-1)


def reward_model_training():
    """å¥–åŠ±æ¨¡å‹è®­ç»ƒ"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå¥–åŠ±æ¨¡å‹")
    print("=" * 60)

    print("""
å¥–åŠ±æ¨¡å‹è®­ç»ƒæ•°æ®ï¼š
    Query: "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"
    Response A: "ç¼–ç¨‹éœ€è¦å¾ªåºæ¸è¿›..."  (ğŸ‘ chosen)
    Response B: "ä¸Šç½‘æœä¸€ä¸‹"            (ğŸ‘ rejected)

è®­ç»ƒç›®æ ‡ (Bradley-Terryæ¨¡å‹):
    L = -log(Ïƒ(r(chosen) - r(rejected)))
    
    è®©chosençš„rewardé«˜äºrejected
    """)

    # æ¨¡æ‹Ÿè®­ç»ƒ
    hidden_size = 256
    rm = SimpleRewardModel(hidden_size)

    # æ¨¡æ‹Ÿéšè—çŠ¶æ€
    chosen_hidden = torch.randn(2, 10, hidden_size)
    rejected_hidden = torch.randn(2, 10, hidden_size)

    chosen_reward = rm(chosen_hidden)
    rejected_reward = rm(rejected_hidden)

    # åå¥½æŸå¤±
    loss = -F.logsigmoid(chosen_reward - rejected_reward).mean()

    print(f"\nChosen reward: {chosen_reward.mean().item():.4f}")
    print(f"Rejected reward: {rejected_reward.mean().item():.4f}")
    print(f"Preference loss: {loss.item():.4f}")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šDPO ====================


def dpo_explained():
    """DPOåŸç†"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šDPO (Direct Preference Optimization)")
    print("=" * 60)

    print("""
DPO: ç®€åŒ–RLHFï¼Œæ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹

æ ¸å¿ƒæ€æƒ³ï¼š
    ç›´æ¥ä»åå¥½æ•°æ®ä¼˜åŒ–ç­–ç•¥
    å°†RLé—®é¢˜è½¬åŒ–ä¸ºåˆ†ç±»é—®é¢˜

æŸå¤±å‡½æ•°ï¼š
    L = -log Ïƒ(Î² Ã— (log Ï€(y_w|x) - log Ï€(y_l|x)
                  - log Ï€_ref(y_w|x) + log Ï€_ref(y_l|x)))

å…¶ä¸­ï¼š
    y_w: åå¥½çš„å›å¤ (winner)
    y_l: ä¸åå¥½çš„å›å¤ (loser)  
    Ï€: å½“å‰ç­–ç•¥
    Ï€_ref: å‚è€ƒç­–ç•¥ï¼ˆSFTæ¨¡å‹ï¼‰
    Î²: æ¸©åº¦å‚æ•°

ä¼˜åŠ¿ï¼š
    - ä¸éœ€è¦å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹
    - è®­ç»ƒæ›´ç¨³å®š
    - å®ç°æ›´ç®€å•
    """)

    print("""
# DPOè®­ç»ƒä»£ç ç¤ºä¾‹
from trl import DPOTrainer

trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    train_dataset=preference_dataset,
    beta=0.1,  # æ¸©åº¦å‚æ•°
)
trainer.train()
    """)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šå¯¹æ¯” ====================


def comparison():
    """æ–¹æ³•å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šRLHF vs DPO")
    print("=" * 60)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           RLHF              vs           DPO            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  éœ€è¦3ä¸ªæ¨¡å‹:                 åªéœ€è¦2ä¸ªæ¨¡å‹:            â”‚
â”‚  - ç­–ç•¥æ¨¡å‹                   - ç­–ç•¥æ¨¡å‹                â”‚
â”‚  - å¥–åŠ±æ¨¡å‹                   - å‚è€ƒæ¨¡å‹                â”‚
â”‚  - å‚è€ƒæ¨¡å‹                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è®­ç»ƒå¤æ‚                     è®­ç»ƒç®€å•                  â”‚
â”‚  PPOä¸ç¨³å®š                    åˆ†ç±»æŸå¤±ç¨³å®š              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å¯ä»¥è¿­ä»£ä¼˜åŒ–                 ä¸€æ¬¡æ€§ä¼˜åŒ–                â”‚
â”‚  æ›´çµæ´»                       æ›´é«˜æ•ˆ                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®è·µé€‰æ‹©ï¼š
    - å¿«é€Ÿå®éªŒ: DPO
    - å¤§è§„æ¨¡ç”Ÿäº§: RLHF (æ›´å¯æ§)
    - èµ„æºå—é™: DPO
    """)


def main():
    introduction()
    reward_model_training()
    dpo_explained()
    comparison()

    print("\n" + "=" * 60)
    print("è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥: 07-quantization.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
