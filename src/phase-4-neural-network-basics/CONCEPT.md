# ç¥ç»ç½‘ç»œæ·±åº¦è§£æï¼šä»é›¶å¼€å§‹

---

## ğŸ“š ç›®å½•

1. [ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œ](#1-ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œ)
2. [ç”Ÿç‰©ç¥ç»å…ƒ vs äººå·¥ç¥ç»å…ƒ](#2-ç”Ÿç‰©ç¥ç»å…ƒ-vs-äººå·¥ç¥ç»å…ƒ)
3. [æ„ŸçŸ¥æœºï¼šæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ](#3-æ„ŸçŸ¥æœºæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ)
4. [å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰](#4-å¤šå±‚æ„ŸçŸ¥æœºmlp)
5. [æ¿€æ´»å‡½æ•°è¯¦è§£](#5-æ¿€æ´»å‡½æ•°è¯¦è§£)
6. [å‰å‘ä¼ æ’­](#6-å‰å‘ä¼ æ’­)
7. [æŸå¤±å‡½æ•°](#7-æŸå¤±å‡½æ•°)
8. [åå‘ä¼ æ’­ç®—æ³•](#8-åå‘ä¼ æ’­ç®—æ³•)
9. [ä¼˜åŒ–å™¨](#9-ä¼˜åŒ–å™¨)
10. [ç¥ç»ç½‘ç»œç±»å‹](#10-ç¥ç»ç½‘ç»œç±»å‹)
11. [å®æˆ˜ä»£ç ç¤ºä¾‹](#11-å®æˆ˜ä»£ç ç¤ºä¾‹)
12. [è®­ç»ƒæŠ€å·§ä¸è°ƒä¼˜](#12-è®­ç»ƒæŠ€å·§ä¸è°ƒä¼˜)
13. [æ€»ç»“ä¸å­¦ä¹ è·¯çº¿](#13-æ€»ç»“ä¸å­¦ä¹ è·¯çº¿)

---

## 1. ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œ

### 1.1 å®šä¹‰

**ç¥ç»ç½‘ç»œï¼ˆNeural Networkï¼‰** æ˜¯ä¸€ç§å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±å¤§é‡ç›¸äº’è¿æ¥çš„å¤„ç†å•å…ƒï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ æ•°æ®ä¸­çš„æ¨¡å¼æ¥æ‰§è¡Œå„ç§ä»»åŠ¡ã€‚

### 1.2 æ ¸å¿ƒæ€æƒ³

```
è¾“å…¥æ•°æ® â†’ å¤šå±‚å¤„ç† â†’ è¾“å‡ºç»“æœ
         â†‘
      é€šè¿‡å­¦ä¹ è°ƒæ•´å‚æ•°
```

### 1.3 å‘å±•å†ç¨‹

| å¹´ä»£  | é‡Œç¨‹ç¢‘                     | å…³é”®äººç‰©                    |
| ----- | -------------------------- | --------------------------- |
| 1943  | McCulloch-Pitts ç¥ç»å…ƒæ¨¡å‹ | McCulloch & Pitts           |
| 1958  | æ„ŸçŸ¥æœºï¼ˆPerceptronï¼‰       | Frank Rosenblatt            |
| 1986  | åå‘ä¼ æ’­ç®—æ³•               | Rumelhart, Hinton, Williams |
| 2006  | æ·±åº¦ä¿¡å¿µç½‘ç»œ               | Geoffrey Hinton             |
| 2012  | AlexNet èµ¢å¾— ImageNet      | Alex Krizhevsky             |
| 2017  | Transformer æ¶æ„           | Google å›¢é˜Ÿ                 |
| 2022+ | å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£             | OpenAI, Google ç­‰           |

---

## 2. ç”Ÿç‰©ç¥ç»å…ƒ vs äººå·¥ç¥ç»å…ƒ

### 2.1 ç”Ÿç‰©ç¥ç»å…ƒç»“æ„

```
æ ‘çªï¼ˆDendritesï¼‰ï¼šæ¥æ”¶ä¿¡å·
    â†“
ç»†èƒä½“ï¼ˆCell Bodyï¼‰ï¼šå¤„ç†ä¿¡å·
    â†“
è½´çªï¼ˆAxonï¼‰ï¼šä¼ é€’ä¿¡å·
    â†“
çªè§¦ï¼ˆSynapseï¼‰ï¼šè¿æ¥ä¸‹ä¸€ä¸ªç¥ç»å…ƒ
```

### 2.2 äººå·¥ç¥ç»å…ƒæ¨¡å‹

```
è¾“å…¥(xâ‚, xâ‚‚, ..., xâ‚™)
        â†“
    åŠ æƒæ±‚å’Œï¼šz = Î£(wáµ¢Â·xáµ¢) + b
        â†“
    æ¿€æ´»å‡½æ•°ï¼ša = f(z)
        â†“
      è¾“å‡º(a)
```

### 2.3 æ•°å­¦è¡¨è¾¾

$$z = \sum_{i=1}^{n} w_i x_i + b = \mathbf{w}^T \mathbf{x} + b$$

$$a = f(z)$$

å…¶ä¸­ï¼š

- $x_i$ï¼šè¾“å…¥
- $w_i$ï¼šæƒé‡ï¼ˆWeightï¼‰
- $b$ï¼šåç½®ï¼ˆBiasï¼‰
- $f$ï¼šæ¿€æ´»å‡½æ•°
- $a$ï¼šè¾“å‡º

---

## 3. æ„ŸçŸ¥æœºï¼šæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ

### 3.1 å•å±‚æ„ŸçŸ¥æœºç»“æ„

```
    xâ‚ ----wâ‚----\
                  \
    xâ‚‚ ----wâ‚‚----â†’ Î£ + b â†’ f(z) â†’ y
                  /
    xâ‚ƒ ----wâ‚ƒ----/
```

### 3.2 Python å®ç°

```python
import numpy as np

class Perceptron:
    def __init__(self, input_size, learning_rate=0.01):
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.lr = learning_rate

    def activation(self, z):
        """é˜¶è·ƒå‡½æ•°"""
        return 1 if z >= 0 else 0

    def predict(self, x):
        """å‰å‘ä¼ æ’­"""
        z = np.dot(self.weights, x) + self.bias
        return self.activation(z)

    def train(self, X, y, epochs=100):
        """è®­ç»ƒæ„ŸçŸ¥æœº"""
        for epoch in range(epochs):
            for xi, yi in zip(X, y):
                prediction = self.predict(xi)
                error = yi - prediction
                # æ›´æ–°è§„åˆ™
                self.weights += self.lr * error * xi
                self.bias += self.lr * error

# ç¤ºä¾‹ï¼šå­¦ä¹ ANDé—¨
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])

perceptron = Perceptron(input_size=2)
perceptron.train(X, y)

# æµ‹è¯•
for xi in X:
    print(f"{xi} -> {perceptron.predict(xi)}")
```

### 3.3 æ„ŸçŸ¥æœºçš„å±€é™æ€§

**é—®é¢˜ï¼šæ— æ³•è§£å†³ XOR é—®é¢˜ï¼ˆçº¿æ€§ä¸å¯åˆ†ï¼‰**

```
XORçœŸå€¼è¡¨ï¼š        å›¾ç¤ºï¼š
0 XOR 0 = 0       1 â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹ 0
0 XOR 1 = 1         â”‚         â”‚
1 XOR 0 = 1         â”‚    âœ—    â”‚  â† æ— æ³•ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€
1 XOR 1 = 0       0 â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â— 1
                    0         1
```

**è§£å†³æ–¹æ¡ˆï¼šå¤šå±‚ç¥ç»ç½‘ç»œ**

---

## 4. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

### 4.1 ç½‘ç»œç»“æ„

```
è¾“å…¥å±‚        éšè—å±‚           è¾“å‡ºå±‚
(Input)      (Hidden)        (Output)

  â—‹             â—‹
   \          / | \
  â—‹ â€”â€”â€”â€”â€”â€”â†’  â—‹  |  â—‹ â€”â€”â€”â€”â€”â€”â†’  â—‹
   /\        \ | / \         /
  â—‹  \        \|/   â—‹ â€”â€”â€”â€”â€”â€”â—‹
      \        â—‹   /
       \â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”/

xâ‚, xâ‚‚, xâ‚ƒ    hâ‚, hâ‚‚, hâ‚ƒ      yâ‚, yâ‚‚
```

### 4.2 å±‚çš„ç±»å‹

| å±‚ç±»å‹ | è¯´æ˜           | ç‰¹ç‚¹           |
| ------ | -------------- | -------------- |
| è¾“å…¥å±‚ | æ¥æ”¶åŸå§‹æ•°æ®   | æ— è®¡ç®—ï¼Œä»…ä¼ é€’ |
| éšè—å±‚ | ç‰¹å¾æå–å’Œå˜æ¢ | å¯æœ‰å¤šå±‚       |
| è¾“å‡ºå±‚ | äº§ç”Ÿæœ€ç»ˆç»“æœ   | æ ¹æ®ä»»åŠ¡è®¾è®¡   |

### 4.3 å…¨è¿æ¥å±‚ï¼ˆDense Layerï¼‰

æ¯ä¸ªç¥ç»å…ƒä¸ä¸Šä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿ï¼š

```python
class DenseLayer:
    def __init__(self, input_size, output_size):
        # Xavieråˆå§‹åŒ–
        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)
        self.bias = np.zeros((1, output_size))

    def forward(self, x):
        self.input = x
        self.output = np.dot(x, self.weights) + self.bias
        return self.output

    def backward(self, grad_output, learning_rate):
        # è®¡ç®—æ¢¯åº¦
        grad_input = np.dot(grad_output, self.weights.T)
        grad_weights = np.dot(self.input.T, grad_output)
        grad_bias = np.sum(grad_output, axis=0, keepdims=True)

        # æ›´æ–°å‚æ•°
        self.weights -= learning_rate * grad_weights
        self.bias -= learning_rate * grad_bias

        return grad_input
```

---

## 5. æ¿€æ´»å‡½æ•°è¯¦è§£

### 5.1 ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

**æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç½‘ç»œç­‰ä»·äºå•å±‚ï¼š**

$$h_1 = W_1 x$$
$$h_2 = W_2 h_1 = W_2 W_1 x = W' x$$

æ¿€æ´»å‡½æ•°å¼•å…¥**éçº¿æ€§**ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚æ¨¡å¼ã€‚

### 5.2 å¸¸ç”¨æ¿€æ´»å‡½æ•°å¯¹æ¯”

#### Sigmoid

```
f(x) = 1 / (1 + e^(-x))

ç‰¹ç‚¹ï¼š
âœ… è¾“å‡ºèŒƒå›´ (0, 1)ï¼Œé€‚åˆæ¦‚ç‡
âŒ æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
âŒ éé›¶ä¸­å¿ƒåŒ–

     1 |        ___________
       |      /
   0.5 |----/---------------
       |  /
     0 |/___________________
       -4   -2   0   2   4
```

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)
```

#### Tanh

```
f(x) = (e^x - e^(-x)) / (e^x + e^(-x))

ç‰¹ç‚¹ï¼š
âœ… è¾“å‡ºèŒƒå›´ (-1, 1)
âœ… é›¶ä¸­å¿ƒåŒ–
âŒ æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

     1 |        ___________
       |      /
     0 |----/---------------
       |  /
    -1 |/___________________
       -4   -2   0   2   4
```

```python
def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2
```

#### ReLUï¼ˆæœ€å¸¸ç”¨ï¼‰

```
f(x) = max(0, x)

ç‰¹ç‚¹ï¼š
âœ… è®¡ç®—ç®€å•é«˜æ•ˆ
âœ… ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
âŒ æ­»äº¡ReLUé—®é¢˜ï¼ˆç¥ç»å…ƒæ°¸ä¹…å¤±æ´»ï¼‰

     y |
       |        /
       |      /
       |    /
     0 |___/________________
       -2   0   2   4   x
```

```python
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)
```

#### Leaky ReLU

```
f(x) = x if x > 0 else Î±x  (é€šå¸¸ Î± = 0.01)

ç‰¹ç‚¹ï¼š
âœ… è§£å†³æ­»äº¡ReLUé—®é¢˜
âœ… ä¿æŒReLUä¼˜ç‚¹
```

```python
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)
```

#### Softmaxï¼ˆè¾“å‡ºå±‚ç”¨äºå¤šåˆ†ç±»ï¼‰

```
softmax(xáµ¢) = e^(xáµ¢) / Î£â±¼ e^(xâ±¼)

ç‰¹ç‚¹ï¼š
âœ… è¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆå’Œä¸º1ï¼‰
âœ… ç”¨äºå¤šåˆ†ç±»é—®é¢˜
```

```python
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # æ•°å€¼ç¨³å®šæ€§
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
```

### 5.3 æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     å¦‚ä½•é€‰æ‹©æ¿€æ´»å‡½æ•°ï¼Ÿ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  éšè—å±‚:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ é¦–é€‰: ReLU                                            â”‚  â”‚
â”‚  â”‚ å¦‚æœReLUæ•ˆæœä¸å¥½: å°è¯• Leaky ReLU, ELU, GELU          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  è¾“å‡ºå±‚:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ äºŒåˆ†ç±»: Sigmoid                                        â”‚  â”‚
â”‚  â”‚ å¤šåˆ†ç±»: Softmax                                        â”‚  â”‚
â”‚  â”‚ å›å½’: æ— æ¿€æ´»ï¼ˆçº¿æ€§ï¼‰æˆ– ReLUï¼ˆæ­£å€¼è¾“å‡ºï¼‰                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. å‰å‘ä¼ æ’­

### 6.1 è®¡ç®—æµç¨‹

```
è¾“å…¥ X
   â†“
[ç¬¬1å±‚] Zâ‚ = XÂ·Wâ‚ + bâ‚ â†’ Aâ‚ = f(Zâ‚)
   â†“
[ç¬¬2å±‚] Zâ‚‚ = Aâ‚Â·Wâ‚‚ + bâ‚‚ â†’ Aâ‚‚ = f(Zâ‚‚)
   â†“
   ...
   â†“
[è¾“å‡ºå±‚] Zâ‚— = Aâ‚—â‚‹â‚Â·Wâ‚— + bâ‚— â†’ Å¶ = f(Zâ‚—)
   â†“
è®¡ç®—æŸå¤± L(Y, Å¶)
```

### 6.2 çŸ©é˜µç»´åº¦åˆ†æ

å‡è®¾æœ‰ä¸€ä¸ª 3 å±‚ç½‘ç»œï¼šè¾“å…¥å±‚(4) â†’ éšè—å±‚(5) â†’ è¾“å‡ºå±‚(3)

```
æ‰¹æ¬¡å¤§å° m = 32

X:   (32, 4)    - 32ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª4ä¸ªç‰¹å¾
Wâ‚:  (4, 5)     - ç¬¬ä¸€å±‚æƒé‡
bâ‚:  (1, 5)     - ç¬¬ä¸€å±‚åç½®
Zâ‚:  (32, 5)    - çº¿æ€§å˜æ¢ç»“æœ
Aâ‚:  (32, 5)    - æ¿€æ´»åç»“æœ

Wâ‚‚:  (5, 3)     - ç¬¬äºŒå±‚æƒé‡
bâ‚‚:  (1, 3)     - ç¬¬äºŒå±‚åç½®
Zâ‚‚:  (32, 3)    - çº¿æ€§å˜æ¢ç»“æœ
Å¶:   (32, 3)    - æœ€ç»ˆè¾“å‡º
```

### 6.3 ä»£ç å®ç°

```python
class NeuralNetwork:
    def __init__(self, layer_sizes):
        """
        layer_sizes: [è¾“å…¥ç»´åº¦, éšè—å±‚1å¤§å°, éšè—å±‚2å¤§å°, ..., è¾“å‡ºç»´åº¦]
        """
        self.layers = []
        self.activations = []

        for i in range(len(layer_sizes) - 1):
            self.layers.append({
                'W': np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01,
                'b': np.zeros((1, layer_sizes[i+1]))
            })

    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.activations = [X]
        self.z_values = []

        A = X
        for i, layer in enumerate(self.layers):
            Z = np.dot(A, layer['W']) + layer['b']
            self.z_values.append(Z)

            # æœ€åä¸€å±‚ç”¨softmaxï¼Œå…¶ä»–ç”¨ReLU
            if i == len(self.layers) - 1:
                A = softmax(Z)
            else:
                A = relu(Z)

            self.activations.append(A)

        return A
```

---

## 7. æŸå¤±å‡½æ•°

### 7.1 ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ

æŸå¤±å‡½æ•°è¡¡é‡**é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®è·**ï¼Œæ˜¯æˆ‘ä»¬è¦æœ€å°åŒ–çš„ç›®æ ‡ã€‚

### 7.2 å¸¸ç”¨æŸå¤±å‡½æ•°

#### å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰- å›å½’ä»»åŠ¡

$$L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

```python
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def mse_gradient(y_true, y_pred):
    return 2 * (y_pred - y_true) / y_true.shape[0]
```

#### äºŒå…ƒäº¤å‰ç†µï¼ˆBCEï¼‰- äºŒåˆ†ç±»ä»»åŠ¡

$$L = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

```python
def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15  # é˜²æ­¢log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
```

#### åˆ†ç±»äº¤å‰ç†µï¼ˆCCEï¼‰- å¤šåˆ†ç±»ä»»åŠ¡

$$L = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})$$

```python
def categorical_cross_entropy(y_true, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
```

### 7.3 æŸå¤±å‡½æ•°é€‰æ‹©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ä»»åŠ¡ç±»å‹                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    å›å½’     â”‚    äºŒåˆ†ç±»      â”‚    å¤šåˆ†ç±»    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    MSE      â”‚     BCE       â”‚     CCE      â”‚
â”‚    MAE      â”‚  + Sigmoid    â”‚  + Softmax   â”‚
â”‚   Huber     â”‚               â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. åå‘ä¼ æ’­ç®—æ³•

### 8.1 æ ¸å¿ƒæ€æƒ³ï¼šé“¾å¼æ³•åˆ™

åå‘ä¼ æ’­ä½¿ç”¨**é“¾å¼æ³•åˆ™**è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚

$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}$$

### 8.2 å›¾è§£åå‘ä¼ æ’­

```
å‰å‘ä¼ æ’­:
X â”€â”€â†’ [Zâ‚=XWâ‚+bâ‚] â”€â”€â†’ [Aâ‚=ReLU(Zâ‚)] â”€â”€â†’ [Zâ‚‚=Aâ‚Wâ‚‚+bâ‚‚] â”€â”€â†’ [Å¶=Softmax(Zâ‚‚)] â”€â”€â†’ L

åå‘ä¼ æ’­:
âˆ‚L/âˆ‚Wâ‚ â†â”€â”€ âˆ‚L/âˆ‚Zâ‚ â†â”€â”€ âˆ‚L/âˆ‚Aâ‚ â†â”€â”€ âˆ‚L/âˆ‚Zâ‚‚ â†â”€â”€ âˆ‚L/âˆ‚Å¶ â†â”€â”€ L
```

### 8.3 æ•°å­¦æ¨å¯¼ï¼ˆä»¥ä¸¤å±‚ç½‘ç»œä¸ºä¾‹ï¼‰

**ç½‘ç»œç»“æ„ï¼š**

- è¾“å…¥ï¼š$X$
- ç¬¬ä¸€å±‚ï¼š$Z_1 = XW_1 + b_1$ï¼Œ$A_1 = \text{ReLU}(Z_1)$
- ç¬¬äºŒå±‚ï¼š$Z_2 = A_1W_2 + b_2$ï¼Œ$\hat{Y} = \text{Softmax}(Z_2)$
- æŸå¤±ï¼š$L = \text{CCE}(Y, \hat{Y})$

**åå‘ä¼ æ’­æ­¥éª¤ï¼š**

```
Step 1: è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦
âˆ‚L/âˆ‚Zâ‚‚ = Å¶ - Y  (Softmax + CCEçš„ç®€åŒ–å½¢å¼)

Step 2: è®¡ç®—ç¬¬äºŒå±‚å‚æ•°æ¢¯åº¦
âˆ‚L/âˆ‚Wâ‚‚ = Aâ‚áµ€ Â· (âˆ‚L/âˆ‚Zâ‚‚)
âˆ‚L/âˆ‚bâ‚‚ = sum(âˆ‚L/âˆ‚Zâ‚‚, axis=0)

Step 3: ä¼ æ’­åˆ°éšè—å±‚
âˆ‚L/âˆ‚Aâ‚ = (âˆ‚L/âˆ‚Zâ‚‚) Â· Wâ‚‚áµ€
âˆ‚L/âˆ‚Zâ‚ = âˆ‚L/âˆ‚Aâ‚ âŠ™ ReLU'(Zâ‚)  (âŠ™è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•)

Step 4: è®¡ç®—ç¬¬ä¸€å±‚å‚æ•°æ¢¯åº¦
âˆ‚L/âˆ‚Wâ‚ = Xáµ€ Â· (âˆ‚L/âˆ‚Zâ‚)
âˆ‚L/âˆ‚bâ‚ = sum(âˆ‚L/âˆ‚Zâ‚, axis=0)
```

### 8.4 å®Œæ•´ä»£ç å®ç°

```python
class NeuralNetwork:
    def __init__(self, layer_sizes):
        self.num_layers = len(layer_sizes) - 1
        self.weights = []
        self.biases = []

        for i in range(self.num_layers):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        self.activations = [X]
        self.z_values = []

        A = X
        for i in range(self.num_layers):
            Z = np.dot(A, self.weights[i]) + self.biases[i]
            self.z_values.append(Z)

            if i == self.num_layers - 1:
                A = softmax(Z)  # è¾“å‡ºå±‚
            else:
                A = relu(Z)     # éšè—å±‚

            self.activations.append(A)

        return A

    def backward(self, Y, learning_rate):
        m = Y.shape[0]
        gradients_w = []
        gradients_b = []

        # è¾“å‡ºå±‚æ¢¯åº¦ (Softmax + Cross-entropy)
        dZ = self.activations[-1] - Y

        for i in range(self.num_layers - 1, -1, -1):
            # å‚æ•°æ¢¯åº¦
            dW = np.dot(self.activations[i].T, dZ) / m
            db = np.sum(dZ, axis=0, keepdims=True) / m

            gradients_w.insert(0, dW)
            gradients_b.insert(0, db)

            if i > 0:
                # ä¼ æ’­åˆ°å‰ä¸€å±‚
                dA = np.dot(dZ, self.weights[i].T)
                dZ = dA * relu_derivative(self.z_values[i-1])

        # æ›´æ–°å‚æ•°
        for i in range(self.num_layers):
            self.weights[i] -= learning_rate * gradients_w[i]
            self.biases[i] -= learning_rate * gradients_b[i]

    def train(self, X, Y, epochs, learning_rate, batch_size=32):
        history = {'loss': [], 'accuracy': []}

        for epoch in range(epochs):
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = np.random.permutation(X.shape[0])
            X_shuffled = X[indices]
            Y_shuffled = Y[indices]

            epoch_loss = 0
            for i in range(0, X.shape[0], batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                Y_batch = Y_shuffled[i:i+batch_size]

                # å‰å‘ä¼ æ’­
                output = self.forward(X_batch)

                # è®¡ç®—æŸå¤±
                loss = categorical_cross_entropy(Y_batch, output)
                epoch_loss += loss

                # åå‘ä¼ æ’­
                self.backward(Y_batch, learning_rate)

            # è®°å½•å†å²
            avg_loss = epoch_loss / (X.shape[0] // batch_size)
            predictions = self.predict(X)
            accuracy = np.mean(predictions == np.argmax(Y, axis=1))

            history['loss'].append(avg_loss)
            history['accuracy'].append(accuracy)

            if epoch % 100 == 0:
                print(f"Epoch {epoch}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}")

        return history

    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)
```

---

## 9. ä¼˜åŒ–å™¨

### 9.1 æ¢¯åº¦ä¸‹é™å˜ä½“

#### æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBGDï¼‰

```python
# ä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—æ¢¯åº¦
weights -= learning_rate * gradient_of_entire_dataset
```

- âœ… ç¨³å®šæ”¶æ•›
- âŒ è®¡ç®—æ…¢ï¼Œå†…å­˜å ç”¨å¤§

#### éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰

```python
# æ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬
for sample in dataset:
    gradient = compute_gradient(sample)
    weights -= learning_rate * gradient
```

- âœ… å¿«é€Ÿæ›´æ–°
- âŒ å™ªå£°å¤§ï¼Œä¸ç¨³å®š

#### å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch SGDï¼‰

```python
# ä½¿ç”¨å°æ‰¹é‡æ•°æ®ï¼ˆå¦‚32ä¸ªæ ·æœ¬ï¼‰
for batch in batches:
    gradient = compute_gradient(batch)
    weights -= learning_rate * gradient
```

- âœ… å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§
- âœ… å®é™…æœ€å¸¸ç”¨

### 9.2 é«˜çº§ä¼˜åŒ–å™¨

#### Momentumï¼ˆåŠ¨é‡ï¼‰

```python
class MomentumOptimizer:
    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocity = {}

    def update(self, params, grads):
        for key in params:
            if key not in self.velocity:
                self.velocity[key] = np.zeros_like(params[key])

            self.velocity[key] = self.momentum * self.velocity[key] - self.lr * grads[key]
            params[key] += self.velocity[key]
```

#### Adamï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
class AdamOptimizer:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {}  # ä¸€é˜¶çŸ©
        self.v = {}  # äºŒé˜¶çŸ©
        self.t = 0   # æ—¶é—´æ­¥

    def update(self, params, grads):
        self.t += 1

        for key in params:
            if key not in self.m:
                self.m[key] = np.zeros_like(params[key])
                self.v[key] = np.zeros_like(params[key])

            # æ›´æ–°åŠ¨é‡
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)

            # åå·®æ ¡æ­£
            m_hat = self.m[key] / (1 - self.beta1 ** self.t)
            v_hat = self.v[key] / (1 - self.beta2 ** self.t)

            # æ›´æ–°å‚æ•°
            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
```

### 9.3 ä¼˜åŒ–å™¨å¯¹æ¯”

```
               æ”¶æ•›é€Ÿåº¦    å†…å­˜    è¶…å‚æ•°    é€‚ç”¨åœºæ™¯
SGD             æ…¢        ä½      å°‘      ç®€å•ä»»åŠ¡
Momentum        ä¸­        ä½      å°‘      éœ€è¦åŠ é€Ÿçš„ä»»åŠ¡
RMSprop         å¿«        ä¸­      ä¸­      RNN/éå¹³ç¨³ç›®æ ‡
Adam            å¿«        ä¸­      ä¸­      é»˜è®¤é¦–é€‰
AdamW           å¿«        ä¸­      ä¸­      éœ€è¦æ­£åˆ™åŒ–æ—¶
```

---

## 10. ç¥ç»ç½‘ç»œç±»å‹

### 10.1 å…¨è¿æ¥ç½‘ç»œï¼ˆFCN/MLPï¼‰

```
é€‚ç”¨ï¼šç»“æ„åŒ–æ•°æ®ï¼ˆè¡¨æ ¼æ•°æ®ï¼‰
ç»“æ„ï¼šæ¯å±‚å…¨è¿æ¥

[Input] â†’ [Dense] â†’ [Dense] â†’ [Output]
```

### 10.2 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

```
é€‚ç”¨ï¼šå›¾åƒã€è§†é¢‘ã€ç©ºé—´æ•°æ®
æ ¸å¿ƒæ“ä½œï¼šå·ç§¯ã€æ± åŒ–

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾“å…¥å›¾åƒ  â†’  å·ç§¯å±‚  â†’  æ± åŒ–å±‚  â†’  å…¨è¿æ¥å±‚  â†’  è¾“å‡º      â”‚
â”‚                                                          â”‚
â”‚  [å›¾åƒ]  â†’  [ç‰¹å¾å›¾] â†’  [ä¸‹é‡‡æ ·] â†’  [å‘é‡]  â†’  [åˆ†ç±»]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# PyTorchå®ç°ç®€å•CNN
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
```

### 10.3 å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰

```
é€‚ç”¨ï¼šåºåˆ—æ•°æ®ï¼ˆæ–‡æœ¬ã€æ—¶é—´åºåˆ—ï¼‰
ç‰¹ç‚¹ï¼šæœ‰è®°å¿†èƒ½åŠ›

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                        â”‚
     â†“                                        â”‚
[hâ‚€] â†’ [RNN Cell] â†’ [hâ‚] â†’ [RNN Cell] â†’ [hâ‚‚] â†’ [RNN Cell] â†’ [hâ‚ƒ]
          â†‘                  â†‘                   â†‘
         xâ‚                 xâ‚‚                  xâ‚ƒ
```

```python
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        # x: (batch, seq_len, input_size)
        out, hidden = self.rnn(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        out = self.fc(out[:, -1, :])
        return out
```

### 10.4 LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ï¼‰

```
è§£å†³RNNçš„é•¿æœŸä¾èµ–é—®é¢˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             LSTM Cell                    â”‚
â”‚                                          â”‚
â”‚  é—å¿˜é—¨ â†’ å†³å®šä¸¢å¼ƒå“ªäº›ä¿¡æ¯                â”‚
â”‚  è¾“å…¥é—¨ â†’ å†³å®šå­˜å‚¨å“ªäº›æ–°ä¿¡æ¯              â”‚
â”‚  è¾“å‡ºé—¨ â†’ å†³å®šè¾“å‡ºå“ªäº›ä¿¡æ¯                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.5 Transformer

```
é€‚ç”¨ï¼šNLPã€è®¡ç®—æœºè§†è§‰
æ ¸å¿ƒï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Transformer                          â”‚
â”‚                                                             â”‚
â”‚  è¾“å…¥ â†’ [ä½ç½®ç¼–ç ] â†’ [å¤šå¤´æ³¨æ„åŠ›] â†’ [å‰é¦ˆç½‘ç»œ] â†’ è¾“å‡º         â”‚
â”‚                          â†‘                                  â”‚
â”‚                     è‡ªæ³¨æ„åŠ›æœºåˆ¶                             â”‚
â”‚                   "æ¯ä¸ªè¯çœ‹æ‰€æœ‰è¯"                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super().__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        self.queries = nn.Linear(embed_size, embed_size)
        self.keys = nn.Linear(embed_size, embed_size)
        self.values = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)

    def forward(self, x):
        N, seq_len, _ = x.shape

        Q = self.queries(x)
        K = self.keys(x)
        V = self.values(x)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attention = torch.matmul(Q, K.transpose(-2, -1)) / (self.embed_size ** 0.5)
        attention = torch.softmax(attention, dim=-1)

        out = torch.matmul(attention, V)
        return self.fc_out(out)
```

### 10.6 ç½‘ç»œç±»å‹æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å¦‚ä½•é€‰æ‹©ç½‘ç»œç»“æ„ï¼Ÿ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ•°æ®ç±»å‹    â”‚                  æ¨èç½‘ç»œ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¡¨æ ¼æ•°æ®    â”‚  MLP, TabNet, XGBoost+NN                        â”‚
â”‚  å›¾åƒ       â”‚  CNN (ResNet, EfficientNet, ViT)                 â”‚
â”‚  æ–‡æœ¬       â”‚  Transformer (BERT, GPT), RNN/LSTM               â”‚
â”‚  æ—¶é—´åºåˆ—   â”‚  LSTM, GRU, Transformer, TCN                     â”‚
â”‚  å›¾ç»“æ„     â”‚  GNN (GCN, GAT, GraphSAGE)                       â”‚
â”‚  å¤šæ¨¡æ€     â”‚  å¤šè¾“å…¥ç½‘ç»œ, CLIP                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. å®æˆ˜ä»£ç ç¤ºä¾‹

### 11.1 NumPy ä»é›¶å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt

# æ¿€æ´»å‡½æ•°
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

def cross_entropy(y_true, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

# å®Œæ•´çš„ç¥ç»ç½‘ç»œç±»
class NeuralNetworkFromScratch:
    def __init__(self, layers):
        """
        layers: æ¯å±‚ç¥ç»å…ƒæ•°é‡çš„åˆ—è¡¨
        ä¾‹å¦‚: [784, 128, 64, 10] è¡¨ç¤ºè¾“å…¥784ç»´ï¼Œä¸¤ä¸ªéšè—å±‚ï¼Œè¾“å‡º10ç±»
        """
        self.layers = layers
        self.num_layers = len(layers) - 1

        # åˆå§‹åŒ–å‚æ•°ï¼ˆHeåˆå§‹åŒ–ï¼‰
        self.params = {}
        for i in range(self.num_layers):
            self.params[f'W{i}'] = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            self.params[f'b{i}'] = np.zeros((1, layers[i+1]))

        self.cache = {}

    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.cache['A0'] = X
        A = X

        for i in range(self.num_layers):
            Z = np.dot(A, self.params[f'W{i}']) + self.params[f'b{i}']
            self.cache[f'Z{i}'] = Z

            if i == self.num_layers - 1:
                A = softmax(Z)  # è¾“å‡ºå±‚
            else:
                A = relu(Z)     # éšè—å±‚

            self.cache[f'A{i+1}'] = A

        return A

    def backward(self, Y):
        """åå‘ä¼ æ’­"""
        m = Y.shape[0]
        grads = {}

        # è¾“å‡ºå±‚æ¢¯åº¦
        dZ = self.cache[f'A{self.num_layers}'] - Y

        for i in range(self.num_layers - 1, -1, -1):
            A_prev = self.cache[f'A{i}']

            grads[f'W{i}'] = np.dot(A_prev.T, dZ) / m
            grads[f'b{i}'] = np.sum(dZ, axis=0, keepdims=True) / m

            if i > 0:
                dA = np.dot(dZ, self.params[f'W{i}'].T)
                dZ = dA * relu_derivative(self.cache[f'Z{i-1}'])

        return grads

    def update_params(self, grads, learning_rate):
        """æ›´æ–°å‚æ•°"""
        for i in range(self.num_layers):
            self.params[f'W{i}'] -= learning_rate * grads[f'W{i}']
            self.params[f'b{i}'] -= learning_rate * grads[f'b{i}']

    def train(self, X_train, Y_train, X_val, Y_val, epochs=1000, lr=0.01, batch_size=32):
        """è®­ç»ƒæ¨¡å‹"""
        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

        n_samples = X_train.shape[0]

        for epoch in range(epochs):
            # æ‰“ä¹±æ•°æ®
            indices = np.random.permutation(n_samples)
            X_shuffled = X_train[indices]
            Y_shuffled = Y_train[indices]

            # å°æ‰¹é‡è®­ç»ƒ
            for i in range(0, n_samples, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                Y_batch = Y_shuffled[i:i+batch_size]

                # å‰å‘ä¼ æ’­
                self.forward(X_batch)

                # åå‘ä¼ æ’­
                grads = self.backward(Y_batch)

                # æ›´æ–°å‚æ•°
                self.update_params(grads, lr)

            # è®°å½•æŒ‡æ ‡
            train_pred = self.forward(X_train)
            val_pred = self.forward(X_val)

            train_loss = cross_entropy(Y_train, train_pred)
            val_loss = cross_entropy(Y_val, val_pred)
            train_acc = np.mean(np.argmax(train_pred, axis=1) == np.argmax(Y_train, axis=1))
            val_acc = np.mean(np.argmax(val_pred, axis=1) == np.argmax(Y_val, axis=1))

            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)

            if epoch % 100 == 0:
                print(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}")

        return history

    def predict(self, X):
        """é¢„æµ‹"""
        probs = self.forward(X)
        return np.argmax(probs, axis=1)


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆå¤šåˆ†ç±»ï¼‰
    np.random.seed(42)

    # åˆ›å»ºèºæ—‹æ•°æ®é›†
    def create_spiral_data(samples_per_class, classes):
        X = np.zeros((samples_per_class * classes, 2))
        Y = np.zeros((samples_per_class * classes, classes))

        for class_idx in range(classes):
            idx = range(samples_per_class * class_idx, samples_per_class * (class_idx + 1))
            r = np.linspace(0.0, 1, samples_per_class)
            t = np.linspace(class_idx * 4, (class_idx + 1) * 4, samples_per_class) + np.random.randn(samples_per_class) * 0.2

            X[idx] = np.c_[r * np.sin(t * 2.5), r * np.cos(t * 2.5)]
            Y[idx, class_idx] = 1

        return X, Y

    # ç”Ÿæˆæ•°æ®
    X, Y = create_spiral_data(100, 3)

    # åˆ†å‰²æ•°æ®
    split = int(0.8 * len(X))
    X_train, X_val = X[:split], X[split:]
    Y_train, Y_val = Y[:split], Y[split:]

    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    model = NeuralNetworkFromScratch([2, 64, 32, 3])
    history = model.train(X_train, Y_train, X_val, Y_val, epochs=1000, lr=0.1, batch_size=32)

    # å¯è§†åŒ–ç»“æœ
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # æŸå¤±æ›²çº¿
    axes[0].plot(history['train_loss'], label='Train Loss')
    axes[0].plot(history['val_loss'], label='Val Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].set_title('Loss Curve')

    # å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(history['train_acc'], label='Train Acc')
    axes[1].plot(history['val_acc'], label='Val Acc')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].legend()
    axes[1].set_title('Accuracy Curve')

    # å†³ç­–è¾¹ç•Œ
    h = 0.02
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axes[2].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
    axes[2].scatter(X[:, 0], X[:, 1], c=np.argmax(Y, axis=1), cmap=plt.cm.RdYlBu, edgecolors='black')
    axes[2].set_title('Decision Boundary')

    plt.tight_layout()
    plt.show()
```

### 11.2 PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class PyTorchNN(nn.Module):
    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.2):
        super().__init__()

        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, num_classes))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# è®­ç»ƒå‡½æ•°
def train_pytorch_model(model, train_loader, val_loader, epochs=100, lr=0.001):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        for X_batch, Y_batch in train_loader:
            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, Y_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for X_batch, Y_batch in val_loader:
                X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, Y_batch)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += Y_batch.size(0)
                correct += (predicted == Y_batch).sum().item()

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        val_acc = correct / total

        scheduler.step(avg_val_loss)

        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_acc)

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}")

    return history

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®
    X_train = torch.randn(1000, 20)
    Y_train = torch.randint(0, 5, (1000,))
    X_val = torch.randn(200, 20)
    Y_val = torch.randint(0, 5, (200,))

    train_dataset = TensorDataset(X_train, Y_train)
    val_dataset = TensorDataset(X_val, Y_val)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    model = PyTorchNN(input_size=20, hidden_sizes=[64, 32], num_classes=5)
    history = train_pytorch_model(model, train_loader, val_loader, epochs=100)
```

---

## 12. è®­ç»ƒæŠ€å·§ä¸è°ƒä¼˜

### 12.1 æ­£åˆ™åŒ–æŠ€æœ¯

#### L1/L2 æ­£åˆ™åŒ–

```python
# L2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰
loss = cross_entropy_loss + lambda * sum(w^2)

# PyTorchä¸­ä½¿ç”¨
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
```

#### Dropout

```python
class DropoutLayer:
    def __init__(self, dropout_rate=0.5):
        self.rate = dropout_rate
        self.mask = None

    def forward(self, x, training=True):
        if training:
            self.mask = (np.random.rand(*x.shape) > self.rate) / (1 - self.rate)
            return x * self.mask
        return x

    def backward(self, grad):
        return grad * self.mask
```

#### æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰

```python
class BatchNorm:
    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):
        self.gamma = np.ones(num_features)
        self.beta = np.zeros(num_features)
        self.epsilon = epsilon
        self.momentum = momentum
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)

    def forward(self, x, training=True):
        if training:
            mean = np.mean(x, axis=0)
            var = np.var(x, axis=0)

            # æ›´æ–°è¿è¡Œç»Ÿè®¡é‡
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var
        else:
            mean = self.running_mean
            var = self.running_var

        x_norm = (x - mean) / np.sqrt(var + self.epsilon)
        return self.gamma * x_norm + self.beta
```

### 12.2 å­¦ä¹ ç‡è°ƒåº¦

```python
# å­¦ä¹ ç‡è¡°å‡ç­–ç•¥
class LearningRateScheduler:
    @staticmethod
    def step_decay(epoch, initial_lr=0.01, drop=0.5, epochs_drop=10):
        """é˜¶æ¢¯è¡°å‡"""
        return initial_lr * (drop ** (epoch // epochs_drop))

    @staticmethod
    def exponential_decay(epoch, initial_lr=0.01, decay_rate=0.96):
        """æŒ‡æ•°è¡°å‡"""
        return initial_lr * (decay_rate ** epoch)

    @staticmethod
    def cosine_annealing(epoch, initial_lr=0.01, T_max=100):
        """ä½™å¼¦é€€ç«"""
        return initial_lr * (1 + np.cos(np.pi * epoch / T_max)) / 2
```

### 12.3 æƒé‡åˆå§‹åŒ–

```python
def xavier_init(shape):
    """é€‚ç”¨äºtanh/sigmoid"""
    fan_in, fan_out = shape[0], shape[1]
    std = np.sqrt(2.0 / (fan_in + fan_out))
    return np.random.randn(*shape) * std

def he_init(shape):
    """é€‚ç”¨äºReLU"""
    fan_in = shape[0]
    std = np.sqrt(2.0 / fan_in)
    return np.random.randn(*shape) * std
```

### 12.4 æ—©åœæ³•ï¼ˆEarly Stoppingï¼‰

```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = np.inf
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.best_weights = model.get_weights()
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                model.set_weights(self.best_weights)
                return True
        return False
```

### 12.5 è¶…å‚æ•°è°ƒä¼˜æ¸…å•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¶…å‚æ•°è°ƒä¼˜æŒ‡å—                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     è¶…å‚æ•°       â”‚                    å»ºè®®                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   å­¦ä¹ ç‡        â”‚ ä»0.001å¼€å§‹ï¼Œä½¿ç”¨å­¦ä¹ ç‡finderæ‰¾æœ€ä½³å€¼              â”‚
â”‚   æ‰¹æ¬¡å¤§å°      â”‚ 32-256ï¼ŒGPUæ˜¾å­˜å…è®¸è¶Šå¤§è¶Šå¥½                       â”‚
â”‚   éšè—å±‚æ•°      â”‚ ä»2-3å±‚å¼€å§‹ï¼Œé€æ­¥å¢åŠ                              â”‚
â”‚   éšè—å•å…ƒæ•°    â”‚ 64-512ï¼Œé€å±‚é€’å‡ï¼ˆå¦‚256â†’128â†’64ï¼‰                  â”‚
â”‚   Dropout      â”‚ 0.2-0.5ï¼Œè¿‡æ‹Ÿåˆæ—¶å¢åŠ                              â”‚
â”‚   æƒé‡è¡°å‡      â”‚ 1e-4åˆ°1e-5                                       â”‚
â”‚   ä¼˜åŒ–å™¨        â”‚ Adamé¦–é€‰ï¼ŒSGD+Momentumä¹Ÿå¸¸ç”¨                      â”‚
â”‚   æ¿€æ´»å‡½æ•°      â”‚ ReLUåŠå…¶å˜ä½“ï¼ˆLeakyReLU, GELUï¼‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 13. æ€»ç»“ä¸å­¦ä¹ è·¯çº¿

### 13.1 æ ¸å¿ƒçŸ¥è¯†å›¾è°±

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   ç¥ç»ç½‘ç»œ                       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                               â”‚                               â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  åŸºç¡€   â”‚                   â”‚    ç½‘ç»œç±»å‹   â”‚                 â”‚    è®­ç»ƒ     â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚                               â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚ ç¥ç»å…ƒ  â”‚                   â”‚  å…¨è¿æ¥(MLP) â”‚                 â”‚   å‰å‘ä¼ æ’­   â”‚
    â”‚ æƒé‡/åç½®â”‚                   â”‚   CNN       â”‚                 â”‚   æŸå¤±å‡½æ•°   â”‚
    â”‚ æ¿€æ´»å‡½æ•° â”‚                   â”‚   RNN/LSTM  â”‚                 â”‚   åå‘ä¼ æ’­   â”‚
    â”‚  å±‚     â”‚                   â”‚ Transformer â”‚                 â”‚   ä¼˜åŒ–å™¨     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   æ­£åˆ™åŒ–     â”‚
                                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.2 å­¦ä¹ è·¯çº¿

```
ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ç†è®ºï¼ˆ2-4å‘¨ï¼‰
â”œâ”€â”€ çº¿æ€§ä»£æ•°åŸºç¡€ï¼ˆçŸ©é˜µè¿ç®—ã€å‘é‡ç©ºé—´ï¼‰
â”œâ”€â”€ å¾®ç§¯åˆ†ï¼ˆå¯¼æ•°ã€é“¾å¼æ³•åˆ™ã€æ¢¯åº¦ï¼‰
â”œâ”€â”€ æ¦‚ç‡ç»Ÿè®¡ï¼ˆæ¦‚ç‡åˆ†å¸ƒã€è´å¶æ–¯ï¼‰
â””â”€â”€ Pythonç¼–ç¨‹ï¼ˆNumPyã€Matplotlibï¼‰

ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒæ¦‚å¿µï¼ˆ4-6å‘¨ï¼‰
â”œâ”€â”€ æ„ŸçŸ¥æœºå’ŒMLP
â”œâ”€â”€ æ¿€æ´»å‡½æ•°
â”œâ”€â”€ æŸå¤±å‡½æ•°
â”œâ”€â”€ åå‘ä¼ æ’­
â””â”€â”€ ä¼˜åŒ–å™¨

ç¬¬ä¸‰é˜¶æ®µï¼šæ·±åº¦ç½‘ç»œï¼ˆ6-8å‘¨ï¼‰
â”œâ”€â”€ CNNï¼ˆå·ç§¯ã€æ± åŒ–ã€ç»å…¸æ¶æ„ï¼‰
â”œâ”€â”€ RNN/LSTM/GRU
â”œâ”€â”€ æ­£åˆ™åŒ–æŠ€æœ¯
â””â”€â”€ æ¡†æ¶ä½¿ç”¨ï¼ˆPyTorch/TensorFlowï¼‰

ç¬¬å››é˜¶æ®µï¼šè¿›é˜¶ä¸»é¢˜ï¼ˆæŒç»­å­¦ä¹ ï¼‰
â”œâ”€â”€ Transformerå’Œæ³¨æ„åŠ›æœºåˆ¶
â”œâ”€â”€ ç”Ÿæˆæ¨¡å‹ï¼ˆGAN, VAE, Diffusionï¼‰
â”œâ”€â”€ å¼ºåŒ–å­¦ä¹ 
â”œâ”€â”€ å›¾ç¥ç»ç½‘ç»œ
â””â”€â”€ æ¨¡å‹éƒ¨ç½²å’Œä¼˜åŒ–
```

### 13.3 æ¨èèµ„æº

| ç±»å‹ | èµ„æº                       | é€‚åˆé˜¶æ®µ  |
| ---- | -------------------------- | --------- |
| è¯¾ç¨‹ | å´æ©è¾¾ã€Šæ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹ã€‹ | å…¥é—¨      |
| è¯¾ç¨‹ | æå®æ¯…ã€Šæœºå™¨å­¦ä¹ ã€‹         | å…¥é—¨-è¿›é˜¶ |
| è¯¾ç¨‹ | CS231nï¼ˆæ–¯å¦ç¦ CNN è¯¾ç¨‹ï¼‰  | è¿›é˜¶      |
| ä¹¦ç± | ã€Šæ·±åº¦å­¦ä¹ ã€‹èŠ±ä¹¦           | ç†è®º      |
| ä¹¦ç± | ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹         | å®è·µ      |
| å®è·µ | Kaggle ç«èµ›                | åº”ç”¨      |
| è®ºæ–‡ | arXiv.org                  | å‰æ²¿      |

### 13.4 å¿«é€Ÿå›é¡¾

```python
"""
ç¥ç»ç½‘ç»œæ ¸å¿ƒå…¬å¼é€ŸæŸ¥ï¼š

1. å‰å‘ä¼ æ’­ï¼š
   Z = X @ W + b
   A = activation(Z)

2. å¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼š
   ReLU: max(0, x)
   Sigmoid: 1 / (1 + exp(-x))
   Softmax: exp(x) / sum(exp(x))

3. æŸå¤±å‡½æ•°ï¼š
   MSE: mean((y - Å·)Â²)
   Cross-Entropy: -mean(y * log(Å·))

4. æ¢¯åº¦ä¸‹é™ï¼š
   W = W - lr * âˆ‚L/âˆ‚W

5. åå‘ä¼ æ’­ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š
   âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚A * âˆ‚A/âˆ‚Z * âˆ‚Z/âˆ‚W
"""
```

---

## ğŸ¯ å®è·µå»ºè®®

1. **åŠ¨æ‰‹å®ç°**ï¼šå…ˆç”¨ NumPy æ‰‹å†™ï¼Œç†è§£åŸç†åå†ç”¨æ¡†æ¶
2. **å¯è§†åŒ–**ï¼šç”»å‡ºç½‘ç»œç»“æ„ã€æŸå¤±æ›²çº¿ã€å†³ç­–è¾¹ç•Œ
3. **è°ƒè¯•æŠ€å·§**ï¼šæ£€æŸ¥æ¢¯åº¦ã€æ‰“å°ä¸­é—´å€¼ã€ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•
4. **é¡¹ç›®é©±åŠ¨**ï¼šé€‰æ‹©æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€æ–‡æœ¬ç”Ÿæˆï¼‰
5. **é˜…è¯»è®ºæ–‡**ï¼šä»ç»å…¸è®ºæ–‡å¼€å§‹ï¼ˆAlexNet, ResNet, Transformerï¼‰

---

**ç¥ä½ å­¦ä¹ æ„‰å¿«ï¼ğŸš€**

_å¦‚æœ‰é—®é¢˜æ¬¢è¿ç»§ç»­æ¢è®¨_
