"""
09-weight-init.py
Phase 4: ç¥ç»ç½‘ç»œåŸºç¡€

æƒé‡åˆå§‹åŒ– - è®­ç»ƒæˆåŠŸçš„å…³é”®

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£æƒé‡åˆå§‹åŒ–çš„é‡è¦æ€§
2. æŒæ¡ Xavier å’Œ He åˆå§‹åŒ–
3. äº†è§£ä¸åŒæ¿€æ´»å‡½æ•°å¯¹åº”çš„åˆå§‹åŒ–ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.nn.init as init
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 60)
print("ç¥ç»ç½‘ç»œåŸºç¡€ - æƒé‡åˆå§‹åŒ–")
print("=" * 60)

# =============================================================================
# 1. ä¸ºä»€ä¹ˆæƒé‡åˆå§‹åŒ–é‡è¦
# =============================================================================
print("\nã€1. ä¸ºä»€ä¹ˆæƒé‡åˆå§‹åŒ–é‡è¦ã€‘")

print("""
é—®é¢˜:
- åˆå§‹åŒ–å¤ªå°: ä¿¡å·åœ¨å‰å‘ä¼ æ’­ä¸­é€æ¸æ¶ˆå¤±
- åˆå§‹åŒ–å¤ªå¤§: ä¿¡å·åœ¨å‰å‘ä¼ æ’­ä¸­çˆ†ç‚¸

ç†æƒ³æƒ…å†µ:
- ä¿æŒæ¯å±‚è¾“å‡ºçš„æ–¹å·®ç¨³å®š
- ä¿æŒæ¢¯åº¦çš„æ–¹å·®ç¨³å®š

ä¸å¥½çš„åˆå§‹åŒ–åæœ:
- è®­ç»ƒç¼“æ…¢æˆ–ä¸æ”¶æ•›
- æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- é™·å…¥å±€éƒ¨æœ€ä¼˜
""")

# =============================================================================
# 2. éªŒè¯åˆå§‹åŒ–çš„å½±å“
# =============================================================================
print("\n" + "=" * 60)
print("ã€2. éªŒè¯åˆå§‹åŒ–çš„å½±å“ã€‘")

def forward_pass_stats(init_fn, n_layers=10, width=256):
    """è®¡ç®—å‰å‘ä¼ æ’­ä¸­æ¿€æ´»å€¼çš„ç»Ÿè®¡é‡"""
    layers = []
    for i in range(n_layers):
        linear = nn.Linear(width, width, bias=False)
        init_fn(linear.weight)
        layers.append(linear)
    
    x = torch.randn(32, width)
    means = []
    stds = []
    
    for layer in layers:
        x = torch.tanh(layer(x))  # ä½¿ç”¨ tanh æ¿€æ´»
        means.append(x.mean().item())
        stds.append(x.std().item())
    
    return means, stds

# ä¸åŒåˆå§‹åŒ–æ–¹æ³•
print("\nä¸åŒåˆå§‹åŒ–çš„æ¿€æ´»å€¼å˜åŒ–:")

# å¤ªå°
means_small, stds_small = forward_pass_stats(lambda w: init.normal_(w, std=0.01))
print(f"å¤ªå° (std=0.01): æœ€åä¸€å±‚ std = {stds_small[-1]:.6f}")

# å¤ªå¤§
means_large, stds_large = forward_pass_stats(lambda w: init.normal_(w, std=1.0))
print(f"å¤ªå¤§ (std=1.0): æœ€åä¸€å±‚ std = {stds_large[-1]:.6f}")

# Xavier
means_xavier, stds_xavier = forward_pass_stats(lambda w: init.xavier_uniform_(w))
print(f"Xavier: æœ€åä¸€å±‚ std = {stds_xavier[-1]:.6f}")

# å¯è§†åŒ–
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(stds_small, 'b-o', label='å¤ªå° (std=0.01)', markersize=6)
plt.plot(stds_large, 'r-s', label='å¤ªå¤§ (std=1.0)', markersize=6)
plt.plot(stds_xavier, 'g-^', label='Xavier', markersize=6)
plt.xlabel('å±‚ç¼–å·')
plt.ylabel('æ¿€æ´»å€¼æ ‡å‡†å·®')
plt.title('å‰å‘ä¼ æ’­ä¸­çš„æ¿€æ´»å€¼å˜åŒ–')
plt.legend()
plt.grid(True, alpha=0.3)
plt.yscale('log')

plt.subplot(1, 2, 2)
# å¯è§†åŒ–æƒé‡åˆ†å¸ƒ
w_small = torch.zeros(256, 256)
w_large = torch.zeros(256, 256)
w_xavier = torch.zeros(256, 256)
init.normal_(w_small, std=0.01)
init.normal_(w_large, std=1.0)
init.xavier_uniform_(w_xavier)

plt.hist(w_small.flatten().numpy(), bins=50, alpha=0.5, label='å¤ªå°', density=True)
plt.hist(w_xavier.flatten().numpy(), bins=50, alpha=0.5, label='Xavier', density=True)
plt.xlabel('æƒé‡å€¼')
plt.ylabel('å¯†åº¦')
plt.title('åˆå§‹æƒé‡åˆ†å¸ƒ')
plt.legend()

plt.tight_layout()
plt.savefig('outputs/init_comparison.png', dpi=100)
plt.close()
print("åˆå§‹åŒ–æ¯”è¾ƒå›¾å·²ä¿å­˜: outputs/init_comparison.png")

# =============================================================================
# 3. Xavier åˆå§‹åŒ–
# =============================================================================
print("\n" + "=" * 60)
print("ã€3. Xavier åˆå§‹åŒ– (Glorot)ã€‘")

print("""
Xavier åˆå§‹åŒ– (Glorot & Bengio, 2010):

åŸç†: ä¿æŒå‰å‘å’Œåå‘ä¼ æ’­ä¸­æ–¹å·®ç¨³å®š

å…¬å¼:
    Uniform: W ~ U[-âˆš(6/(fan_in+fan_out)), âˆš(6/(fan_in+fan_out))]
    Normal:  W ~ N(0, 2/(fan_in+fan_out))

å…¶ä¸­:
    fan_in = è¾“å…¥ç¥ç»å…ƒæ•°
    fan_out = è¾“å‡ºç¥ç»å…ƒæ•°

é€‚ç”¨:
    - Sigmoid, Tanh æ¿€æ´»å‡½æ•°
    - çº¿æ€§æ¿€æ´»
""")

# æ¼”ç¤º
linear = nn.Linear(256, 128)
init.xavier_uniform_(linear.weight)
print(f"Xavier Uniform: std = {linear.weight.std():.4f}")
print(f"ç†è®º std = âˆš(2/(256+128)) = {np.sqrt(2/384):.4f}")

init.xavier_normal_(linear.weight)
print(f"Xavier Normal: std = {linear.weight.std():.4f}")

# =============================================================================
# 4. He åˆå§‹åŒ– (Kaiming)
# =============================================================================
print("\n" + "=" * 60)
print("ã€4. He åˆå§‹åŒ– (Kaiming)ã€‘")

print("""
He åˆå§‹åŒ– (He et al., 2015):

åŸç†: é’ˆå¯¹ ReLU æ¿€æ´»å‡½æ•°ä¼˜åŒ–

å…¬å¼:
    Normal:  W ~ N(0, 2/fan_in)
    Uniform: W ~ U[-âˆš(6/fan_in), âˆš(6/fan_in)]

æ”¹è¿›:
    - ReLU ä¼šå°†ä¸€åŠçš„æ¿€æ´»ç½®é›¶
    - éœ€è¦æ–¹å·®åŠ å€è¡¥å¿

mode å‚æ•°:
    - 'fan_in': ä¿æŒå‰å‘ä¼ æ’­æ–¹å·®
    - 'fan_out': ä¿æŒåå‘ä¼ æ’­æ–¹å·®

é€‚ç”¨:
    - ReLU, Leaky ReLU æ¿€æ´»å‡½æ•°
""")

# éªŒè¯ He åˆå§‹åŒ–å¯¹ ReLU çš„æ•ˆæœ
def forward_pass_relu(init_fn, n_layers=10, width=256):
    layers = []
    for i in range(n_layers):
        linear = nn.Linear(width, width, bias=False)
        init_fn(linear.weight)
        layers.append(linear)
    
    x = torch.randn(32, width)
    stds = []
    
    for layer in layers:
        x = torch.relu(layer(x))  # ä½¿ç”¨ ReLU
        stds.append(x.std().item())
    
    return stds

print("\nReLU ç½‘ç»œæµ‹è¯•:")
stds_xavier_relu = forward_pass_relu(lambda w: init.xavier_normal_(w))
stds_he_relu = forward_pass_relu(lambda w: init.kaiming_normal_(w, mode='fan_in', nonlinearity='relu'))

print(f"Xavier + ReLU: æœ€åä¸€å±‚ std = {stds_xavier_relu[-1]:.4f}")
print(f"He + ReLU: æœ€åä¸€å±‚ std = {stds_he_relu[-1]:.4f}")

# =============================================================================
# 5. PyTorch åˆå§‹åŒ–å‡½æ•°
# =============================================================================
print("\n" + "=" * 60)
print("ã€5. PyTorch åˆå§‹åŒ–å‡½æ•°ã€‘")

print("""
å¸¸ç”¨åˆå§‹åŒ–å‡½æ•°:

init.zeros_(tensor)           # å…¨é›¶
init.ones_(tensor)            # å…¨ä¸€
init.constant_(tensor, val)   # å¸¸æ•°
init.normal_(tensor, mean, std)   # æ­£æ€åˆ†å¸ƒ
init.uniform_(tensor, a, b)       # å‡åŒ€åˆ†å¸ƒ
init.xavier_uniform_(tensor)      # Xavier å‡åŒ€
init.xavier_normal_(tensor)       # Xavier æ­£æ€
init.kaiming_uniform_(tensor)     # He å‡åŒ€
init.kaiming_normal_(tensor)      # He æ­£æ€
init.orthogonal_(tensor)          # æ­£äº¤åˆå§‹åŒ–
init.sparse_(tensor, sparsity)    # ç¨€ç–åˆå§‹åŒ–
""")

# æ¼”ç¤º
linear = nn.Linear(256, 128)

# He åˆå§‹åŒ–
init.kaiming_normal_(linear.weight, mode='fan_out', nonlinearity='relu')
init.zeros_(linear.bias)  # bias é€šå¸¸åˆå§‹åŒ–ä¸º 0
print(f"He Normal (fan_out): std = {linear.weight.std():.4f}")

# =============================================================================
# 6. è‡ªå®šä¹‰åˆå§‹åŒ–
# =============================================================================
print("\n" + "=" * 60)
print("ã€6. è‡ªå®šä¹‰åˆå§‹åŒ–ã€‘")

def init_weights(m):
    """è‡ªå®šä¹‰æ¨¡å‹åˆå§‹åŒ–å‡½æ•°"""
    if isinstance(m, nn.Linear):
        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            init.zeros_(m.bias)
    elif isinstance(m, nn.Conv2d):
        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            init.zeros_(m.bias)
    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
        init.ones_(m.weight)
        init.zeros_(m.bias)

# åº”ç”¨åˆ°æ¨¡å‹
class SampleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(256, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
    
    def forward(self, x):
        x = torch.relu(self.bn1(self.fc1(x)))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

model = SampleModel()
model.apply(init_weights)  # åº”ç”¨è‡ªå®šä¹‰åˆå§‹åŒ–
print("è‡ªå®šä¹‰åˆå§‹åŒ–å·²åº”ç”¨")

# éªŒè¯
for name, param in model.named_parameters():
    if 'weight' in name:
        print(f"  {name}: std = {param.std():.4f}")

# =============================================================================
# 7. åˆå§‹åŒ–å¯¹è®­ç»ƒçš„å½±å“
# =============================================================================
print("\n" + "=" * 60)
print("ã€7. åˆå§‹åŒ–å¯¹è®­ç»ƒçš„å½±å“ã€‘")

from torch.utils.data import DataLoader, TensorDataset

# åˆ›å»ºæ•°æ®
np.random.seed(42)
X = np.random.randn(1000, 128)
y = (X[:, 0] + X[:, 1] > 0).astype(np.float32)
X_t, y_t = torch.FloatTensor(X), torch.FloatTensor(y).unsqueeze(1)
loader = DataLoader(TensorDataset(X_t, y_t), batch_size=64, shuffle=True)

def train_with_init(init_name, init_fn, epochs=100):
    torch.manual_seed(42)
    model = nn.Sequential(
        nn.Linear(128, 64), nn.ReLU(),
        nn.Linear(64, 32), nn.ReLU(),
        nn.Linear(32, 1), nn.Sigmoid()
    )
    
    # åº”ç”¨åˆå§‹åŒ–
    for m in model:
        if isinstance(m, nn.Linear):
            init_fn(m.weight)
            init.zeros_(m.bias)
    
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    criterion = nn.BCELoss()
    losses = []
    
    for epoch in range(epochs):
        total_loss = 0
        for x, y in loader:
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        losses.append(total_loss / len(loader))
    
    return losses

print("è®­ç»ƒä¸åŒåˆå§‹åŒ–çš„æ¨¡å‹...")
losses_zero = train_with_init('é›¶åˆå§‹åŒ–', lambda w: init.zeros_(w))
losses_small = train_with_init('å¤ªå°', lambda w: init.normal_(w, std=0.001))
losses_large = train_with_init('å¤ªå¤§', lambda w: init.normal_(w, std=1.0))
losses_xavier = train_with_init('Xavier', lambda w: init.xavier_normal_(w))
losses_he = train_with_init('He', lambda w: init.kaiming_normal_(w, nonlinearity='relu'))

print(f"é›¶åˆå§‹åŒ–: æœ€ç»ˆ loss = {losses_zero[-1]:.4f}")
print(f"å¤ªå°: æœ€ç»ˆ loss = {losses_small[-1]:.4f}")
print(f"å¤ªå¤§: æœ€ç»ˆ loss = {losses_large[-1]:.4f}")
print(f"Xavier: æœ€ç»ˆ loss = {losses_xavier[-1]:.4f}")
print(f"He: æœ€ç»ˆ loss = {losses_he[-1]:.4f}")

# å¯è§†åŒ–
plt.figure(figsize=(10, 5))
plt.plot(losses_zero, label='é›¶åˆå§‹åŒ–', linewidth=2, alpha=0.7)
plt.plot(losses_small, label='å¤ªå° (std=0.001)', linewidth=2, alpha=0.7)
plt.plot(losses_large, label='å¤ªå¤§ (std=1.0)', linewidth=2, alpha=0.7)
plt.plot(losses_xavier, label='Xavier', linewidth=2)
plt.plot(losses_he, label='He/Kaiming', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('ä¸åŒåˆå§‹åŒ–æ–¹æ³•çš„è®­ç»ƒæ›²çº¿')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('outputs/init_training.png', dpi=100)
plt.close()
print("è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜: outputs/init_training.png")

# =============================================================================
# 8. åˆå§‹åŒ–é€‰æ‹©æŒ‡å—
# =============================================================================
print("\n" + "=" * 60)
print("ã€8. åˆå§‹åŒ–é€‰æ‹©æŒ‡å—ã€‘")

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   åˆå§‹åŒ–æ–¹æ³•é€‰æ‹©æŒ‡å—                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æ¿€æ´»å‡½æ•°          â•‘  æ¨èåˆå§‹åŒ–                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Sigmoid, Tanh    â•‘  Xavier (Glorot)                              â•‘
â•‘  ReLU             â•‘  He (Kaiming, mode='fan_in')                  â•‘
â•‘  Leaky ReLU       â•‘  He (nonlinearity='leaky_relu')               â•‘
â•‘  SELU             â•‘  LeCun Normal                                 â•‘
â•‘  æ— æ¿€æ´»           â•‘  Xavier                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ç‰¹æ®Šå±‚            â•‘  åˆå§‹åŒ–                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  BatchNorm        â•‘  weight=1, bias=0                             â•‘
â•‘  Embedding        â•‘  Normal(0, 1) æˆ–é¢„è®­ç»ƒ                        â•‘
â•‘  æ®‹å·®è¿æ¥æœ€åå±‚   â•‘  åˆå§‹åŒ–ä¸º 0                                    â•‘
â•‘  Transformer      â•‘  Xavier + ç‰¹æ®Šç¼©æ”¾                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PyTorch é»˜è®¤:
- nn.Linear: Kaiming Uniform (é€‚åˆ ReLU)
- nn.Conv2d: Kaiming Uniform (é€‚åˆ ReLU)
""")

# =============================================================================
# 9. ç»ƒä¹ é¢˜
# =============================================================================
print("\n" + "=" * 60)
print("ã€ç»ƒä¹ é¢˜ã€‘")
print("=" * 60)

print("""
1. æ¨å¯¼ Xavier åˆå§‹åŒ–çš„æ–¹å·®å…¬å¼

2. è§£é‡Šä¸ºä»€ä¹ˆ He åˆå§‹åŒ–è¦ä¹˜ä»¥ 2

3. å®ç°æ­£äº¤åˆå§‹åŒ– (Orthogonal)

4. åˆ†æ LSTM åº”è¯¥ä½¿ç”¨ä»€ä¹ˆåˆå§‹åŒ–

5. æµ‹è¯•æ·±å±‚ç½‘ç»œ (50å±‚) ä¸åŒåˆå§‹åŒ–çš„æ¢¯åº¦ä¼ æ’­
""")

# === ç­”æ¡ˆæç¤º ===
# 1: Var(y) = n * Var(w) * Var(x)
#    è¦ä¿æŒ Var(y) = Var(x)ï¼Œéœ€è¦ Var(w) = 1/n

# 2: ReLU å°†çº¦ä¸€åŠçš„æ¿€æ´»ç½®é›¶
#    ä¸ºä¿æŒæ–¹å·®ï¼Œéœ€è¦ Var(w) = 2/n

# 3: æ­£äº¤åˆå§‹åŒ–
# Q, _ = torch.linalg.qr(torch.randn(n, n))
# weight.data = Q[:weight.shape[0], :weight.shape[1]]

# 4: LSTM:
#    - è¾“å…¥å’Œéšè—æƒé‡ç”¨ Xavier
#    - é—å¿˜é—¨ bias åˆå§‹åŒ–ä¸º 1-2

# 5: æ·±å±‚ç½‘ç»œå®éªŒ
# for depth in [10, 30, 50]:
#     test_gradient_flow(depth, init_method)

print("\nâœ… æƒé‡åˆå§‹åŒ–å®Œæˆï¼")
print("ğŸ‰ Phase 4 å…¨éƒ¨æ¨¡å—å®Œæˆï¼")
print("\nä¸‹ä¸€æ­¥ï¼šå®Œæˆæˆ¿ä»·é¢„æµ‹å®æˆ˜é¡¹ç›®")
