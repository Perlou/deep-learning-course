# æ³¨æ„åŠ›æœºåˆ¶ä¸ Transformer æ¶æ„æ·±åº¦è§£æ

> ä»é›¶å¼€å§‹ï¼Œå…¨é¢ç†è§£ç°ä»£æ·±åº¦å­¦ä¹ æœ€é‡è¦çš„æ¶æ„

---

## ç›®å½•

1. [å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦æ³¨æ„åŠ›æœºåˆ¶](#1-å¼•è¨€ä¸ºä»€ä¹ˆéœ€è¦æ³¨æ„åŠ›æœºåˆ¶)
2. [æ³¨æ„åŠ›æœºåˆ¶åŸºç¡€](#2-æ³¨æ„åŠ›æœºåˆ¶åŸºç¡€)
3. [è‡ªæ³¨æ„åŠ›æœºåˆ¶](#3-è‡ªæ³¨æ„åŠ›æœºåˆ¶self-attention)
4. [Transformer æ¶æ„å…¨è§£](#4-transformeræ¶æ„å…¨è§£)
5. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#5-æ ¸å¿ƒç»„ä»¶è¯¦è§£)
6. [å®Œæ•´ä»£ç å®ç°](#6-å®Œæ•´ä»£ç å®ç°)
7. [Transformer å˜ä½“ä¸åº”ç”¨](#7-transformerå˜ä½“ä¸åº”ç”¨)
8. [æ€»ç»“ä¸å±•æœ›](#8-æ€»ç»“ä¸å±•æœ›)

---

## 1. å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦æ³¨æ„åŠ›æœºåˆ¶

### 1.1 ä¼ ç»Ÿåºåˆ—æ¨¡å‹çš„å›°å¢ƒ

åœ¨æ³¨æ„åŠ›æœºåˆ¶å‡ºç°ä¹‹å‰ï¼Œå¤„ç†åºåˆ—æ•°æ®ä¸»è¦ä¾èµ–**RNN**ï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰åŠå…¶å˜ä½“**LSTM**ã€**GRU**ã€‚

```
ä¼ ç»ŸRNNå¤„ç†æµç¨‹:
è¾“å…¥: "æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†"
      â†“    â†“    â†“    â†“    â†“
     h1 â†’ h2 â†’ h3 â†’ h4 â†’ h5 â†’ è¾“å‡º
```

**å­˜åœ¨çš„é—®é¢˜ï¼š**

| é—®é¢˜              | æè¿°                                 |
| ----------------- | ------------------------------------ |
| **é•¿ç¨‹ä¾èµ–é—®é¢˜**  | è·ç¦»è¾ƒè¿œçš„è¯ä¹‹é—´éš¾ä»¥å»ºç«‹æœ‰æ•ˆè”ç³»     |
| **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸** | åå‘ä¼ æ’­æ—¶æ¢¯åº¦é€å±‚è¡°å‡æˆ–å¢å¤§         |
| **é¡ºåºè®¡ç®—**      | å¿…é¡»æŒ‰æ—¶é—´æ­¥é¡ºåºå¤„ç†ï¼Œæ— æ³•å¹¶è¡ŒåŒ–     |
| **ä¿¡æ¯ç“¶é¢ˆ**      | æ‰€æœ‰å†å²ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šç»´åº¦çš„éšè—çŠ¶æ€ |

### 1.2 æ³¨æ„åŠ›çš„ç›´è§‰ç†è§£

æƒ³è±¡ä½ åœ¨é˜…è¯»ä¸€ç¯‡é•¿æ–‡ç« ï¼Œå›ç­”é—®é¢˜"ä½œè€…çš„å®¶ä¹¡æ˜¯å“ªé‡Œï¼Ÿ"æ—¶ï¼š

- âŒ **ä¸ä¼š**ï¼šé€å­—é˜…è¯»æ•´ç¯‡æ–‡ç« 
- âœ… **ä¼šåš**ï¼šå¿«é€Ÿæ‰«æï¼Œ**èšç„¦**äºä¸"å®¶ä¹¡"ç›¸å…³çš„å¥å­

è¿™å°±æ˜¯**æ³¨æ„åŠ›æœºåˆ¶**çš„æ ¸å¿ƒæ€æƒ³ï¼š**åŠ¨æ€åœ°å…³æ³¨è¾“å…¥ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†**ã€‚

---

## 2. æ³¨æ„åŠ›æœºåˆ¶åŸºç¡€

### 2.1 æ³¨æ„åŠ›çš„æ•°å­¦æŠ½è±¡

æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æŠ½è±¡ä¸ºä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ³¨æ„åŠ›ä¸‰è¦ç´                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query (Q)  : æŸ¥è¯¢å‘é‡ - "æˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ"                  â”‚
â”‚  Key (K)    : é”®å‘é‡   - "æˆ‘æœ‰ä»€ä¹ˆå¯ä»¥æä¾›ï¼Ÿ"            â”‚
â”‚  Value (V)  : å€¼å‘é‡   - "å®é™…çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç±»æ¯”ï¼šå›¾ä¹¦é¦†æŸ¥ä¹¦**

- **Query**ï¼šä½ æƒ³æ‰¾çš„ä¹¦çš„å…³é”®è¯ï¼ˆå¦‚"æ·±åº¦å­¦ä¹ "ï¼‰
- **Key**ï¼šæ¯æœ¬ä¹¦çš„æ ‡ç­¾/ç´¢å¼•
- **Value**ï¼šä¹¦çš„å®é™…å†…å®¹

### 2.2 æ³¨æ„åŠ›è®¡ç®—å…¬å¼

**æ ¸å¿ƒå…¬å¼**ï¼š

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**é€æ­¥åˆ†è§£**ï¼š

```
æ­¥éª¤1: è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
       Score = Q Â· K^T

æ­¥éª¤2: ç¼©æ”¾ (é˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´softmaxæ¢¯åº¦æ¶ˆå¤±)
       Scaled_Score = Score / âˆšd_k

æ­¥éª¤3: å½’ä¸€åŒ– (è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ)
       Attention_Weights = softmax(Scaled_Score)

æ­¥éª¤4: åŠ æƒæ±‚å’Œ
       Output = Attention_Weights Â· V
```

### 2.3 å›¾è§£æ³¨æ„åŠ›è®¡ç®—

```
Query (1Ã—d)     Key^T (dÃ—n)        Value (nÃ—d)
    â”‚               â”‚                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
            â”‚                          â”‚
            â–¼                          â”‚
      MatMul (1Ã—n)                     â”‚
            â”‚                          â”‚
            â–¼                          â”‚
      Scale (Ã·âˆšd)                      â”‚
            â”‚                          â”‚
            â–¼                          â”‚
       Softmax (1Ã—n)                   â”‚
            â”‚                          â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                 MatMul (1Ã—d)
                       â”‚
                       â–¼
                 Output (1Ã—d)
```

### 2.4 NumPy å®ç°åŸºç¡€æ³¨æ„åŠ›

```python
import numpy as np

def basic_attention(Q, K, V):
    """
    åŸºç¡€æ³¨æ„åŠ›æœºåˆ¶å®ç°

    å‚æ•°:
        Q: QueryçŸ©é˜µ, shape (seq_len_q, d_k)
        K: KeyçŸ©é˜µ, shape (seq_len_k, d_k)
        V: ValueçŸ©é˜µ, shape (seq_len_k, d_v)

    è¿”å›:
        output: æ³¨æ„åŠ›è¾“å‡º, shape (seq_len_q, d_v)
        attention_weights: æ³¨æ„åŠ›æƒé‡, shape (seq_len_q, seq_len_k)
    """
    d_k = K.shape[-1]

    # æ­¥éª¤1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = np.matmul(Q, K.T)  # (seq_len_q, seq_len_k)

    # æ­¥éª¤2: ç¼©æ”¾
    scaled_scores = scores / np.sqrt(d_k)

    # æ­¥éª¤3: Softmaxå½’ä¸€åŒ–
    attention_weights = np.exp(scaled_scores) / np.sum(
        np.exp(scaled_scores), axis=-1, keepdims=True
    )

    # æ­¥éª¤4: åŠ æƒæ±‚å’Œ
    output = np.matmul(attention_weights, V)  # (seq_len_q, d_v)

    return output, attention_weights


# ç¤ºä¾‹
np.random.seed(42)
Q = np.random.randn(1, 64)   # 1ä¸ªæŸ¥è¯¢
K = np.random.randn(10, 64)  # 10ä¸ªé”®
V = np.random.randn(10, 64)  # 10ä¸ªå€¼

output, weights = basic_attention(Q, K, V)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")      # (1, 64)
print(f"æ³¨æ„åŠ›æƒé‡: {weights.round(3)}")  # æ¯ä¸ªé”®çš„é‡è¦ç¨‹åº¦
```

---

## 3. è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰

### 3.1 ä»€ä¹ˆæ˜¯è‡ªæ³¨æ„åŠ›

**è‡ªæ³¨æ„åŠ›**æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹æ®Šå½¢å¼ï¼š**Queryã€Keyã€Value éƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—**ã€‚

```
æ™®é€šæ³¨æ„åŠ›:
    Queryæ¥è‡ªè§£ç å™¨ï¼ŒKey/Valueæ¥è‡ªç¼–ç å™¨

è‡ªæ³¨æ„åŠ›:
    Queryã€Keyã€Valueéƒ½æ¥è‡ªåŒä¸€ä¸ªè¾“å…¥åºåˆ—
    æ¯ä¸ªä½ç½®éƒ½å¯ä»¥å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
```

### 3.2 è‡ªæ³¨æ„åŠ›çš„æ„ä¹‰

ä»¥å¥å­"**The animal didn't cross the street because it was too tired**"ä¸ºä¾‹ï¼š

- ç†è§£"**it**"æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿ
- è‡ªæ³¨æ„åŠ›è®©"it"èƒ½å¤Ÿå…³æ³¨"animal"ï¼Œå»ºç«‹é•¿è·ç¦»ä¾èµ–

```
å…³æ³¨åº¦å¯è§†åŒ–:

The animal didn't cross the street because it was too tired
                                          â†‘
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
            [0.02] [0.45] [0.01] [0.02] [0.01] [0.03] [0.02] [1.0] [0.01] [0.02] [0.01]
                     â†‘
                   "animal" è·å¾—é«˜æ³¨æ„åŠ›æƒé‡
```

### 3.3 è‡ªæ³¨æ„åŠ›è®¡ç®—æµç¨‹

```python
def self_attention(X, W_Q, W_K, W_V):
    """
    è‡ªæ³¨æ„åŠ›æœºåˆ¶

    å‚æ•°:
        X: è¾“å…¥åºåˆ—, shape (seq_len, d_model)
        W_Q, W_K, W_V: æŠ•å½±çŸ©é˜µ, shape (d_model, d_k/d_v)
    """
    # çº¿æ€§æŠ•å½±ï¼šåŒä¸€ä¸ªè¾“å…¥äº§ç”ŸQã€Kã€V
    Q = np.matmul(X, W_Q)  # (seq_len, d_k)
    K = np.matmul(X, W_K)  # (seq_len, d_k)
    V = np.matmul(X, W_V)  # (seq_len, d_v)

    # è®¡ç®—è‡ªæ³¨æ„åŠ›
    d_k = K.shape[-1]
    scores = np.matmul(Q, K.T) / np.sqrt(d_k)
    attention_weights = softmax(scores)
    output = np.matmul(attention_weights, V)

    return output
```

**å›¾ç¤º**ï¼š

```
è¾“å…¥ X (seq_len Ã— d_model)
    â”‚
    â”œâ”€â”€â†’ W_Q â”€â”€â†’ Q
    â”‚
    â”œâ”€â”€â†’ W_K â”€â”€â†’ K
    â”‚
    â””â”€â”€â†’ W_V â”€â”€â†’ V
                 â”‚
                 â–¼
         Scaled Dot-Product Attention
                 â”‚
                 â–¼
         è¾“å‡º (seq_len Ã— d_v)
```

---

## 4. Transformer æ¶æ„å…¨è§£

### 4.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSFORMER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         ENCODER            â”‚            DECODER                  â”‚
â”‚         (ç¼–ç å™¨)            â”‚            (è§£ç å™¨)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                            â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Output (to     â”‚      â”‚      â”‚  Output          â”‚          â”‚
â”‚  â”‚   Decoder)       â”‚      â”‚      â”‚  Probabilities   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                â”‚               â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Add & Norm      â”‚      â”‚      â”‚     Linear       â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â”‚     + Softmax    â”‚          â”‚
â”‚           â”‚                â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚               â”‚                     â”‚
â”‚  â”‚   Feed Forward   â”‚      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â”‚   Add & Norm     â”‚          â”‚
â”‚           â”‚                â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚               â”‚                     â”‚
â”‚  â”‚  Add & Norm      â”‚      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â”‚   Feed Forward   â”‚          â”‚
â”‚           â”‚                â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚               â”‚                     â”‚
â”‚  â”‚  Multi-Head      â”‚      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Ã—N   â”‚
â”‚  â”‚  Self-Attention  â”‚      â”‚      â”‚   Add & Norm     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚           Ã—N   â”‚               â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Add & Norm      â”‚â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â†’â”‚  Cross-Attention â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                â”‚               â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Multi-Head      â”‚      â”‚      â”‚   Add & Norm     â”‚          â”‚
â”‚  â”‚  Self-Attention  â”‚      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚               â”‚                     â”‚
â”‚           â”‚                â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”‚  Masked Multi-   â”‚          â”‚
â”‚  â”‚  Positional      â”‚      â”‚      â”‚  Head Attention  â”‚          â”‚
â”‚  â”‚  Encoding        â”‚      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚               â”‚                     â”‚
â”‚           â”‚                â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”‚  Positional      â”‚          â”‚
â”‚  â”‚  Input           â”‚      â”‚      â”‚  Encoding        â”‚          â”‚
â”‚  â”‚  Embedding       â”‚      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚               â”‚                     â”‚
â”‚           â”‚                â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”         â”‚      â”‚  Output          â”‚          â”‚
â”‚    â”‚   Inputs    â”‚         â”‚      â”‚  Embedding       â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                            â”‚               â”‚                     â”‚
â”‚                            â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                            â”‚      â”‚  Outputs         â”‚          â”‚
â”‚                            â”‚      â”‚  (shifted right) â”‚          â”‚
â”‚                            â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ ¸å¿ƒè®¾è®¡æ€æƒ³

| è®¾è®¡           | ç›®çš„                                     |
| -------------- | ---------------------------------------- |
| **è‡ªæ³¨æ„åŠ›**   | æ•è·åºåˆ—å†…ä»»æ„ä½ç½®é—´çš„ä¾èµ–å…³ç³»           |
| **å¤šå¤´æ³¨æ„åŠ›** | ä»ä¸åŒè¡¨ç¤ºå­ç©ºé—´å­¦ä¹ å¤šç§å…³è”æ¨¡å¼         |
| **ä½ç½®ç¼–ç **   | æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼ˆæ³¨æ„åŠ›æœ¬èº«ä¸å…·å¤‡é¡ºåºæ„ŸçŸ¥ï¼‰ |
| **æ®‹å·®è¿æ¥**   | ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œä¾¿äºè®­ç»ƒæ·±å±‚ç½‘ç»œ           |
| **å±‚å½’ä¸€åŒ–**   | ç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•›                       |
| **å¹¶è¡Œè®¡ç®—**   | æ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—ï¼Œè®­ç»ƒæ•ˆç‡é«˜             |

### 4.3 ç¼–ç å™¨ vs è§£ç å™¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         åŒºåˆ«å¯¹æ¯”                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        ç¼–ç å™¨           â”‚              è§£ç å™¨                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åŒå‘è‡ªæ³¨æ„åŠ›            â”‚ æ©ç è‡ªæ³¨æ„åŠ›ï¼ˆåªçœ‹è¿‡å»ï¼‰                â”‚
â”‚ å¯ä»¥çœ‹åˆ°æ•´ä¸ªè¾“å…¥åºåˆ—     â”‚ ç”Ÿæˆæ—¶åªèƒ½çœ‹åˆ°å·²ç”Ÿæˆçš„token            â”‚
â”‚ å¹¶è¡Œå¤„ç†æ‰€æœ‰è¾“å…¥        â”‚ è®­ç»ƒå¹¶è¡Œï¼Œæ¨ç†è‡ªå›å½’                    â”‚
â”‚ è¾“å‡ºä¸Šä¸‹æ–‡è¡¨ç¤º          â”‚ è¾“å‡ºä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 5.1 å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ**

å•ä¸ªæ³¨æ„åŠ›å¤´åªèƒ½å­¦ä¹ ä¸€ç§å…³è”æ¨¡å¼ã€‚å¤šå¤´æ³¨æ„åŠ›å…è®¸æ¨¡å‹ï¼š

- åŒæ—¶å…³æ³¨ä¸åŒä½ç½®çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´
- æ•è·å¤šç§ç±»å‹çš„ä¾èµ–å…³ç³»

**æ•°å­¦å…¬å¼**ï¼š

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

å…¶ä¸­ï¼š
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

**å‚æ•°ç»´åº¦**ï¼š

- $W_i^Q, W_i^K \in \mathbb{R}^{d_{model} \times d_k}$
- $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$
- $W^O \in \mathbb{R}^{hd_v \times d_{model}}$
- é€šå¸¸ $d_k = d_v = d_{model} / h$

**å›¾ç¤º**ï¼š

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚           Multi-Head Attention           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                   â”‚                   â”‚
                â–¼                   â–¼                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Head 1  â”‚         â”‚ Head 2  â”‚   ...   â”‚ Head h  â”‚
           â”‚ Attn    â”‚         â”‚ Attn    â”‚         â”‚ Attn    â”‚
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                â”‚                   â”‚                   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Concat   â”‚
                              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Linear   â”‚
                              â”‚   (W^O)   â”‚
                              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                                 Output
```

### 5.2 ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

**é—®é¢˜**ï¼šè‡ªæ³¨æ„åŠ›æ˜¯**ç½®æ¢ä¸å˜**çš„â€”â€”æ‰“ä¹±è¾“å…¥é¡ºåºä¸å½±å“è¾“å‡ºã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šæ·»åŠ ä½ç½®ç¼–ç ï¼Œæ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚

**æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç **ï¼š

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

```python
import numpy as np
import matplotlib.pyplot as plt

def get_positional_encoding(max_seq_len, d_model):
    """ç”Ÿæˆä½ç½®ç¼–ç çŸ©é˜µ"""
    PE = np.zeros((max_seq_len, d_model))

    for pos in range(max_seq_len):
        for i in range(0, d_model, 2):
            denominator = np.power(10000, 2*i/d_model)
            PE[pos, i] = np.sin(pos / denominator)
            PE[pos, i+1] = np.cos(pos / denominator)

    return PE

# å¯è§†åŒ–
PE = get_positional_encoding(100, 512)
plt.figure(figsize=(15, 5))
plt.pcolormesh(PE, cmap='RdBu')
plt.xlabel('Dimension')
plt.ylabel('Position')
plt.colorbar()
plt.title('Positional Encoding Visualization')
plt.show()
```

**ä½ç½®ç¼–ç ç‰¹æ€§**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. æ¯ä¸ªä½ç½®æœ‰å”¯ä¸€çš„ç¼–ç å‘é‡                                     â”‚
â”‚  2. ä¸åŒä½ç½®é—´çš„ç›¸å¯¹è·ç¦»å¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢è¡¨ç¤º                      â”‚
â”‚  3. å¯ä»¥æ‰©å±•åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„åºåˆ—é•¿åº¦                             â”‚
â”‚  4. ä½ç»´åº¦ç¼–ç å˜åŒ–å¿«ï¼ˆæ•è·å±€éƒ¨ä½ç½®ï¼‰ï¼Œé«˜ç»´åº¦å˜åŒ–æ…¢ï¼ˆæ•è·å…¨å±€ä½ç½®ï¼‰ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰

æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨ç›¸åŒçš„å…¨è¿æ¥ç½‘ç»œï¼š

$$\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$$

```
è¾“å…¥ (seq_len, d_model=512)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Linear  â”‚  W1: (512, 2048)
    â”‚  + ReLU â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    (seq_len, d_ff=2048)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Linear  â”‚  W2: (2048, 512)
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
         â–¼
è¾“å‡º (seq_len, d_model=512)
```

**ä½œç”¨**ï¼šä¸ºæ¨¡å‹å¼•å…¥éçº¿æ€§ï¼Œå¢åŠ è¡¨è¾¾èƒ½åŠ›ã€‚

### 5.4 æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–

```
        è¾“å…¥ x
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                       â”‚
           â–¼                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚ Sublayer    â”‚                â”‚
    â”‚ (Attention  â”‚                â”‚
    â”‚  or FFN)    â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚
           â”‚                       â”‚
           â–¼                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚   Dropout   â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚
           â”‚                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Add (æ®‹å·®è¿æ¥)
                       â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Layer Norm  â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                     è¾“å‡º
```

**å±‚å½’ä¸€åŒ–å…¬å¼**ï¼š

$$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sigma + \epsilon} + \beta$$

å…¶ä¸­ $\mu$ã€$\sigma$ æ˜¯ç‰¹å¾ç»´åº¦ä¸Šçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚

### 5.5 æ©ç æœºåˆ¶ï¼ˆMaskingï¼‰

**Padding Mask**ï¼šå¤„ç†å˜é•¿åºåˆ—ï¼Œå¿½ç•¥å¡«å……ä½ç½®

```python
def create_padding_mask(seq, pad_token=0):
    """åˆ›å»ºå¡«å……æ©ç """
    return (seq == pad_token).astype(np.float32)

# ç¤ºä¾‹
seq = np.array([[1, 2, 3, 0, 0],   # å®é™…é•¿åº¦3
                [1, 2, 0, 0, 0]])   # å®é™…é•¿åº¦2
mask = create_padding_mask(seq)
# [[0, 0, 0, 1, 1],
#  [0, 0, 1, 1, 1]]
```

**Look-ahead Maskï¼ˆå› æœæ©ç ï¼‰**ï¼šè§£ç å™¨ä¸­é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯

```python
def create_look_ahead_mask(size):
    """åˆ›å»ºå‰ç»æ©ç ï¼ˆä¸Šä¸‰è§’çŸ©é˜µï¼‰"""
    mask = np.triu(np.ones((size, size)), k=1)
    return mask

# ç¤ºä¾‹: size=5
# [[0, 1, 1, 1, 1],
#  [0, 0, 1, 1, 1],
#  [0, 0, 0, 1, 1],
#  [0, 0, 0, 0, 1],
#  [0, 0, 0, 0, 0]]
```

**åº”ç”¨æ©ç **ï¼š

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    d_k = K.shape[-1]
    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)

    if mask is not None:
        # å°†maskä¸º1çš„ä½ç½®è®¾ä¸ºè´Ÿæ— ç©·ï¼Œsoftmaxåæ¥è¿‘0
        scores = scores + (mask * -1e9)

    attention_weights = softmax(scores, axis=-1)
    output = np.matmul(attention_weights, V)

    return output, attention_weights
```

---

## 6. å®Œæ•´ä»£ç å®ç°

### 6.1 PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # çº¿æ€§æŠ•å½±å±‚
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        output = torch.matmul(attention_weights, V)
        return output, attention_weights

    def split_heads(self, x, batch_size):
        """åˆ†å‰²ä¸ºå¤šå¤´: (batch, seq_len, d_model) -> (batch, heads, seq_len, d_k)"""
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # çº¿æ€§æŠ•å½±
        Q = self.W_Q(Q)
        K = self.W_K(K)
        V = self.W_V(V)

        # åˆ†å‰²å¤šå¤´
        Q = self.split_heads(Q, batch_size)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)

        # æ³¨æ„åŠ›è®¡ç®—
        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)

        # åˆå¹¶å¤šå¤´: (batch, heads, seq_len, d_k) -> (batch, seq_len, d_model)
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, -1, self.d_model)

        # æœ€ç»ˆçº¿æ€§æŠ•å½±
        output = self.W_O(attn_output)

        return output, attn_weights


class PositionwiseFeedForward(nn.Module):
    """é€ä½ç½®å‰é¦ˆç½‘ç»œ"""

    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.linear1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """

    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        PE = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )

        PE[:, 0::2] = torch.sin(position * div_term)
        PE[:, 1::2] = torch.cos(position * div_term)

        PE = PE.unsqueeze(0)  # (1, max_seq_len, d_model)
        self.register_buffer('PE', PE)

    def forward(self, x):
        x = x + self.PE[:, :x.size(1), :]
        return self.dropout(x)


class EncoderLayer(nn.Module):
    """Transformerç¼–ç å™¨å±‚"""

    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()

        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout1(attn_output))

        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout2(ff_output))

        return x


class DecoderLayer(nn.Module):
    """Transformerè§£ç å™¨å±‚"""

    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()

        self.masked_self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):
        # æ©ç è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.masked_self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout1(attn_output))

        # äº¤å‰æ³¨æ„åŠ›ï¼ˆQæ¥è‡ªè§£ç å™¨ï¼ŒK/Væ¥è‡ªç¼–ç å™¨ï¼‰
        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)
        x = self.norm2(x + self.dropout2(attn_output))

        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout3(ff_output))

        return x


class Transformer(nn.Module):
    """å®Œæ•´Transformeræ¨¡å‹"""

    def __init__(
        self,
        src_vocab_size,
        tgt_vocab_size,
        d_model=512,
        num_heads=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        d_ff=2048,
        max_seq_len=5000,
        dropout=0.1
    ):
        super().__init__()

        # åµŒå…¥å±‚
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)

        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)

        # ç¼–ç å™¨
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_encoder_layers)
        ])

        # è§£ç å™¨
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_decoder_layers)
        ])

        # è¾“å‡ºå±‚
        self.output_linear = nn.Linear(d_model, tgt_vocab_size)

        self.d_model = d_model
        self._init_parameters()

    def _init_parameters(self):
        """å‚æ•°åˆå§‹åŒ–"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def generate_square_subsequent_mask(self, size):
        """ç”Ÿæˆå› æœæ©ç """
        mask = torch.triu(torch.ones(size, size), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask

    def encode(self, src, src_mask=None):
        """ç¼–ç å™¨å‰å‘ä¼ æ’­"""
        x = self.src_embedding(src) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)

        for layer in self.encoder_layers:
            x = layer(x, src_mask)

        return x

    def decode(self, tgt, enc_output, src_mask=None, tgt_mask=None):
        """è§£ç å™¨å‰å‘ä¼ æ’­"""
        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)

        for layer in self.decoder_layers:
            x = layer(x, enc_output, src_mask, tgt_mask)

        return x

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """å®Œæ•´å‰å‘ä¼ æ’­"""
        enc_output = self.encode(src, src_mask)
        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)
        output = self.output_linear(dec_output)

        return output


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡å‹å‚æ•°
    src_vocab_size = 10000
    tgt_vocab_size = 10000

    # åˆ›å»ºæ¨¡å‹
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=512,
        num_heads=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        d_ff=2048,
        dropout=0.1
    )

    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    src_seq_len = 10
    tgt_seq_len = 12

    src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))
    tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))

    # ç”Ÿæˆæ©ç 
    tgt_mask = model.generate_square_subsequent_mask(tgt_seq_len)

    # å‰å‘ä¼ æ’­
    output = model(src, tgt, tgt_mask=tgt_mask)

    print(f"è¾“å…¥æºåºåˆ—å½¢çŠ¶: {src.shape}")      # (2, 10)
    print(f"è¾“å…¥ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}")    # (2, 12)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")         # (2, 12, 10000)

    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")   # çº¦65Må‚æ•°
```

### 6.2 å‚æ•°é‡è®¡ç®—

```python
def count_transformer_params(
    vocab_size=32000,
    d_model=512,
    num_heads=8,
    d_ff=2048,
    num_layers=6
):
    """è®¡ç®—Transformerå‚æ•°é‡"""

    # åµŒå…¥å±‚
    embedding_params = vocab_size * d_model

    # æ¯ä¸ªç¼–ç å™¨/è§£ç å™¨å±‚
    # å¤šå¤´æ³¨æ„åŠ›: 4ä¸ªçº¿æ€§å±‚ (Q, K, V, O)
    attention_params = 4 * (d_model * d_model + d_model)

    # å‰é¦ˆç½‘ç»œ: 2ä¸ªçº¿æ€§å±‚
    ffn_params = (d_model * d_ff + d_ff) + (d_ff * d_model + d_model)

    # å±‚å½’ä¸€åŒ–: 2ä¸ª (æ³¨æ„åŠ›åå’ŒFFNå)
    layer_norm_params = 4 * d_model  # 2 * (gamma + beta) * d_model

    # å•å±‚æ€»å‚æ•°
    layer_params = attention_params + ffn_params + layer_norm_params

    # ç¼–ç å™¨å’Œè§£ç å™¨
    encoder_params = num_layers * layer_params
    decoder_params = num_layers * (layer_params + attention_params + 2 * d_model)  # é¢å¤–çš„äº¤å‰æ³¨æ„åŠ›

    # è¾“å‡ºå±‚
    output_params = d_model * vocab_size + vocab_size

    total = 2 * embedding_params + encoder_params + decoder_params + output_params

    print(f"åµŒå…¥å±‚å‚æ•°: {2 * embedding_params:,}")
    print(f"ç¼–ç å™¨å‚æ•°: {encoder_params:,}")
    print(f"è§£ç å™¨å‚æ•°: {decoder_params:,}")
    print(f"è¾“å‡ºå±‚å‚æ•°: {output_params:,}")
    print(f"æ€»å‚æ•°é‡: {total:,}")

    return total

count_transformer_params()
```

---

## 7. Transformer å˜ä½“ä¸åº”ç”¨

### 7.1 ä¸»æµå˜ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Transformer å®¶æ—æ ‘                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  åŸå§‹Transformer (2017)                                         â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â”€â”€ ä»…ç¼–ç å™¨ (Encoder-only)                             â”‚
â”‚       â”‚        â”‚                                                â”‚
â”‚       â”‚        â”œâ”€â”€ BERT (2018) - åŒå‘è¯­è¨€ç†è§£                   â”‚
â”‚       â”‚        â”œâ”€â”€ RoBERTa (2019) - æ›´å¼ºçš„é¢„è®­ç»ƒ                â”‚
â”‚       â”‚        â””â”€â”€ ALBERT (2019) - å‚æ•°å…±äº«                     â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â”€â”€ ä»…è§£ç å™¨ (Decoder-only)                             â”‚
â”‚       â”‚        â”‚                                                â”‚
â”‚       â”‚        â”œâ”€â”€ GPTç³»åˆ— (2018-2023) - è‡ªå›å½’ç”Ÿæˆ             â”‚
â”‚       â”‚        â”œâ”€â”€ LLaMA (2023) - å¼€æºå¤§æ¨¡å‹                    â”‚
â”‚       â”‚        â””â”€â”€ Claude, ChatGPTç­‰                            â”‚
â”‚       â”‚                                                         â”‚
â”‚       â””â”€â”€â”€â”€ ç¼–ç å™¨-è§£ç å™¨ (Encoder-Decoder)                     â”‚
â”‚                â”‚                                                â”‚
â”‚                â”œâ”€â”€ T5 (2019) - æ–‡æœ¬åˆ°æ–‡æœ¬                       â”‚
â”‚                â”œâ”€â”€ BART (2019) - å»å™ªè‡ªç¼–ç å™¨                   â”‚
â”‚                â””â”€â”€ mT5, mBART - å¤šè¯­è¨€ç‰ˆæœ¬                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 å„å˜ä½“ç‰¹ç‚¹å¯¹æ¯”

| æ¨¡å‹ç±»å‹            | ä»£è¡¨ | æ³¨æ„åŠ›æ¨¡å¼   | å…¸å‹ä»»åŠ¡         |
| ------------------- | ---- | ------------ | ---------------- |
| **Encoder-only**    | BERT | åŒå‘ï¼ˆå…¨å±€ï¼‰ | åˆ†ç±»ã€NERã€é—®ç­”  |
| **Decoder-only**    | GPT  | å•å‘ï¼ˆå› æœï¼‰ | æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯   |
| **Encoder-Decoder** | T5   | æ··åˆ         | ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­” |

### 7.3 æ•ˆç‡æ”¹è¿›å˜ä½“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ³¨æ„åŠ›æ•ˆç‡ä¼˜åŒ–                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¨€ç–æ³¨æ„åŠ›   â”‚ Longformer, BigBird - å±€éƒ¨ + å…¨å±€æ³¨æ„åŠ›          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ çº¿æ€§æ³¨æ„åŠ›   â”‚ Linear Transformer, Performer - O(n) å¤æ‚åº¦      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ»‘åŠ¨çª—å£     â”‚ Sliding Window Attention - å›ºå®šçª—å£èŒƒå›´          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Flash Attn  â”‚ FlashAttention - IOæ„ŸçŸ¥çš„é«˜æ•ˆå®ç°                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.4 å¤šæ¨¡æ€æ‰©å±•

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Transformer â”‚
                    â”‚     Core     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   æ–‡æœ¬    â”‚     â”‚   è§†è§‰    â”‚     â”‚   éŸ³é¢‘    â”‚
  â”‚   NLP     â”‚     â”‚  Vision   â”‚     â”‚  Audio    â”‚
  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
    GPT, BERT         ViT, CLIP        Whisper
    LLaMA, T5         DALL-E           AudioLM
```

---

## 8. æ€»ç»“ä¸å±•æœ›

### 8.1 æ ¸å¿ƒè¦ç‚¹å›é¡¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æ ¸å¿ƒè¦ç‚¹                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. æ³¨æ„åŠ›æœºåˆ¶                                                  â”‚
â”‚     â€¢ Query-Key-Value ä¸‰è¦ç´                                     â”‚
â”‚     â€¢ ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›: Softmax(QK^T/âˆšd_k)V                       â”‚
â”‚     â€¢ è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜                                         â”‚
â”‚                                                                 â”‚
â”‚  2. è‡ªæ³¨æ„åŠ›                                                    â”‚
â”‚     â€¢ Q, K, V æ¥è‡ªåŒä¸€è¾“å…¥                                      â”‚
â”‚     â€¢ æ¯ä¸ªä½ç½®å…³æ³¨æ‰€æœ‰ä½ç½®                                       â”‚
â”‚     â€¢ O(nÂ²) æ—¶é—´å¤æ‚åº¦                                          â”‚
â”‚                                                                 â”‚
â”‚  3. å¤šå¤´æ³¨æ„åŠ›                                                  â”‚
â”‚     â€¢ å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´                                          â”‚
â”‚     â€¢ æ•è·ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»                                     â”‚
â”‚     â€¢ h å¤´, æ¯å¤´ç»´åº¦ d_k = d_model / h                          â”‚
â”‚                                                                 â”‚
â”‚  4. Transformer æ¶æ„                                            â”‚
â”‚     â€¢ ç¼–ç å™¨: è‡ªæ³¨æ„åŠ› + FFN                                    â”‚
â”‚     â€¢ è§£ç å™¨: æ©ç è‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ› + FFN                    â”‚
â”‚     â€¢ æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–                                        â”‚
â”‚     â€¢ ä½ç½®ç¼–ç æä¾›é¡ºåºä¿¡æ¯                                       â”‚
â”‚                                                                 â”‚
â”‚  5. å…³é”®ä¼˜åŠ¿                                                    â”‚
â”‚     â€¢ å¹¶è¡Œè®¡ç®—æ•ˆç‡é«˜                                            â”‚
â”‚     â€¢ å…¨å±€ä¾èµ–å»ºæ¨¡èƒ½åŠ›å¼º                                         â”‚
â”‚     â€¢ å¯æ‰©å±•æ€§å¥½ï¼ˆå‚æ•°é‡ä¸æ€§èƒ½æ­£ç›¸å…³ï¼‰                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 å­¦ä¹ è·¯å¾„å»ºè®®

```
Level 1: åŸºç¡€ç†è§£
â”œâ”€â”€ ç†è§£å‘é‡è¿ç®—å’ŒçŸ©é˜µä¹˜æ³•
â”œâ”€â”€ æŒæ¡Softmaxå’Œæ¢¯åº¦ä¸‹é™
â””â”€â”€ åŠ¨æ‰‹å®ç°åŸºç¡€æ³¨æ„åŠ›

Level 2: æ ¸å¿ƒæŒæ¡
â”œâ”€â”€ å®ç°å®Œæ•´Transformer
â”œâ”€â”€ ç†è§£å„ç»„ä»¶ä½œç”¨
â””â”€â”€ è®­ç»ƒå°å‹ç¿»è¯‘æ¨¡å‹

Level 3: è¿›é˜¶åº”ç”¨
â”œâ”€â”€ å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ (BERT, GPT)
â”œâ”€â”€ ç†è§£ä¸åŒå˜ä½“çš„è®¾è®¡åŠ¨æœº
â””â”€â”€ é˜…è¯»ç›¸å…³è®ºæ–‡

Level 4: ç ”ç©¶å‰æ²¿
â”œâ”€â”€ æ•ˆç‡ä¼˜åŒ– (FlashAttentionç­‰)
â”œâ”€â”€ é•¿åºåˆ—å»ºæ¨¡
â””â”€â”€ å¤šæ¨¡æ€Transformer
```

### 8.3 æ¨èèµ„æº

| ç±»å‹     | èµ„æº                                               |
| -------- | -------------------------------------------------- |
| **è®ºæ–‡** | "Attention Is All You Need" (Vaswani et al., 2017) |
| **æ•™ç¨‹** | The Illustrated Transformer (Jay Alammar)          |
| **ä»£ç ** | Harvard NLP - The Annotated Transformer            |
| **è¯¾ç¨‹** | Stanford CS224N, Hugging Face NLP Course           |
| **åº“**   | Hugging Face Transformers, PyTorch                 |

---

## é™„å½•ï¼šå¸¸ç”¨é…ç½®å‚æ•°

### æ ‡å‡† Transformer é…ç½®

| å‚æ•°       | Base æ¨¡å‹ | Large æ¨¡å‹ |
| ---------- | --------- | ---------- |
| d_model    | 512       | 1024       |
| d_ff       | 2048      | 4096       |
| num_heads  | 8         | 16         |
| num_layers | 6         | 6          |
| d_k = d_v  | 64        | 64         |

### ç°ä»£å¤§æ¨¡å‹é…ç½®å‚è€ƒ

| æ¨¡å‹      | å‚æ•°é‡ | å±‚æ•° | éšè—ç»´åº¦ | æ³¨æ„åŠ›å¤´ |
| --------- | ------ | ---- | -------- | -------- |
| GPT-2     | 1.5B   | 48   | 1600     | 25       |
| GPT-3     | 175B   | 96   | 12288    | 96       |
| LLaMA-7B  | 7B     | 32   | 4096     | 32       |
| LLaMA-70B | 70B    | 80   | 8192     | 64       |

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2024 å¹´

> "Attention is all you need." â€” ä½†ç†è§£å®ƒéœ€è¦çš„è¿œä¸æ­¢æ³¨æ„åŠ› ğŸš€
