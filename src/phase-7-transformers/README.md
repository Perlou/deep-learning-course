# Phase 7: æ³¨æ„åŠ›æœºåˆ¶ä¸ Transformer

> **ç›®æ ‡**ï¼šæ·±å…¥ç†è§£ç°ä»£æ·±åº¦å­¦ä¹ æ ¸å¿ƒæ¶æ„  
> **é¢„è®¡æ—¶é•¿**ï¼š2-3 å‘¨  
> **å‰ç½®æ¡ä»¶**ï¼šPhase 1-6 å®Œæˆ

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬é˜¶æ®µåï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. æ·±å…¥ç†è§£è‡ªæ³¨æ„åŠ› (Self-Attention) æœºåˆ¶
2. ä»é›¶å®ç°å®Œæ•´çš„ Transformer æ¶æ„
3. ç†è§£ BERT å’Œ GPT çš„åŒºåˆ«ä¸è”ç³»
4. èƒ½å¤Ÿä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ
5. å®Œæˆæœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬åˆ†ç±»é¡¹ç›®

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### è‡ªæ³¨æ„åŠ› (Self-Attention)

```python
# Scaled Dot-Product Attention
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

# Q: Query, K: Key, V: Value
# d_k: Key çš„ç»´åº¦
```

### å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)

å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•æ‰ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ï¼š

```python
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) @ W_O
```

### ä½ç½®ç¼–ç  (Positional Encoding)

ä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯ï¼š

```python
PE(pos, 2i) = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
```

### Transformer æ¶æ„

```
è¾“å…¥ â†’ Embedding + PE â†’ [Encoder Layers] â†’ ç¼–ç è¾“å‡º
                                    â†“
è§£ç è¾“å…¥ â†’ Embedding + PE â†’ [Decoder Layers] â†’ è¾“å‡º
```

---

## ğŸ“ æ–‡ä»¶åˆ—è¡¨

| æ–‡ä»¶                         | æè¿°             | çŠ¶æ€ |
| ---------------------------- | ---------------- | ---- |
| `01-self-attention.py`       | è‡ªæ³¨æ„åŠ›ä»é›¶å®ç° | â³   |
| `02-multi-head-attention.py` | å¤šå¤´æ³¨æ„åŠ›       | â³   |
| `03-positional-encoding.py`  | ä½ç½®ç¼–ç          | â³   |
| `04-transformer-encoder.py`  | ç¼–ç å™¨å®ç°       | â³   |
| `05-transformer-decoder.py`  | è§£ç å™¨å®ç°       | â³   |
| `06-transformer-full.py`     | å®Œæ•´ Transformer | â³   |
| `07-bert-architecture.py`    | BERT ç»“æ„ç†è§£    | â³   |
| `08-bert-finetuning.py`      | BERT å¾®è°ƒå®è·µ    | â³   |
| `09-gpt-architecture.py`     | GPT æ¶æ„         | â³   |
| `10-gpt-generation.py`       | æ–‡æœ¬ç”Ÿæˆ         | â³   |

---

## ğŸš€ è¿è¡Œæ–¹å¼

```bash
python src/phase-7-transformers/01-self-attention.py
python src/phase-7-transformers/06-transformer-full.py
```

---

## ğŸ“– æ¨èèµ„æº

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
- è®ºæ–‡ï¼šAttention Is All You Need, BERT, GPT

---

## âœ… å®Œæˆæ£€æŸ¥

- [ ] èƒ½å¤Ÿæ‰‹åŠ¨è®¡ç®—è‡ªæ³¨æ„åŠ›
- [ ] ç†è§£ Q, K, V çš„å«ä¹‰
- [ ] èƒ½å¤Ÿè§£é‡Šä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›
- [ ] ç†è§£ä½ç½®ç¼–ç çš„ä½œç”¨
- [ ] èƒ½å¤Ÿä»é›¶å®ç° Transformer
- [ ] ç†è§£ BERT å’Œ GPT çš„åŒºåˆ«
- [ ] èƒ½å¤Ÿå¾®è°ƒ BERT è¿›è¡Œåˆ†ç±»ä»»åŠ¡
- [ ] å®Œæˆæœºå™¨ç¿»è¯‘é¡¹ç›®
