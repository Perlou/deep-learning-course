"""
HuggingFace Transformers å…¥é—¨
============================

å­¦ä¹ ç›®æ ‡ï¼š
    1. äº†è§£ HuggingFace ç”Ÿæ€ç³»ç»Ÿ
    2. æŒæ¡ Tokenizer çš„ä½¿ç”¨
    3. å­¦ä¹ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    4. ä½¿ç”¨ Pipeline å¿«é€Ÿæ¨ç†

æ ¸å¿ƒæ¦‚å¿µï¼š
    - HuggingFace Hubï¼šæ¨¡å‹å’Œæ•°æ®é›†ä»“åº“
    - Tokenizerï¼šæ–‡æœ¬åˆ†è¯å’Œç¼–ç 
    - AutoModelï¼šè‡ªåŠ¨åŠ è½½æ¨¡å‹
    - Pipelineï¼šé«˜çº§æ¨ç†æ¥å£

å‰ç½®çŸ¥è¯†ï¼š
    - Phase 7: Transformer
    - å‰é¢çš„ NLP è¯¾ç¨‹
"""

import torch


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šHuggingFace ç”Ÿæ€ç³»ç»Ÿ ====================


def introduction():
    """HuggingFace ç”Ÿæ€ç³»ç»Ÿä»‹ç»"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šHuggingFace ç”Ÿæ€ç³»ç»Ÿ")
    print("=" * 60)

    print("""
HuggingFace ç”Ÿæ€ç³»ç»Ÿï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HuggingFace ç»„ä»¶                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤— Transformers    é¢„è®­ç»ƒæ¨¡å‹åº“                             â”‚
â”‚     - BERT, GPT, T5, LLaMA...                               â”‚
â”‚     - æ”¯æŒ PyTorch, TensorFlow, JAX                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤— Datasets        æ•°æ®é›†åº“                                 â”‚
â”‚     - 3000+ æ•°æ®é›†                                          â”‚
â”‚     - é«˜æ•ˆçš„æ•°æ®åŠ è½½                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤— Hub             æ¨¡å‹æ‰˜ç®¡å¹³å°                             â”‚
â”‚     - åˆ†äº«å’Œä¸‹è½½æ¨¡å‹                                         â”‚
â”‚     - æ¨¡å‹å¡ç‰‡å’Œæ–‡æ¡£                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤— Accelerate      åˆ†å¸ƒå¼è®­ç»ƒ                               â”‚
â”‚  ğŸ¤— PEFT            å‚æ•°é«˜æ•ˆå¾®è°ƒ                             â”‚
â”‚  ğŸ¤— Evaluate        è¯„ä¼°æŒ‡æ ‡                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®‰è£…ï¼š
    pip install transformers datasets
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šTokenizer ====================


def tokenizer_demo():
    """Tokenizer ä½¿ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šTokenizer")
    print("=" * 60)

    print("""
Tokenizer çš„ä½œç”¨ï¼š
    æ–‡æœ¬ â†’ tokens â†’ input_ids â†’ æ¨¡å‹

åŸºæœ¬ä½¿ç”¨ï¼š

    from transformers import AutoTokenizer
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
    
    # ç¼–ç 
    text = "æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£"
    encoded = tokenizer(text)
    # {'input_ids': [101, 3918, 2428, ...], 'attention_mask': [1, 1, ...]}
    
    # è§£ç 
    decoded = tokenizer.decode(encoded['input_ids'])
    # "[CLS] æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£ [SEP]"

å¸¸ç”¨å‚æ•°ï¼š

    encoded = tokenizer(
        text,
        max_length=128,           # æœ€å¤§é•¿åº¦
        padding='max_length',     # å¡«å……ç­–ç•¥
        truncation=True,          # æ˜¯å¦æˆªæ–­
        return_tensors='pt'       # è¿”å› PyTorch å¼ é‡
    )

æ‰¹é‡å¤„ç†ï¼š

    texts = ["ç¬¬ä¸€æ®µæ–‡æœ¬", "ç¬¬äºŒæ®µæ–‡æœ¬"]
    encoded = tokenizer(texts, padding=True, return_tensors='pt')

ç‰¹æ®Š tokensï¼š
    - [CLS]: å¥å­å¼€å§‹ï¼Œç”¨äºåˆ†ç±»
    - [SEP]: å¥å­åˆ†éš”/ç»“æŸ
    - [PAD]: å¡«å……
    - [MASK]: æ©ç ï¼ˆBERT MLMï¼‰
    """)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ====================


def model_loading():
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
    print("=" * 60)

    print("""
ä½¿ç”¨ AutoModel åŠ è½½æ¨¡å‹ï¼š

    from transformers import AutoModel, AutoTokenizer
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    model_name = "bert-base-chinese"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    # æ¨ç†
    text = "æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£"
    inputs = tokenizer(text, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    # outputs.last_hidden_state: (batch, seq_len, hidden_size)
    print(f"è¾“å‡ºå½¢çŠ¶: {outputs.last_hidden_state.shape}")

é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ï¼š

    # æ–‡æœ¬åˆ†ç±»
    from transformers import AutoModelForSequenceClassification
    model = AutoModelForSequenceClassification.from_pretrained(
        "bert-base-chinese", num_labels=2
    )
    
    # é—®ç­”
    from transformers import AutoModelForQuestionAnswering
    model = AutoModelForQuestionAnswering.from_pretrained("bert-base-chinese")
    
    # Token åˆ†ç±» (NER)
    from transformers import AutoModelForTokenClassification
    model = AutoModelForTokenClassification.from_pretrained(
        "bert-base-chinese", num_labels=9
    )

æ¨¡å‹é…ç½®ï¼š

    from transformers import AutoConfig
    
    config = AutoConfig.from_pretrained("bert-base-chinese")
    print(config.hidden_size)  # 768
    print(config.num_hidden_layers)  # 12
    """)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šPipeline ====================


def pipeline_demo():
    """Pipeline ä½¿ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šPipeline")
    print("=" * 60)

    print("""
Pipeline æ˜¯æœ€ç®€å•çš„æ¨ç†æ–¹å¼ï¼š

    from transformers import pipeline
    
    # æƒ…æ„Ÿåˆ†æ
    classifier = pipeline("sentiment-analysis")
    result = classifier("I love deep learning!")
    # [{'label': 'POSITIVE', 'score': 0.9998}]
    
    # æ–‡æœ¬ç”Ÿæˆ
    generator = pipeline("text-generation", model="gpt2")
    result = generator("Deep learning is", max_length=50)
    
    # é—®ç­”
    qa = pipeline("question-answering")
    result = qa(
        question="What is deep learning?",
        context="Deep learning is a branch of machine learning..."
    )
    
    # å‘½åå®ä½“è¯†åˆ«
    ner = pipeline("ner", aggregation_strategy="simple")
    result = ner("Bill Gates founded Microsoft in Seattle")
    
    # å¡«ç©º
    fill = pipeline("fill-mask", model="bert-base-chinese")
    result = fill("æ·±åº¦[MASK]æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯")

å¸¸ç”¨ Pipeline ä»»åŠ¡ï¼š

    - "text-classification / sentiment-analysis"
    - "token-classification / ner"
    - "question-answering"
    - "fill-mask"
    - "text-generation"
    - "summarization"
    - "translation"
    - "zero-shot-classification"
    """)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šå®æˆ˜ç¤ºä¾‹ ====================


def practical_example():
    """å®æˆ˜ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šå®æˆ˜ç¤ºä¾‹")
    print("=" * 60)

    print("""
å®Œæ•´çš„æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹ï¼š

    from transformers import (
        AutoTokenizer, 
        AutoModelForSequenceClassification,
        Trainer, 
        TrainingArguments
    )
    from datasets import load_dataset
    
    # 1. åŠ è½½æ•°æ®é›†
    dataset = load_dataset("imdb")
    
    # 2. åŠ è½½æ¨¡å‹å’Œ tokenizer
    model_name = "bert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, num_labels=2
    )
    
    # 3. æ•°æ®é¢„å¤„ç†
    def tokenize(examples):
        return tokenizer(
            examples["text"], 
            truncation=True, 
            max_length=512
        )
    
    tokenized = dataset.map(tokenize, batched=True)
    
    # 4. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=8,
        learning_rate=2e-5,
        evaluation_strategy="epoch",
    )
    
    # 5. åˆ›å»º Trainer å¹¶è®­ç»ƒ
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized["train"],
        eval_dataset=tokenized["test"],
    )
    
    trainer.train()
    
    # 6. ä¿å­˜æ¨¡å‹
    trainer.save_model("./my_model")
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
ç»ƒä¹  1ï¼šä½¿ç”¨ Pipeline
    ä»»åŠ¡ï¼šç”¨ä¸åŒçš„ Pipeline å¤„ç†åŒä¸€æ®µæ–‡æœ¬

ç»ƒä¹  1 ç­”æ¡ˆï¼š
    from transformers import pipeline
    
    text = "Apple CEO Tim Cook announced the new iPhone in California"
    
    # æƒ…æ„Ÿåˆ†æ
    sentiment = pipeline("sentiment-analysis")
    print(sentiment(text))
    
    # NER
    ner = pipeline("ner", aggregation_strategy="simple")
    print(ner(text))
    
    # é—®ç­”
    qa = pipeline("question-answering")
    print(qa(question="Who is the CEO?", context=text))

ç»ƒä¹  2ï¼šè‡ªå®šä¹‰ Tokenizer
    ä»»åŠ¡ï¼šæ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Tokenizer è¾“å‡ºå·®å¼‚

ç»ƒä¹  2 ç­”æ¡ˆï¼š
    from transformers import AutoTokenizer
    
    text = "æ·±åº¦å­¦ä¹ "
    
    for model_name in ["bert-base-chinese", "hfl/chinese-roberta-wwm-ext"]:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokens = tokenizer.tokenize(text)
        print(f"{model_name}: {tokens}")

æ€è€ƒé¢˜ 1ï¼šAutoModel vs ç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼Ÿ
ç­”æ¡ˆï¼š
    - AutoModelï¼šåªè¿”å›ç¼–ç å™¨è¾“å‡ºï¼Œéœ€è¦è‡ªå·±åŠ åˆ†ç±»å¤´
    - AutoModelForXXXï¼šåŒ…å«ä»»åŠ¡ç›¸å…³çš„å¤´ï¼Œå¯ç›´æ¥ç”¨äºç‰¹å®šä»»åŠ¡
    - é€‰æ‹©ä¾æ®ï¼šæ˜¯å¦éœ€è¦è‡ªå®šä¹‰è¾“å‡ºå±‚

æ€è€ƒé¢˜ 2ï¼šå¦‚ä½•é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿ
ç­”æ¡ˆï¼š
    è€ƒè™‘å› ç´ ï¼š
    - è¯­è¨€ï¼šä¸­æ–‡ç”¨ bert-base-chinese, è‹±æ–‡ç”¨ bert-base-uncased
    - ä»»åŠ¡ï¼šç”Ÿæˆç”¨ GPTï¼Œç†è§£ç”¨ BERT
    - è§„æ¨¡ï¼šèµ„æºæœ‰é™ç”¨ baseï¼Œè¿½æ±‚æ•ˆæœç”¨ large
    - é¢†åŸŸï¼šç‰¹å®šé¢†åŸŸå¯èƒ½æœ‰é¢†åŸŸä¸“ç”¨æ¨¡å‹
    """)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    introduction()
    tokenizer_demo()
    model_loading()
    pipeline_demo()
    practical_example()
    exercises()

    print("\n" + "=" * 60)
    print("è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š07-transformer-finetuning.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
