# ä» 0 å¼€å§‹æ·±å…¥è§£æ Triton ç®—å­

## ğŸ“š ç›®å½•

1. [Triton ç®€ä»‹](#1-tritonç®€ä»‹)
2. [ç¯å¢ƒé…ç½®](#2-ç¯å¢ƒé…ç½®)
3. [æ ¸å¿ƒæ¦‚å¿µ](#3-æ ¸å¿ƒæ¦‚å¿µ)
4. [åŸºç¡€ç®—å­å®æˆ˜](#4-åŸºç¡€ç®—å­å®æˆ˜)
5. [é«˜çº§ç‰¹æ€§](#5-é«˜çº§ç‰¹æ€§)
6. [ç¼–è¯‘åŸç†](#6-ç¼–è¯‘åŸç†)
7. [å®æˆ˜æ¡ˆä¾‹](#7-å®æˆ˜æ¡ˆä¾‹)

---

## 1. Triton ç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯ Tritonï¼Ÿ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GPUç¼–ç¨‹æŠ½è±¡å±‚æ¬¡                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Python/PyTorch    â†â”€â”€ æœ€é«˜å±‚æŠ½è±¡ï¼Œæ˜“ç”¨ä½†ä¸å¤Ÿçµæ´»            â”‚
â”‚         â†“                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚   Triton    â”‚   â†â”€â”€ ä¸­é—´å±‚ï¼šå¹³è¡¡æ˜“ç”¨æ€§å’Œæ€§èƒ½              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â†“                                                   â”‚
â”‚      CUDA C++      â†â”€â”€ åº•å±‚ï¼šçµæ´»ä½†å¤æ‚                       â”‚
â”‚         â†“                                                   â”‚
â”‚      PTX/SASS      â†â”€â”€ æœ€åº•å±‚ï¼šGPUæ±‡ç¼–æŒ‡ä»¤                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Triton vs CUDA å¯¹æ¯”

| ç‰¹æ€§     | CUDA                  | Triton        |
| -------- | --------------------- | ------------- |
| ç¼–ç¨‹è¯­è¨€ | C/C++                 | Python        |
| å†…å­˜ç®¡ç† | æ‰‹åŠ¨ç®¡ç†å…±äº«å†…å­˜      | è‡ªåŠ¨ç®¡ç†      |
| çº¿ç¨‹åŒæ­¥ | æ‰‹åŠ¨`__syncthreads()` | è‡ªåŠ¨å¤„ç†      |
| å­¦ä¹ æ›²çº¿ | é™¡å³­                  | å¹³ç¼“          |
| æ€§èƒ½     | æè‡´ä¼˜åŒ–å¯è¾¾ 100%     | é€šå¸¸è¾¾åˆ° 90%+ |
| å¼€å‘æ•ˆç‡ | ä½                    | é«˜            |

---

## 2. ç¯å¢ƒé…ç½®

```bash
# å®‰è£…Triton
pip install triton

# éªŒè¯å®‰è£…
python -c "import triton; print(triton.__version__)"
```

```python
# åŸºæœ¬å¯¼å…¥
import torch
import triton
import triton.language as tl
```

---

## 3. æ ¸å¿ƒæ¦‚å¿µ

### 3.1 Block-based ç¼–ç¨‹æ¨¡å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GPU Grid                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Program  â”‚ â”‚ Program  â”‚ â”‚ Program  â”‚ â”‚ Program  â”‚           â”‚
â”‚  â”‚   ID=0   â”‚ â”‚   ID=1   â”‚ â”‚   ID=2   â”‚ â”‚   ID=3   â”‚  ...      â”‚
â”‚  â”‚          â”‚ â”‚          â”‚ â”‚          â”‚ â”‚          â”‚           â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚           â”‚
â”‚  â”‚ â”‚Block â”‚ â”‚ â”‚ â”‚Block â”‚ â”‚ â”‚ â”‚Block â”‚ â”‚ â”‚ â”‚Block â”‚ â”‚           â”‚
â”‚  â”‚ â”‚ Size â”‚ â”‚ â”‚ â”‚ Size â”‚ â”‚ â”‚ â”‚ Size â”‚ â”‚ â”‚ â”‚ Size â”‚ â”‚           â”‚
â”‚  â”‚ â”‚ =128 â”‚ â”‚ â”‚ â”‚ =128 â”‚ â”‚ â”‚ â”‚ =128 â”‚ â”‚ â”‚ â”‚ =128 â”‚ â”‚           â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â”‚  æ¯ä¸ªProgramå®ä¾‹å¤„ç†ä¸€ä¸ªæ•°æ®å—(Block)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 å…³é”®æ¦‚å¿µå›¾è§£

```python
"""
æ ¸å¿ƒæ¦‚å¿µè§£é‡Šï¼š

1. program_id: å½“å‰ç¨‹åºå®ä¾‹çš„å”¯ä¸€æ ‡è¯†ç¬¦
2. BLOCK_SIZE: æ¯ä¸ªç¨‹åºå®ä¾‹å¤„ç†çš„å…ƒç´ æ•°é‡
3. offsets: å½“å‰blockè¦å¤„ç†çš„æ•°æ®åç§»é‡
4. mask: è¾¹ç•Œæ£€æŸ¥ï¼Œé˜²æ­¢è¶Šç•Œè®¿é—®
"""

# æ¦‚å¿µç¤ºæ„
@triton.jit
def concept_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    # 1. è·å–å½“å‰ç¨‹åºID
    pid = tl.program_id(axis=0)  # ç¬¬0ç»´çš„ç¨‹åºID

    # 2. è®¡ç®—å½“å‰blockçš„èµ·å§‹ä½ç½®
    block_start = pid * BLOCK_SIZE

    # 3. è®¡ç®—å½“å‰blockå†…çš„åç§»é‡
    offsets = block_start + tl.arange(0, BLOCK_SIZE)

    # 4. åˆ›å»ºmaské˜²æ­¢è¶Šç•Œ
    mask = offsets < n_elements

    # 5. åŠ è½½æ•°æ®
    x = tl.load(x_ptr + offsets, mask=mask)

    # 6. è®¡ç®—
    output = x * 2

    # 7. å­˜å‚¨ç»“æœ
    tl.store(output_ptr + offsets, output, mask=mask)
```

### 3.3 å†…å­˜è®¿é—®æ¨¡å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å†…å­˜è®¿é—®å¯¹æ¯”                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  âœ“ Coalesced Access (åˆå¹¶è®¿é—®) - é«˜æ•ˆ                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  0  â”‚  1  â”‚  2  â”‚  3  â”‚  4  â”‚  5  â”‚  6  â”‚  7  â”‚ Memory  â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”¬â”€â”€â”˜         â”‚
â”‚     â†“     â†“     â†“     â†“     â†“     â†“     â†“     â†“            â”‚
â”‚  Thread0  T1    T2    T3    T4    T5    T6    T7           â”‚
â”‚                                                             â”‚
â”‚  âœ— Strided Access (è·¨æ­¥è®¿é—®) - ä½æ•ˆ                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  0  â”‚     â”‚  2  â”‚     â”‚  4  â”‚     â”‚  6  â”‚     â”‚ Memory  â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”˜         â”‚
â”‚     â†“           â†“           â†“           â†“                  â”‚
â”‚  Thread0      T1          T2          T3                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. åŸºç¡€ç®—å­å®æˆ˜

### 4.1 Vector Add (å‘é‡åŠ æ³•)

```python
import torch
import triton
import triton.language as tl

@triton.jit
def add_kernel(
    x_ptr,          # è¾“å…¥æŒ‡é’ˆx
    y_ptr,          # è¾“å…¥æŒ‡é’ˆy
    output_ptr,     # è¾“å‡ºæŒ‡é’ˆ
    n_elements,     # æ€»å…ƒç´ æ•°
    BLOCK_SIZE: tl.constexpr,  # ç¼–è¯‘æ—¶å¸¸é‡
):
    """
    è®¡ç®—: output = x + y

    å†…å­˜å¸ƒå±€:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ x:      [x0, x1, x2, x3, x4, x5, x6, x7, ...]   â”‚
    â”‚ y:      [y0, y1, y2, y3, y4, y5, y6, y7, ...]   â”‚
    â”‚ output: [z0, z1, z2, z3, z4, z5, z6, z7, ...]   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    æ¯ä¸ªprogramå¤„ç†BLOCK_SIZEä¸ªå…ƒç´ :
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Program 0   â”‚ â”‚  Program 1   â”‚ â”‚  Program 2   â”‚
    â”‚ [0:BLOCK_SIZE]â”‚ â”‚[BS:2*BS]    â”‚ â”‚[2*BS:3*BS]  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    # è·å–å½“å‰programçš„ID
    pid = tl.program_id(axis=0)

    # è®¡ç®—å½“å‰blockå¤„ç†çš„æ•°æ®èŒƒå›´
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)

    # åˆ›å»ºmaskï¼Œå¤„ç†è¾¹ç•Œæƒ…å†µ
    mask = offsets < n_elements

    # ä»å…¨å±€å†…å­˜åŠ è½½æ•°æ®
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)

    # æ‰§è¡ŒåŠ æ³•
    output = x + y

    # å°†ç»“æœå†™å›å…¨å±€å†…å­˜
    tl.store(output_ptr + offsets, output, mask=mask)


def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """å‘é‡åŠ æ³•çš„PythonåŒ…è£…å‡½æ•°"""
    # æ£€æŸ¥è¾“å…¥
    assert x.is_cuda and y.is_cuda
    assert x.shape == y.shape

    # åˆ†é…è¾“å‡º
    output = torch.empty_like(x)
    n_elements = output.numel()

    # è®¡ç®—gridå¤§å°
    BLOCK_SIZE = 1024
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)

    # å¯åŠ¨kernel
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)

    return output


# æµ‹è¯•
if __name__ == "__main__":
    torch.manual_seed(0)
    size = 98432
    x = torch.rand(size, device='cuda')
    y = torch.rand(size, device='cuda')

    output_triton = add(x, y)
    output_torch = x + y

    print(f"æœ€å¤§è¯¯å·®: {torch.max(torch.abs(output_triton - output_torch))}")
    print("âœ“ æµ‹è¯•é€šè¿‡!" if torch.allclose(output_triton, output_torch) else "âœ— æµ‹è¯•å¤±è´¥!")
```

### 4.2 Softmax

```python
@triton.jit
def softmax_kernel(
    input_ptr,
    output_ptr,
    input_row_stride,   # è¡Œæ­¥é•¿
    output_row_stride,
    n_cols,             # åˆ—æ•°
    BLOCK_SIZE: tl.constexpr,
):
    """
    è®¡ç®—è¡Œæ–¹å‘çš„Softmax

    Softmax(x_i) = exp(x_i - max(x)) / sum(exp(x - max(x)))

    æ•°æ®å¸ƒå±€ (æ¯ä¸ªprogramå¤„ç†ä¸€è¡Œ):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Row 0: [x00, x01, x02, ..., x0n] â†’ Program 0
    â”‚ Row 1: [x10, x11, x12, ..., x1n] â†’ Program 1
    â”‚ Row 2: [x20, x21, x22, ..., x2n] â†’ Program 2
    â”‚ ...
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    # æ¯ä¸ªprogramå¤„ç†ä¸€è¡Œ
    row_idx = tl.program_id(0)

    # è®¡ç®—å½“å‰è¡Œçš„èµ·å§‹æŒ‡é’ˆ
    row_start_ptr = input_ptr + row_idx * input_row_stride

    # åˆ—åç§»é‡
    col_offsets = tl.arange(0, BLOCK_SIZE)
    input_ptrs = row_start_ptr + col_offsets

    # è¾¹ç•Œmask
    mask = col_offsets < n_cols

    # åŠ è½½ä¸€è¡Œæ•°æ® (è¶Šç•Œä½ç½®ç”¨-infå¡«å……)
    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))

    # æ•°å€¼ç¨³å®šçš„softmaxè®¡ç®—
    # Step 1: å‡å»æœ€å¤§å€¼
    row_max = tl.max(row, axis=0)
    row_minus_max = row - row_max

    # Step 2: è®¡ç®—exp
    numerator = tl.exp(row_minus_max)

    # Step 3: è®¡ç®—sum
    denominator = tl.sum(numerator, axis=0)

    # Step 4: å½’ä¸€åŒ–
    softmax_output = numerator / denominator

    # å­˜å‚¨ç»“æœ
    output_row_start_ptr = output_ptr + row_idx * output_row_stride
    output_ptrs = output_row_start_ptr + col_offsets
    tl.store(output_ptrs, softmax_output, mask=mask)


def softmax(x: torch.Tensor) -> torch.Tensor:
    """Softmaxçš„PythonåŒ…è£…"""
    n_rows, n_cols = x.shape

    # æ‰¾åˆ°æœ€è¿‘çš„2çš„å¹‚æ¬¡ä½œä¸ºBLOCK_SIZE
    BLOCK_SIZE = triton.next_power_of_2(n_cols)

    # åˆ›å»ºè¾“å‡ºtensor
    y = torch.empty_like(x)

    # å¯åŠ¨kernelï¼Œæ¯è¡Œä¸€ä¸ªprogram
    softmax_kernel[(n_rows,)](
        x, y,
        x.stride(0), y.stride(0),
        n_cols,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    return y
```

### 4.3 Matrix Multiplication (çŸ©é˜µä¹˜æ³•)

```python
@triton.jit
def matmul_kernel(
    # çŸ©é˜µæŒ‡é’ˆ
    a_ptr, b_ptr, c_ptr,
    # çŸ©é˜µç»´åº¦
    M, N, K,
    # çŸ©é˜µæ­¥é•¿
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    # å—å¤§å°
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    """
    è®¡ç®— C = A @ B

    A: (M, K)  B: (K, N)  C: (M, N)

    åˆ†å—è®¡ç®—ç¤ºæ„å›¾:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         A           â”‚     â”‚         B           â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”          â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”  â”‚
    â”‚    â”‚Blockâ”‚â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â”‚ Ã— â†’â†’â”‚â†’â†’â”‚     â”‚     â”‚   â”‚  â”‚
    â”‚    â”‚ A   â”‚  Kæ–¹å‘    â”‚     â”‚  â”‚Blockâ”‚     â”‚   â”‚  â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”˜          â”‚     â”‚  â”‚ B   â”‚     â”‚   â”‚  â”‚
    â”‚                     â”‚     â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â†“
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚         C           â”‚
                                â”‚    â”Œâ”€â”€â”€â”€â”€â”          â”‚
                                â”‚    â”‚Blockâ”‚ = ç´¯åŠ ç»“æœâ”‚
                                â”‚    â”‚ C   â”‚          â”‚
                                â”‚    â””â”€â”€â”€â”€â”€â”˜          â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    # 2D program id
    pid_m = tl.program_id(axis=0)  # è¡Œæ–¹å‘
    pid_n = tl.program_id(axis=1)  # åˆ—æ–¹å‘

    # è®¡ç®—å½“å‰å—çš„åç§»é‡
    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # åˆå§‹åŒ–Aå’ŒBçš„æŒ‡é’ˆ
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

    # ç´¯åŠ å™¨åˆå§‹åŒ–ä¸º0
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # æ²¿Kç»´åº¦å¾ªç¯
    for k in range(0, K, BLOCK_SIZE_K):
        # è¾¹ç•Œæ£€æŸ¥
        a_mask = (offs_am[:, None] < M) & ((k + offs_k[None, :]) < K)
        b_mask = ((k + offs_k[:, None]) < K) & (offs_bn[None, :] < N)

        # åŠ è½½Aå’ŒBçš„å—
        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs, mask=b_mask, other=0.0)

        # å—çŸ©é˜µä¹˜æ³•ç´¯åŠ 
        accumulator += tl.dot(a, b)

        # ç§»åŠ¨æŒ‡é’ˆåˆ°ä¸‹ä¸€ä¸ªKå—
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk

    # è¾“å‡ºç»“æœ
    c = accumulator.to(tl.float16)

    # è®¡ç®—è¾“å‡ºä½ç½®
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)

    tl.store(c_ptrs, c, mask=c_mask)
```

---

## 5. é«˜çº§ç‰¹æ€§

### 5.1 è‡ªåŠ¨è°ƒä¼˜ (Auto-tuning)

```python
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),
    ],
    key=['M', 'N', 'K'],  # æ ¹æ®è¿™äº›å‚æ•°é€‰æ‹©æœ€ä¼˜é…ç½®
)
@triton.jit
def matmul_kernel_autotune(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    """
    Auto-tuningé…ç½®å‚æ•°è¯´æ˜:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å‚æ•°          â”‚  è¯´æ˜                                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  BLOCK_SIZE_*  â”‚  å—å¤§å°ï¼Œå½±å“å¹¶è¡Œåº¦å’Œå¯„å­˜å™¨ä½¿ç”¨              â”‚
    â”‚  num_stages    â”‚  è½¯ä»¶æµæ°´çº¿çº§æ•°ï¼Œéšè—å†…å­˜å»¶è¿Ÿ                â”‚
    â”‚  num_warps     â”‚  warpæ•°é‡ï¼Œå½±å“SMå ç”¨ç‡                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    # ... (kernelå®ç°åŒä¸Š)
    pass
```

### 5.2 Triton å¸¸ç”¨ API

```python
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Triton Language API                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  ã€ç¨‹åºæ ‡è¯†ã€‘                                                             â”‚
â”‚  tl.program_id(axis)     - è·å–å½“å‰programåœ¨æŒ‡å®šç»´åº¦çš„ID                  â”‚
â”‚  tl.num_programs(axis)   - è·å–æŒ‡å®šç»´åº¦çš„programæ€»æ•°                      â”‚
â”‚                                                                          â”‚
â”‚  ã€å†…å­˜æ“ä½œã€‘                                                             â”‚
â”‚  tl.load(ptr, mask)      - ä»å…¨å±€å†…å­˜åŠ è½½æ•°æ®                             â”‚
â”‚  tl.store(ptr, val, mask) - å‘å…¨å±€å†…å­˜å­˜å‚¨æ•°æ®                            â”‚
â”‚  tl.atomic_add(ptr, val)  - åŸå­åŠ æ“ä½œ                                    â”‚
â”‚                                                                          â”‚
â”‚  ã€æ•°å­¦è¿ç®—ã€‘                                                             â”‚
â”‚  tl.exp(x)               - æŒ‡æ•°è¿ç®—                                      â”‚
â”‚  tl.log(x)               - å¯¹æ•°è¿ç®—                                      â”‚
â”‚  tl.sqrt(x)              - å¹³æ–¹æ ¹                                        â”‚
â”‚  tl.abs(x)               - ç»å¯¹å€¼                                        â”‚
â”‚  tl.dot(a, b)            - çŸ©é˜µä¹˜æ³•                                      â”‚
â”‚                                                                          â”‚
â”‚  ã€è§„çº¦æ“ä½œã€‘                                                             â”‚
â”‚  tl.sum(x, axis)         - æ±‚å’Œ                                          â”‚
â”‚  tl.max(x, axis)         - æ±‚æœ€å¤§å€¼                                      â”‚
â”‚  tl.min(x, axis)         - æ±‚æœ€å°å€¼                                      â”‚
â”‚                                                                          â”‚
â”‚  ã€æ•°æ®æ“ä½œã€‘                                                             â”‚
â”‚  tl.arange(start, end)   - åˆ›å»ºè¿ç»­æ•´æ•°åºåˆ—                               â”‚
â”‚  tl.zeros(shape, dtype)  - åˆ›å»ºé›¶å¼ é‡                                    â”‚
â”‚  tl.broadcast_to(x, shape) - å¹¿æ’­                                        â”‚
â”‚  tl.where(cond, x, y)    - æ¡ä»¶é€‰æ‹©                                      â”‚
â”‚                                                                          â”‚
â”‚  ã€åŒæ­¥æ“ä½œã€‘                                                             â”‚
â”‚  tl.debug_barrier()      - åŒæ­¥å±éšœ                                      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
```

---

## 6. ç¼–è¯‘åŸç†

### 6.1 Triton ç¼–è¯‘æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Triton ç¼–è¯‘æµç¨‹                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Pythonä»£ç                                                               â”‚
â”‚   @triton.jit                                                            â”‚
â”‚   def kernel(...)                                                        â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â–¼                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚  Python AST  â”‚  â† è§£æPythonä»£ç ä¸ºæŠ½è±¡è¯­æ³•æ ‘                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚          â”‚                                                               â”‚
â”‚          â–¼                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚  Triton IR   â”‚  â† è½¬æ¢ä¸ºTritonä¸­é—´è¡¨ç¤º                                â”‚
â”‚   â”‚   (TTIR)     â”‚                                                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚          â”‚                                                               â”‚
â”‚          â–¼                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚  Triton GPU  â”‚  â† GPUç‰¹å®šä¼˜åŒ–                                        â”‚
â”‚   â”‚   (TTGIR)    â”‚    - æ•°æ®å¸ƒå±€ä¼˜åŒ–                                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    - å…±äº«å†…å­˜åˆ†é…                                      â”‚
â”‚          â”‚                                                               â”‚
â”‚          â–¼                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚   LLVM IR    â”‚  â† è½¬æ¢ä¸ºLLVMä¸­é—´è¡¨ç¤º                                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚          â”‚                                                               â”‚
â”‚          â–¼                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚     PTX      â”‚  â† ç”ŸæˆPTXæ±‡ç¼–                                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚          â”‚                                                               â”‚
â”‚          â–¼                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚    CUBIN     â”‚  â† ç¼–è¯‘ä¸ºGPUäºŒè¿›åˆ¶                                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 

```python
# æŸ¥çœ‹PTXä»£ç 
@triton.jit
def simple_kernel(x_ptr, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    x = tl.load(x_ptr + offs)
    tl.store(x_ptr + offs, x * 2)

# ç¼–è¯‘å¹¶æŸ¥çœ‹
x = torch.randn(1024, device='cuda')
simple_kernel[(1,)](x, BLOCK_SIZE=1024)

# è·å–ç¼–è¯‘åçš„kernelä¿¡æ¯
print(simple_kernel.cache)  # æŸ¥çœ‹ç¼“å­˜çš„ç¼–è¯‘ç»“æœ
```

---

## 7. å®æˆ˜æ¡ˆä¾‹

### 7.1 Fused Attention (ç®€åŒ–ç‰ˆ Flash Attention)

```python
@triton.jit
def fused_attention_kernel(
    Q, K, V, Out,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vn, stride_vk,
    stride_oz, stride_oh, stride_om, stride_ok,
    Z, H, N_CTX,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Fused Attention: O = softmax(Q @ K.T / sqrt(d)) @ V

    ä¼˜åŠ¿:
    1. å‡å°‘å…¨å±€å†…å­˜è®¿é—®
    2. é¿å…å­˜å‚¨ä¸­é—´ç»“æœS = Q @ K.T
    3. åœ¨çº¿Softmaxè®¡ç®—

    æ•°æ®æµ:
    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”
    â”‚  Q  â”‚ @  â”‚ K.T â”‚  â†’ S â†’  â”‚ Softmax â”‚ @  â”‚  V  â”‚ â†’  â”‚  O  â”‚
    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜
         \____________________________/
                èåˆåœ¨ä¸€èµ·è®¡ç®—
    """
    start_m = tl.program_id(0)
    off_hz = tl.program_id(1)

    # åˆå§‹åŒ–æŒ‡é’ˆ
    q_offset = off_hz * stride_qh
    k_offset = off_hz * stride_kh
    v_offset = off_hz * stride_vh
    o_offset = off_hz * stride_oh

    # åˆ›å»ºåç§»é‡
    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # åŠ è½½Qå—
    q_ptrs = Q + q_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk
    q = tl.load(q_ptrs, mask=offs_m[:, None] < N_CTX, other=0.0)

    # åˆå§‹åŒ–ç´¯åŠ å™¨
    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")
    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)
    acc = tl.zeros([BLOCK_M, BLOCK_K], dtype=tl.float32)

    # æ²¿Nç»´åº¦è¿­ä»£
    for start_n in range(0, N_CTX, BLOCK_N):
        # åŠ è½½K, V
        k_ptrs = K + k_offset + (start_n + offs_n)[:, None] * stride_kn + offs_k[None, :] * stride_kk
        v_ptrs = V + v_offset + (start_n + offs_n)[:, None] * stride_vn + offs_k[None, :] * stride_vk

        k = tl.load(k_ptrs, mask=(start_n + offs_n)[:, None] < N_CTX, other=0.0)
        v = tl.load(v_ptrs, mask=(start_n + offs_n)[:, None] < N_CTX, other=0.0)

        # è®¡ç®—QK^T
        s = tl.dot(q, tl.trans(k))

        # åœ¨çº¿Softmaxè®¡ç®—
        m_ij = tl.max(s, axis=1)
        m_i_new = tl.maximum(m_i, m_ij)
        alpha = tl.exp(m_i - m_i_new)
        p = tl.exp(s - m_i_new[:, None])

        # æ›´æ–°ç´¯åŠ å™¨
        l_i = l_i * alpha + tl.sum(p, axis=1)
        acc = acc * alpha[:, None] + tl.dot(p.to(v.dtype), v)
        m_i = m_i_new

    # å½’ä¸€åŒ–
    acc = acc / l_i[:, None]

    # å­˜å‚¨è¾“å‡º
    o_ptrs = Out + o_offset + offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok
    tl.store(o_ptrs, acc.to(Out.dtype.element_ty), mask=offs_m[:, None] < N_CTX)
```

### 7.2 LayerNorm

```python
@triton.jit
def layernorm_kernel(
    X, Y, W, B,      # è¾“å…¥ã€è¾“å‡ºã€æƒé‡ã€åç½®
    Mean, Rstd,      # å‡å€¼å’Œé€†æ ‡å‡†å·® (å¯é€‰è¾“å‡º)
    stride,          # è¡Œæ­¥é•¿
    N,               # æ¯è¡Œå…ƒç´ æ•°
    eps,             # epsilon
    BLOCK_SIZE: tl.constexpr,
):
    """
    LayerNorm: y = (x - mean) / sqrt(var + eps) * weight + bias

    è®¡ç®—è¿‡ç¨‹:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. è®¡ç®—å‡å€¼: mean = sum(x) / N                          â”‚
    â”‚  2. è®¡ç®—æ–¹å·®: var = sum((x - mean)^2) / N                â”‚
    â”‚  3. å½’ä¸€åŒ–: x_norm = (x - mean) / sqrt(var + eps)       â”‚
    â”‚  4. ç¼©æ”¾å¹³ç§»: y = x_norm * weight + bias                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    row = tl.program_id(0)

    # è®¡ç®—å½“å‰è¡Œçš„èµ·å§‹æŒ‡é’ˆ
    X += row * stride
    Y += row * stride

    # åˆ—åç§»
    cols = tl.arange(0, BLOCK_SIZE)
    mask = cols < N

    # åŠ è½½æ•°æ®
    x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)

    # è®¡ç®—å‡å€¼
    mean = tl.sum(x, axis=0) / N

    # è®¡ç®—æ–¹å·®
    xmean = x - mean
    var = tl.sum(xmean * xmean, axis=0) / N

    # è®¡ç®—é€†æ ‡å‡†å·®
    rstd = 1 / tl.sqrt(var + eps)

    # å½’ä¸€åŒ–
    x_hat = xmean * rstd

    # åŠ è½½æƒé‡å’Œåç½®
    w = tl.load(W + cols, mask=mask, other=1.0)
    b = tl.load(B + cols, mask=mask, other=0.0)

    # ç¼©æ”¾å’Œå¹³ç§»
    y = x_hat * w + b

    # å­˜å‚¨ç»“æœ
    tl.store(Y + cols, y, mask=mask)

    # å¯é€‰ï¼šå­˜å‚¨å‡å€¼å’Œé€†æ ‡å‡†å·®
    tl.store(Mean + row, mean)
    tl.store(Rstd + row, rstd)
```

---

## 8. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æ€§èƒ½ä¼˜åŒ–æŠ€å·§                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  1. åˆå¹¶å†…å­˜è®¿é—® (Coalesced Access)                                       â”‚
â”‚     âœ“ è¿ç»­è®¿é—®: offs = block_start + tl.arange(0, BLOCK_SIZE)            â”‚
â”‚     âœ— è·¨æ­¥è®¿é—®: offs = tl.arange(0, BLOCK_SIZE) * stride                 â”‚
â”‚                                                                          â”‚
â”‚  2. é€‰æ‹©åˆé€‚çš„BLOCK_SIZE                                                  â”‚
â”‚     - å¤ªå°: ä¸èƒ½å……åˆ†åˆ©ç”¨å†…å­˜å¸¦å®½                                           â”‚
â”‚     - å¤ªå¤§: å¯„å­˜å™¨æº¢å‡ºï¼Œé™ä½occupancy                                      â”‚
â”‚     - æ¨è: 64, 128, 256, 512, 1024                                      â”‚
â”‚                                                                          â”‚
â”‚  3. ä½¿ç”¨tl.constexpr                                                     â”‚
â”‚     - å…è®¸ç¼–è¯‘æ—¶ä¼˜åŒ–                                                       â”‚
â”‚     - å¾ªç¯å±•å¼€                                                            â”‚
â”‚     - å†…å­˜å¸ƒå±€ä¼˜åŒ–                                                        â”‚
â”‚                                                                          â”‚
â”‚  4. å‡å°‘å…¨å±€å†…å­˜è®¿é—®                                                       â”‚
â”‚     - ç®—å­èåˆ                                                            â”‚
â”‚     - åˆ©ç”¨å¯„å­˜å™¨å’Œå…±äº«å†…å­˜                                                 â”‚
â”‚                                                                          â”‚
â”‚  5. ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹                                                     â”‚
â”‚     - è®¡ç®—ç”¨float32                                                       â”‚
â”‚     - å­˜å‚¨ç”¨float16/bfloat16                                             â”‚
â”‚                                                                          â”‚
â”‚  6. åˆ©ç”¨Auto-tuning                                                      â”‚
â”‚     - å°è¯•ä¸åŒçš„é…ç½®ç»„åˆ                                                   â”‚
â”‚     - è®©Tritonè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°                                             â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. è°ƒè¯•æŠ€å·§

```python
# 1. ä½¿ç”¨printè°ƒè¯• (ä»…åœ¨ç¬¬ä¸€ä¸ªprogram)
@triton.jit
def debug_kernel(x_ptr, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    if pid == 0:
        tl.device_print("Program ID:", pid)

    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    x = tl.load(x_ptr + offs)

    if pid == 0:
        tl.device_print("First element:", tl.load(x_ptr))

# 2. ä½¿ç”¨triton.testingè¿›è¡ŒåŸºå‡†æµ‹è¯•
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['size'],                   # xè½´å‚æ•°å
        x_vals=[2**i for i in range(12, 28, 1)],  # xè½´å€¼
        line_arg='provider',                # ä¸åŒçš„çº¿
        line_vals=['triton', 'torch'],     # çº¿çš„å€¼
        line_names=['Triton', 'Torch'],    # çº¿çš„åç§°
        styles=[('blue', '-'), ('green', '-')],
        ylabel='GB/s',                     # yè½´æ ‡ç­¾
        plot_name='vector-add-performance', # å›¾è¡¨åç§°
        args={},
    )
)
def benchmark(size, provider):
    x = torch.rand(size, device='cuda', dtype=torch.float32)
    y = torch.rand(size, device='cuda', dtype=torch.float32)

    if provider == 'torch':
        ms = triton.testing.do_bench(lambda: x + y)
    if provider == 'triton':
        ms = triton.testing.do_bench(lambda: add(x, y))

    gbps = 3 * x.numel() * x.element_size() / ms * 1e-6
    return gbps
```

---

## ğŸ“Š æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Triton å­¦ä¹ è·¯çº¿                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Level 1: åŸºç¡€                                                          â”‚
â”‚   â”œâ”€â”€ ç†è§£block-basedç¼–ç¨‹æ¨¡å‹                                             â”‚
â”‚   â”œâ”€â”€ æŒæ¡program_id, arange, load, store                                â”‚
â”‚   â””â”€â”€ å®ç°Vector Add, Element-wise ops                                   â”‚
â”‚                                                                          â”‚
â”‚   Level 2: è¿›é˜¶                                                          â”‚
â”‚   â”œâ”€â”€ æŒæ¡reductionæ“ä½œ                                                   â”‚
â”‚   â”œâ”€â”€ å®ç°Softmax, LayerNorm                                             â”‚
â”‚   â””â”€â”€ ç†è§£å†…å­˜è®¿é—®æ¨¡å¼                                                     â”‚
â”‚                                                                          â”‚
â”‚   Level 3: é«˜çº§                                                          â”‚
â”‚   â”œâ”€â”€ å®ç°Matrix Multiplication                                          â”‚
â”‚   â”œâ”€â”€ ä½¿ç”¨Auto-tuning                                                    â”‚
â”‚   â””â”€â”€ ç†è§£ç¼–è¯‘æµç¨‹                                                        â”‚
â”‚                                                                          â”‚
â”‚   Level 4: ä¸“å®¶                                                          â”‚
â”‚   â”œâ”€â”€ å®ç°Flash Attention                                                â”‚
â”‚   â”œâ”€â”€ è‡ªå®šä¹‰å¤æ‚èåˆç®—å­                                                   â”‚
â”‚   â””â”€â”€ æ€§èƒ½è°ƒä¼˜è¾¾åˆ°CUDAæ°´å¹³                                                 â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

éœ€è¦æˆ‘æ·±å…¥è®²è§£æŸä¸ªç‰¹å®šä¸»é¢˜å—ï¼Ÿæ¯”å¦‚ï¼š

- Flash Attention çš„è¯¦ç»†å®ç°
- æ›´å¤šå®æˆ˜ç®—å­ç¤ºä¾‹
- Triton ä¸ PyTorch çš„é›†æˆ
- æ€§èƒ½è°ƒä¼˜å®æˆ˜
