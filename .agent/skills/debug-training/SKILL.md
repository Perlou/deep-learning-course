---
name: debug-training
description: è¯Šæ–­å’Œä¿®å¤æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­çš„å¸¸è§é—®é¢˜
---

# è°ƒè¯•è®­ç»ƒé—®é¢˜æŠ€èƒ½

æ­¤æŠ€èƒ½ç”¨äºè¯Šæ–­å’Œä¿®å¤æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸­çš„å¸¸è§é—®é¢˜ã€‚

## å¸¸è§é—®é¢˜æ£€æŸ¥æ¸…å•

### 1. Loss ä¸ä¸‹é™

**å¯èƒ½åŸå› **ï¼š

- [ ] å­¦ä¹ ç‡è¿‡é«˜æˆ–è¿‡ä½
- [ ] æ•°æ®æ²¡æœ‰æ­£ç¡®åŠ è½½ï¼ˆæ ‡ç­¾é”™ä¹±ï¼‰
- [ ] æ¨¡å‹ç»“æ„é—®é¢˜
- [ ] æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- [ ] æ•°æ®é¢„å¤„ç†é—®é¢˜

**è¯Šæ–­æ­¥éª¤**ï¼š

```python
# 1. æ£€æŸ¥æ•°æ®
for batch_idx, (data, target) in enumerate(train_loader):
    print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"æ ‡ç­¾å½¢çŠ¶: {target.shape}")
    print(f"æ•°æ®èŒƒå›´: [{data.min():.3f}, {data.max():.3f}]")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {torch.bincount(target)}")
    break

# 2. æ£€æŸ¥æ¨¡å‹è¾“å‡º
model.eval()
with torch.no_grad():
    output = model(data)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")

# 3. æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad mean={param.grad.mean():.6f}, grad std={param.grad.std():.6f}")
```

### 2. æ¢¯åº¦çˆ†ç‚¸

**ç—‡çŠ¶**ï¼š

- Loss å˜æˆ NaN æˆ– Inf
- æƒé‡å˜å¾—éå¸¸å¤§

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 2. é™ä½å­¦ä¹ ç‡
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

# 3. ä½¿ç”¨æ›´å¥½çš„åˆå§‹åŒ–
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)

model.apply(init_weights)
```

### 3. æ¢¯åº¦æ¶ˆå¤±

**ç—‡çŠ¶**ï¼š

- å‰å±‚æ¢¯åº¦æ¥è¿‘é›¶
- æƒé‡å‡ ä¹ä¸æ›´æ–°

**è¯Šæ–­**ï¼š

```python
# æ£€æŸ¥å„å±‚æ¢¯åº¦
def check_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            print(f"{name}: {grad_norm:.6e}")
            if grad_norm < 1e-7:
                print(f"  âš ï¸ æ¢¯åº¦å¯èƒ½æ¶ˆå¤±ï¼")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

- ä½¿ç”¨ ReLU ä»£æ›¿ Sigmoid/Tanh
- æ·»åŠ æ®‹å·®è¿æ¥
- ä½¿ç”¨ Batch Normalization
- ä½¿ç”¨ Xavier/He åˆå§‹åŒ–

### 4. è¿‡æ‹Ÿåˆ

**ç—‡çŠ¶**ï¼š

- è®­ç»ƒ loss æŒç»­ä¸‹é™
- éªŒè¯ loss å¼€å§‹ä¸Šå‡

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. Dropout
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Dropout(0.5),  # æ·»åŠ  Dropout
    nn.Linear(256, 10)
)

# 2. æƒé‡è¡°å‡
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# 3. æ—©åœ
best_val_loss = float('inf')
patience = 5
counter = 0

for epoch in range(epochs):
    val_loss = validate()
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

### 5. æ¬ æ‹Ÿåˆ

**ç—‡çŠ¶**ï¼š

- è®­ç»ƒå’ŒéªŒè¯ loss éƒ½å¾ˆé«˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

- å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆæ›´å¤šå±‚/ç¥ç»å…ƒï¼‰
- è®­ç»ƒæ›´å¤šè½®æ¬¡
- å‡å°‘æ­£åˆ™åŒ–
- æ£€æŸ¥æ•°æ®è´¨é‡

## è¯Šæ–­å·¥å…·å‡½æ•°

```python
def diagnose_training(model, train_loader, criterion, device):
    """
    è®­ç»ƒè¯Šæ–­å·¥å…·
    """
    print("=" * 50)
    print("è®­ç»ƒè¯Šæ–­æŠ¥å‘Š")
    print("=" * 50)

    # 1. æ¨¡å‹æ¦‚è§ˆ
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,} / å¯è®­ç»ƒ {trainable_params:,}")

    # 2. æ•°æ®æ£€æŸ¥
    data, target = next(iter(train_loader))
    print(f"\nğŸ“¦ æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"   æ ‡ç­¾å½¢çŠ¶: {target.shape}")
    print(f"   æ•°æ®ç±»å‹: {data.dtype}")

    # 3. å‰å‘ä¼ æ’­æ£€æŸ¥
    model.eval()
    data = data.to(device)
    target = target.to(device)

    with torch.no_grad():
        output = model(data)
        loss = criterion(output, target)

    print(f"\nğŸ”„ å‰å‘ä¼ æ’­:")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"   åˆå§‹ Loss: {loss.item():.4f}")

    # 4. æ¢¯åº¦æ£€æŸ¥
    model.train()
    output = model(data)
    loss = criterion(output, target)
    loss.backward()

    print(f"\nğŸ“ˆ æ¢¯åº¦ç»Ÿè®¡:")
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad = param.grad
            print(f"   {name}:")
            print(f"      èŒƒå›´: [{grad.min():.2e}, {grad.max():.2e}]")
            print(f"      å‡å€¼: {grad.mean():.2e}, æ ‡å‡†å·®: {grad.std():.2e}")

    print("\n" + "=" * 50)
```

## è°ƒè¯•æœ€ä½³å®è·µ

1. **ä»ç®€å•å¼€å§‹**ï¼šå…ˆç”¨å°æ•°æ®é›†ã€ç®€å•æ¨¡å‹éªŒè¯æµç¨‹
2. **é€æ­¥æ·»åŠ å¤æ‚æ€§**ï¼šä¸€æ¬¡åªæ”¹ä¸€ä¸ªä¸œè¥¿
3. **è®°å½•æ‰€æœ‰å®éªŒ**ï¼šä½¿ç”¨æ—¥å¿—è®°å½•æ¯æ¬¡å®éªŒçš„é…ç½®å’Œç»“æœ
4. **å¯è§†åŒ–ä¸€åˆ‡**ï¼šloss æ›²çº¿ã€æ¢¯åº¦åˆ†å¸ƒã€æƒé‡åˆ†å¸ƒ
5. **å…ˆè¿‡æ‹Ÿåˆå†æ­£åˆ™åŒ–**ï¼šç¡®ä¿æ¨¡å‹æœ‰èƒ½åŠ›æ‹Ÿåˆæ•°æ®
