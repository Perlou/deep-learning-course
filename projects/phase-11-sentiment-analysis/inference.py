"""
æ¨ç†è„šæœ¬
========

å¯¹æ–°æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ææ¨ç†ã€‚
"""

import argparse
from pathlib import Path

import torch

from config import DEVICE, MODELS_DIR, LABELS, NUM_CLASSES
from model import create_model
from dataset import text_to_indices, tokenize


def load_model(model_type="textcnn", model_path=None, device=None):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    device = device or DEVICE

    if model_path is None:
        model_path = MODELS_DIR / f"best_{model_type}.pth"
    model_path = Path(model_path)

    if not model_path.exists():
        print(f"âš  æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        return None, None, None

    checkpoint = torch.load(model_path, weights_only=False, map_location=device)
    vocab = checkpoint.get("vocab")

    # åˆ›å»ºæ¨¡å‹
    if model_type == "textcnn":
        from config import TEXTCNN_CONFIG

        model = create_model(
            model_type,
            vocab_size=len(vocab) if vocab else 20000,
            num_classes=NUM_CLASSES,
            **TEXTCNN_CONFIG,
        )
    elif model_type == "lstm":
        from config import LSTM_CONFIG

        model = create_model(
            model_type,
            vocab_size=len(vocab) if vocab else 20000,
            num_classes=NUM_CLASSES,
            **LSTM_CONFIG,
        )
    elif model_type == "bert":
        from config import BERT_CONFIG

        model = create_model(model_type, num_classes=NUM_CLASSES, **BERT_CONFIG)

    model.load_state_dict(checkpoint["model_state_dict"])
    model = model.to(device)
    model.eval()

    tokenizer = None
    if model_type == "bert":
        try:
            from transformers import AutoTokenizer
            from config import BERT_CONFIG

            tokenizer = AutoTokenizer.from_pretrained(BERT_CONFIG["model_name"])
        except ImportError:
            pass

    return model, vocab, tokenizer


@torch.no_grad()
def predict(text, model, vocab=None, tokenizer=None, model_type="textcnn", device=None):
    """
    é¢„æµ‹å•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ

    Args:
        text: è¾“å…¥æ–‡æœ¬
        model: æ¨¡å‹
        vocab: è¯è¡¨ (textcnn/lstm)
        tokenizer: tokenizer (bert)
        model_type: æ¨¡å‹ç±»å‹
        device: è®¾å¤‡

    Returns:
        label: é¢„æµ‹æ ‡ç­¾
        confidence: ç½®ä¿¡åº¦
    """
    device = device or DEVICE

    if model_type == "bert":
        if tokenizer is None:
            raise ValueError("BERT æ¨¡å‹éœ€è¦ tokenizer")

        encoding = tokenizer(
            text,
            truncation=True,
            max_length=256,
            padding="max_length",
            return_tensors="pt",
        )
        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)

        outputs = model(input_ids, attention_mask)
    else:
        if vocab is None:
            raise ValueError("TextCNN/LSTM æ¨¡å‹éœ€è¦ vocab")

        indices = text_to_indices(text, vocab)
        input_tensor = torch.tensor([indices], dtype=torch.long).to(device)

        outputs = model(input_tensor)

    probs = torch.softmax(outputs, dim=1)
    confidence, pred = probs.max(dim=1)

    label = pred.item()
    confidence = confidence.item()

    return label, confidence


def predict_interactive(model_type="textcnn"):
    """äº¤äº’å¼é¢„æµ‹"""
    print("=" * 60)
    print("ğŸ’¬ æƒ…æ„Ÿåˆ†æäº¤äº’æ¨¡å¼")
    print("=" * 60)
    print(f"æ¨¡å‹: {model_type}")
    print("è¾“å…¥ 'quit' æˆ– 'q' é€€å‡º")
    print("=" * 60)

    model, vocab, tokenizer = load_model(model_type)
    if model is None:
        return

    while True:
        try:
            text = input("\nè¯·è¾“å…¥æ–‡æœ¬: ").strip()

            if text.lower() in ["quit", "q", "exit"]:
                print("å†è§!")
                break

            if not text:
                continue

            label, confidence = predict(text, model, vocab, tokenizer, model_type)

            sentiment = LABELS[label]
            print(f"æƒ…æ„Ÿ: {sentiment}")
            print(f"ç½®ä¿¡åº¦: {confidence:.2%}")

        except KeyboardInterrupt:
            print("\nå†è§!")
            break


def predict_batch(texts, model_type="textcnn"):
    """æ‰¹é‡é¢„æµ‹"""
    model, vocab, tokenizer = load_model(model_type)
    if model is None:
        return []

    results = []
    for text in texts:
        label, confidence = predict(text, model, vocab, tokenizer, model_type)
        results.append(
            {
                "text": text,
                "label": label,
                "sentiment": LABELS[label],
                "confidence": confidence,
            }
        )

    return results


def demo_inference():
    """æ¨ç†æ¼”ç¤º"""
    print("=" * 60)
    print("ğŸ’¬ æƒ…æ„Ÿåˆ†ææ¨ç†æ¼”ç¤º")
    print("=" * 60)

    # æ£€æŸ¥æ¨¡å‹
    model_path = MODELS_DIR / "best_textcnn.pth"
    if not model_path.exists():
        print("\nâš  æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒ:")
        print("   python main.py train --quick")
        return

    model, vocab, _ = load_model("textcnn")
    if model is None:
        return

    # ç¤ºä¾‹æ–‡æœ¬
    test_texts = [
        "I love this movie, it's amazing!",
        "This is a terrible film, very disappointing.",
        "Great acting and wonderful story!",
        "Boring and waste of time.",
    ]

    print("\nğŸ“ ç¤ºä¾‹é¢„æµ‹:\n")
    for text in test_texts:
        label, confidence = predict(text, model, vocab, None, "textcnn")
        sentiment = LABELS[label]
        print(f"æ–‡æœ¬: {text}")
        print(f"æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {confidence:.2%})")
        print()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æƒ…æ„Ÿåˆ†ææ¨ç†")
    parser.add_argument(
        "--model",
        type=str,
        default="textcnn",
        choices=["textcnn", "lstm", "bert"],
        help="æ¨¡å‹ç±»å‹",
    )
    parser.add_argument("--text", type=str, default=None, help="è¾“å…¥æ–‡æœ¬")
    parser.add_argument("--interactive", "-i", action="store_true", help="äº¤äº’æ¨¡å¼")
    parser.add_argument("--demo", action="store_true", help="æ¼”ç¤ºæ¨¡å¼")

    args = parser.parse_args()

    if args.demo:
        demo_inference()
    elif args.interactive:
        predict_interactive(model_type=args.model)
    elif args.text:
        model, vocab, tokenizer = load_model(args.model)
        if model is not None:
            label, confidence = predict(args.text, model, vocab, tokenizer, args.model)
            print(f"æ–‡æœ¬: {args.text}")
            print(f"æƒ…æ„Ÿ: {LABELS[label]}")
            print(f"ç½®ä¿¡åº¦: {confidence:.2%}")
    else:
        print("ç”¨æ³•:")
        print("  python inference.py --demo")
        print("  python inference.py --text 'I love this movie!'")
        print("  python inference.py --interactive")
