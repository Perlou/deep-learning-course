"""
è®­ç»ƒè„šæœ¬
========

è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‚
"""

import argparse
from pathlib import Path

import torch
import torch.nn as nn
from torch.optim import Adam, AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from tqdm import tqdm

from config import (
    DEVICE,
    MODELS_DIR,
    RESULTS_DIR,
    TEXTCNN_CONFIG,
    LSTM_CONFIG,
    BERT_CONFIG,
    TRAIN_CONFIG,
    NUM_CLASSES,
)
from model import create_model, count_parameters
from dataset import get_dataloaders


def train_one_epoch(
    model, train_loader, criterion, optimizer, device, model_type="textcnn"
):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    pbar = tqdm(train_loader, desc="Training", leave=False)
    for batch in pbar:
        if model_type == "bert":
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)
            outputs = model(input_ids, attention_mask)
        else:
            texts, labels = batch
            texts = texts.to(device)
            labels = labels.to(device)
            outputs = model(texts)

        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

        pbar.set_postfix(
            {"loss": f"{loss.item():.4f}", "acc": f"{correct / total:.4f}"}
        )

    avg_loss = total_loss / len(train_loader)
    accuracy = correct / total

    return avg_loss, accuracy


@torch.no_grad()
def validate(model, val_loader, criterion, device, model_type="textcnn"):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    for batch in val_loader:
        if model_type == "bert":
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)
            outputs = model(input_ids, attention_mask)
        else:
            texts, labels = batch
            texts = texts.to(device)
            labels = labels.to(device)
            outputs = model(texts)

        loss = criterion(outputs, labels)

        total_loss += loss.item()
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    avg_loss = total_loss / len(val_loader)
    accuracy = correct / total

    return avg_loss, accuracy


def train(
    model_type="textcnn",
    data_dir=None,
    epochs=None,
    batch_size=None,
    learning_rate=None,
    device=None,
):
    """
    è®­ç»ƒæ¨¡å‹

    Args:
        model_type: æ¨¡å‹ç±»å‹ ("textcnn", "lstm", "bert")
        data_dir: æ•°æ®ç›®å½•
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹é‡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        device: è®¾å¤‡
    """
    epochs = epochs or TRAIN_CONFIG["epochs"]
    batch_size = batch_size or TRAIN_CONFIG["batch_size"]
    device = device or DEVICE

    if learning_rate is None:
        learning_rate = (
            TRAIN_CONFIG["bert_learning_rate"]
            if model_type == "bert"
            else TRAIN_CONFIG["learning_rate"]
        )

    print("=" * 60)
    print("ğŸ“ æƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    print(f"æ¨¡å‹: {model_type}")
    print(f"è®¾å¤‡: {device}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print("=" * 60)

    # æ•°æ®åŠ è½½
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    tokenizer = None
    if model_type == "bert":
        try:
            from transformers import AutoTokenizer

            tokenizer = AutoTokenizer.from_pretrained(BERT_CONFIG["model_name"])
        except ImportError:
            print("âš  æœªå®‰è£… transformersï¼Œè¯·è¿è¡Œ: pip install transformers")
            return None, None

    train_loader, val_loader, vocab_size, vocab = get_dataloaders(
        data_dir=data_dir,
        batch_size=batch_size,
        num_workers=4,
        model_type=model_type,
        tokenizer=tokenizer,
    )

    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    if model_type == "textcnn":
        model = create_model(
            model_type, vocab_size=vocab_size, num_classes=NUM_CLASSES, **TEXTCNN_CONFIG
        )
    elif model_type == "lstm":
        model = create_model(
            model_type, vocab_size=vocab_size, num_classes=NUM_CLASSES, **LSTM_CONFIG
        )
    elif model_type == "bert":
        model = create_model(model_type, num_classes=NUM_CLASSES, **BERT_CONFIG)

    model = model.to(device)
    print(f"å‚æ•°é‡: {count_parameters(model):,}")

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()

    if model_type == "bert":
        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    else:
        optimizer = Adam(
            model.parameters(),
            lr=learning_rate,
            weight_decay=TRAIN_CONFIG["weight_decay"],
        )

    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)

    # è®­ç»ƒå†å²
    history = {
        "train_loss": [],
        "train_acc": [],
        "val_loss": [],
        "val_acc": [],
    }

    # æ—©åœ
    best_acc = 0
    patience = TRAIN_CONFIG["early_stopping_patience"]
    patience_counter = 0

    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(epochs):
        print(f"\nEpoch {epoch + 1}/{epochs}")
        print("-" * 40)

        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, model_type
        )

        val_loss, val_acc = validate(model, val_loader, criterion, device, model_type)

        scheduler.step()

        history["train_loss"].append(train_loss)
        history["train_acc"].append(train_acc)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)

        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            patience_counter = 0

            save_path = MODELS_DIR / f"best_{model_type}.pth"
            torch.save(
                {
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "val_acc": val_acc,
                    "vocab": vocab if model_type != "bert" else None,
                },
                save_path,
            )
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.4f})")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"\nâš  æ—©åœè§¦å‘ (è¿ç»­ {patience} è½®æœªæå‡)")
            break

    print("\n" + "=" * 60)
    print("âœ“ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")
    print(f"æ¨¡å‹ä¿å­˜äº: {MODELS_DIR}")
    print("=" * 60)

    return model, history


def quick_train(model_type="textcnn", epochs=5):
    """å¿«é€Ÿè®­ç»ƒ"""
    print("\nâš¡ å¿«é€Ÿè®­ç»ƒæ¨¡å¼")
    return train(
        model_type=model_type,
        epochs=epochs,
        batch_size=32,
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹")
    parser.add_argument(
        "--model",
        type=str,
        default="textcnn",
        choices=["textcnn", "lstm", "bert"],
        help="æ¨¡å‹ç±»å‹",
    )
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=32, help="æ‰¹é‡å¤§å°")
    parser.add_argument("--lr", type=float, default=None, help="å­¦ä¹ ç‡")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿè®­ç»ƒæ¨¡å¼")

    args = parser.parse_args()

    if args.quick:
        quick_train(model_type=args.model)
    else:
        train(
            model_type=args.model,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
        )
