"""
CIFAR-10 å›¾åƒåˆ†ç±» - ResNet å®ç°
Phase 5 å®æˆ˜é¡¹ç›®

å­¦ä¹ ç›®æ ‡ï¼š
1. å·ç§¯ç¥ç»ç½‘ç»œå›¾åƒåˆ†ç±»
2. æ®‹å·®è¿æ¥åŸç†ä¸å®ç°
3. æ•°æ®å¢å¼ºæŠ€æœ¯
4. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import os
import time

plt.rcParams["font.sans-serif"] = ["Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

print("=" * 60)
print("Phase 5 å®æˆ˜é¡¹ç›®ï¼šCIFAR-10 å›¾åƒåˆ†ç±»")
print("=" * 60)


# =============================================================================
# 1. é…ç½®
# =============================================================================
class Config:
    # æ•°æ®
    data_dir = "./data"
    batch_size = 128
    num_workers = 0  # macOS å…¼å®¹

    # æ¨¡å‹
    num_classes = 10

    # è®­ç»ƒ
    num_epochs = 100
    learning_rate = 0.1
    momentum = 0.9
    weight_decay = 5e-4

    # ä¿å­˜
    save_dir = "./outputs"

    # è®¾å¤‡
    device = torch.device(
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )


config = Config()
os.makedirs(config.save_dir, exist_ok=True)
os.makedirs(config.data_dir, exist_ok=True)
print(f"\nä½¿ç”¨è®¾å¤‡: {config.device}")

# CIFAR-10 ç±»åˆ«åç§°
CLASSES = [
    "airplane",
    "automobile",
    "bird",
    "cat",
    "deer",
    "dog",
    "frog",
    "horse",
    "ship",
    "truck",
]


# =============================================================================
# 2. æ•°æ®å‡†å¤‡
# =============================================================================
print("\n" + "=" * 60)
print("ã€1. æ•°æ®å‡†å¤‡ã€‘")

# CIFAR-10 æ ‡å‡†åŒ–å‚æ•°
CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)
CIFAR10_STD = (0.2470, 0.2435, 0.2616)

# è®­ç»ƒé›†æ•°æ®å¢å¼º
train_transform = transforms.Compose(
    [
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
        transforms.ToTensor(),
        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),
    ]
)

# æµ‹è¯•é›†å˜æ¢
test_transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)]
)

# åŠ è½½æ•°æ®é›†
train_dataset = torchvision.datasets.CIFAR10(
    root=config.data_dir, train=True, download=True, transform=train_transform
)
test_dataset = torchvision.datasets.CIFAR10(
    root=config.data_dir, train=False, download=True, transform=test_transform
)

train_loader = DataLoader(
    train_dataset,
    batch_size=config.batch_size,
    shuffle=True,
    num_workers=config.num_workers,
    pin_memory=True,
)
test_loader = DataLoader(
    test_dataset,
    batch_size=config.batch_size,
    shuffle=False,
    num_workers=config.num_workers,
    pin_memory=True,
)

print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
print(f"ç±»åˆ«æ•°: {config.num_classes}")
print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")


# =============================================================================
# 3. å¯è§†åŒ–æ ·æœ¬
# =============================================================================
print("\n" + "=" * 60)
print("ã€2. æ ·æœ¬å¯è§†åŒ–ã€‘")


def visualize_samples(dataset, num_samples=16):
    """å¯è§†åŒ–æ•°æ®æ ·æœ¬"""
    fig, axes = plt.subplots(4, 4, figsize=(8, 8))

    for i, ax in enumerate(axes.flat):
        if i >= num_samples:
            break
        img, label = dataset[i]
        # åæ ‡å‡†åŒ–
        img = img.numpy().transpose(1, 2, 0)
        img = img * np.array(CIFAR10_STD) + np.array(CIFAR10_MEAN)
        img = np.clip(img, 0, 1)

        ax.imshow(img)
        ax.set_title(CLASSES[label])
        ax.axis("off")

    plt.suptitle("CIFAR-10 æ ·æœ¬", fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{config.save_dir}/samples.png", dpi=100)
    plt.close()


# ä½¿ç”¨åŸå§‹æ•°æ®é›†å¯è§†åŒ–ï¼ˆä¸å¸¦å¢å¼ºï¼‰
vis_dataset = torchvision.datasets.CIFAR10(
    root=config.data_dir,
    train=True,
    download=False,
    transform=transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)]
    ),
)
visualize_samples(vis_dataset)
print(f"æ ·æœ¬å›¾å·²ä¿å­˜: {config.save_dir}/samples.png")


# =============================================================================
# 4. ResNet æ¨¡å‹å®šä¹‰
# =============================================================================
print("\n" + "=" * 60)
print("ã€3. æ¨¡å‹å®šä¹‰ã€‘")


class BasicBlock(nn.Module):
    """ResNet åŸºæœ¬æ®‹å·®å—"""

    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample

    def forward(self, x):
        identity = x

        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = F.relu(out)
        return out


class ResNetCIFAR(nn.Module):
    """
    ResNet for CIFAR-10

    ä¸ ImageNet ç‰ˆæœ¬çš„åŒºåˆ«ï¼š
    - ç¬¬ä¸€å±‚ç”¨ 3Ã—3 å·ç§¯ï¼ˆè€Œé 7Ã—7ï¼‰
    - ç§»é™¤ç¬¬ä¸€ä¸ª MaxPoolï¼ˆå› ä¸ºå›¾åƒåªæœ‰ 32Ã—32ï¼‰
    """

    def __init__(self, block, layers, num_classes=10):
        super().__init__()
        self.in_channels = 64

        # ç¬¬ä¸€å±‚ï¼š3Ã—3 å·ç§¯ï¼Œä¸é™é‡‡æ ·
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        # æ®‹å·®å±‚
        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        # åˆ†ç±»å™¨
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()

    def _make_layer(self, block, out_channels, blocks, stride=1):
        """åˆ›å»ºæ®‹å·®å±‚"""
        downsample = None
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.in_channels,
                    out_channels * block.expansion,
                    1,
                    stride,
                    bias=False,
                ),
                nn.BatchNorm2d(out_channels * block.expansion),
            )

        layers = [block(self.in_channels, out_channels, stride, downsample)]
        self.in_channels = out_channels * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))

        return nn.Sequential(*layers)

    def _initialize_weights(self):
        """Kaiming åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


def resnet18_cifar(num_classes=10):
    """ResNet-18 for CIFAR-10"""
    return ResNetCIFAR(BasicBlock, [2, 2, 2, 2], num_classes)


# åˆ›å»ºæ¨¡å‹
model = resnet18_cifar(num_classes=config.num_classes).to(config.device)
print(f"\næ¨¡å‹: ResNet-18 for CIFAR-10")

# ç»Ÿè®¡å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ€»å‚æ•°é‡: {total_params:,} ({total_params / 1e6:.2f}M)")

# æµ‹è¯•å‰å‘ä¼ æ’­
dummy_input = torch.randn(2, 3, 32, 32).to(config.device)
dummy_output = model(dummy_input)
print(f"è¾“å…¥: {dummy_input.shape} â†’ è¾“å‡º: {dummy_output.shape}")


# =============================================================================
# 5. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
# =============================================================================
print("\n" + "=" * 60)
print("ã€4. è®­ç»ƒé…ç½®ã€‘")

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(
    model.parameters(),
    lr=config.learning_rate,
    momentum=config.momentum,
    weight_decay=config.weight_decay,
)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)

print(f"æŸå¤±å‡½æ•°: CrossEntropyLoss")
print(
    f"ä¼˜åŒ–å™¨: SGD (lr={config.learning_rate}, momentum={config.momentum}, wd={config.weight_decay})"
)
print(f"å­¦ä¹ ç‡è°ƒåº¦: CosineAnnealingLR")


# =============================================================================
# 6. è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# =============================================================================
def train_epoch(model, loader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    return running_loss / total, correct / total


def evaluate(model, loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return running_loss / total, correct / total, all_preds, all_labels


# =============================================================================
# 7. è®­ç»ƒå¾ªç¯
# =============================================================================
print("\n" + "=" * 60)
print("ã€5. å¼€å§‹è®­ç»ƒã€‘")

history = {"train_loss": [], "train_acc": [], "test_loss": [], "test_acc": [], "lr": []}

best_acc = 0.0
best_model_state = None
start_time = time.time()

for epoch in range(config.num_epochs):
    # è®­ç»ƒ
    train_loss, train_acc = train_epoch(
        model, train_loader, criterion, optimizer, config.device
    )

    # éªŒè¯
    test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, config.device)

    # è®°å½•å†å²
    current_lr = optimizer.param_groups[0]["lr"]
    history["train_loss"].append(train_loss)
    history["train_acc"].append(train_acc)
    history["test_loss"].append(test_loss)
    history["test_acc"].append(test_acc)
    history["lr"].append(current_lr)

    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if test_acc > best_acc:
        best_acc = test_acc
        best_model_state = model.state_dict().copy()

    # æ‰“å°è¿›åº¦
    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(
            f"Epoch {epoch + 1:3d}/{config.num_epochs} | "
            f"Train: {train_acc * 100:.2f}% | Test: {test_acc * 100:.2f}% | "
            f"LR: {current_lr:.4f}"
        )

elapsed_time = time.time() - start_time
print(f"\nè®­ç»ƒå®Œæˆ! ç”¨æ—¶: {elapsed_time / 60:.1f} åˆ†é’Ÿ")
print(f"æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc * 100:.2f}%")


# =============================================================================
# 8. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
# =============================================================================
print("\n" + "=" * 60)
print("ã€6. è®­ç»ƒå¯è§†åŒ–ã€‘")

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# æŸå¤±æ›²çº¿
ax1 = axes[0]
ax1.plot(history["train_loss"], label="Train", color="blue")
ax1.plot(history["test_loss"], label="Test", color="orange")
ax1.set_xlabel("Epoch")
ax1.set_ylabel("Loss")
ax1.set_title("æŸå¤±æ›²çº¿")
ax1.legend()
ax1.grid(True, alpha=0.3)

# å‡†ç¡®ç‡æ›²çº¿
ax2 = axes[1]
ax2.plot([acc * 100 for acc in history["train_acc"]], label="Train", color="blue")
ax2.plot([acc * 100 for acc in history["test_acc"]], label="Test", color="orange")
ax2.set_xlabel("Epoch")
ax2.set_ylabel("Accuracy (%)")
ax2.set_title("å‡†ç¡®ç‡æ›²çº¿")
ax2.legend()
ax2.grid(True, alpha=0.3)
ax2.axhline(y=90, color="red", linestyle="--", alpha=0.5, label="90% ç›®æ ‡")

# å­¦ä¹ ç‡æ›²çº¿
ax3 = axes[2]
ax3.plot(history["lr"], color="green")
ax3.set_xlabel("Epoch")
ax3.set_ylabel("Learning Rate")
ax3.set_title("å­¦ä¹ ç‡è°ƒåº¦")
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{config.save_dir}/training_curves.png", dpi=100)
plt.close()
print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {config.save_dir}/training_curves.png")


# =============================================================================
# 9. æœ€ç»ˆè¯„ä¼°
# =============================================================================
print("\n" + "=" * 60)
print("ã€7. æœ€ç»ˆè¯„ä¼°ã€‘")

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(best_model_state)

# æœ€ç»ˆè¯„ä¼°
test_loss, test_acc, all_preds, all_labels = evaluate(
    model, test_loader, criterion, config.device
)

print(f"\næœ€ç»ˆæµ‹è¯•é›†ç»“æœ:")
print(f"  Loss: {test_loss:.4f}")
print(f"  Accuracy: {test_acc * 100:.2f}%")

# æ˜¯å¦è¾¾æ ‡
if test_acc >= 0.90:
    print(f"\nğŸ‰ æ­å–œï¼å·²è¾¾åˆ° 90%+ å‡†ç¡®ç‡ç›®æ ‡ï¼")
else:
    print(f"\nâš ï¸ æœªè¾¾åˆ° 90% ç›®æ ‡ï¼Œå¯å°è¯•å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´è¶…å‚æ•°")


# =============================================================================
# 10. æ··æ·†çŸ©é˜µ
# =============================================================================
print("\n" + "=" * 60)
print("ã€8. æ··æ·†çŸ©é˜µã€‘")

cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(10, 8))
plt.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
plt.title("æ··æ·†çŸ©é˜µ")
plt.colorbar()

tick_marks = np.arange(len(CLASSES))
plt.xticks(tick_marks, CLASSES, rotation=45, ha="right")
plt.yticks(tick_marks, CLASSES)

# æ·»åŠ æ•°å­—æ ‡æ³¨
thresh = cm.max() / 2.0
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(
            j,
            i,
            format(cm[i, j], "d"),
            ha="center",
            va="center",
            color="white" if cm[i, j] > thresh else "black",
            fontsize=8,
        )

plt.xlabel("é¢„æµ‹ç±»åˆ«")
plt.ylabel("çœŸå®ç±»åˆ«")
plt.tight_layout()
plt.savefig(f"{config.save_dir}/confusion_matrix.png", dpi=100)
plt.close()
print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜: {config.save_dir}/confusion_matrix.png")

# æ¯ç±»å‡†ç¡®ç‡
print("\nå„ç±»åˆ«å‡†ç¡®ç‡:")
for i, class_name in enumerate(CLASSES):
    class_acc = cm[i, i] / cm[i].sum() * 100
    print(f"  {class_name:12s}: {class_acc:.1f}%")


# =============================================================================
# 11. é¢„æµ‹å¯è§†åŒ–
# =============================================================================
print("\n" + "=" * 60)
print("ã€9. é¢„æµ‹å¯è§†åŒ–ã€‘")


def visualize_predictions(model, dataset, device, num_samples=16):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    model.eval()
    fig, axes = plt.subplots(4, 4, figsize=(10, 10))

    # éšæœºé€‰æ‹©æ ·æœ¬
    indices = np.random.choice(len(dataset), num_samples, replace=False)

    for i, ax in enumerate(axes.flat):
        idx = indices[i]
        image, true_label = dataset[idx]

        # é¢„æµ‹
        with torch.no_grad():
            output = model(image.unsqueeze(0).to(device))
            _, pred_label = output.max(1)
            pred_label = pred_label.item()

        # åæ ‡å‡†åŒ–æ˜¾ç¤º
        img = image.numpy().transpose(1, 2, 0)
        img = img * np.array(CIFAR10_STD) + np.array(CIFAR10_MEAN)
        img = np.clip(img, 0, 1)

        ax.imshow(img)

        # æ ‡é¢˜é¢œè‰²
        color = "green" if pred_label == true_label else "red"
        ax.set_title(
            f"True: {CLASSES[true_label]}\nPred: {CLASSES[pred_label]}",
            color=color,
            fontsize=9,
        )
        ax.axis("off")

    plt.suptitle("é¢„æµ‹ç»“æœ (ç»¿è‰²=æ­£ç¡®, çº¢è‰²=é”™è¯¯)", fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{config.save_dir}/predictions.png", dpi=100)
    plt.close()


visualize_predictions(model, test_dataset, config.device)
print(f"é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜: {config.save_dir}/predictions.png")


# =============================================================================
# 12. ä¿å­˜æ¨¡å‹
# =============================================================================
print("\n" + "=" * 60)
print("ã€10. ä¿å­˜æ¨¡å‹ã€‘")

model_path = f"{config.save_dir}/best_model.pth"
torch.save(
    {
        "model_state_dict": best_model_state,
        "test_acc": best_acc,
        "config": {"num_classes": config.num_classes, "architecture": "ResNet-18"},
    },
    model_path,
)
print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")


# =============================================================================
# 13. æ€»ç»“
# =============================================================================
print("\n" + "=" * 60)
print("ã€é¡¹ç›®æ€»ç»“ã€‘")
print("=" * 60)

print(f"""
åº”ç”¨çš„ Phase 5 çŸ¥è¯†ç‚¹:
  âœ… å·ç§¯å±‚ (Conv2d) - ç‰¹å¾æå–
  âœ… æ± åŒ–å±‚ (AdaptiveAvgPool2d) - é™ç»´
  âœ… æ®‹å·®è¿æ¥ - è§£å†³æ¢¯åº¦æ¶ˆå¤±
  âœ… BatchNorm - åŠ é€Ÿè®­ç»ƒã€ç¨³å®šæ¢¯åº¦
  âœ… æ•°æ®å¢å¼º - æå‡æ³›åŒ–èƒ½åŠ›
  âœ… Kaiming åˆå§‹åŒ– - é€‚åˆ ReLU
  âœ… CosineAnnealing å­¦ä¹ ç‡è°ƒåº¦

CIFAR-10 ç‰ˆ ResNet ä¿®æ”¹:
  â€¢ ç¬¬ä¸€å±‚: 3Ã—3 å·ç§¯ (é 7Ã—7)
  â€¢ ç§»é™¤ç¬¬ä¸€ä¸ª MaxPool

ç»“æœ:
  â€¢ æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc * 100:.2f}%
  â€¢ è®­ç»ƒæ—¶é—´: {elapsed_time / 60:.1f} åˆ†é’Ÿ

ç”Ÿæˆæ–‡ä»¶:
  ğŸ“Š {config.save_dir}/samples.png
  ğŸ“Š {config.save_dir}/training_curves.png
  ğŸ“Š {config.save_dir}/confusion_matrix.png
  ğŸ“Š {config.save_dir}/predictions.png
  ğŸ’¾ {config.save_dir}/best_model.pth
""")

print("âœ… Phase 5 å®æˆ˜é¡¹ç›®å®Œæˆï¼")
