"""
æˆ¿ä»·é¢„æµ‹ MLP å›å½’æ¨¡å‹
Phase 4 å®æˆ˜é¡¹ç›®

å­¦ä¹ ç›®æ ‡ï¼š
1. MLP åœ¨å›å½’ä»»åŠ¡ä¸­çš„åº”ç”¨
2. ç‰¹å¾æ ‡å‡†åŒ–å’Œæ•°æ®é¢„å¤„ç†
3. æ­£åˆ™åŒ–æŠ€æœ¯ï¼šDropout, BatchNorm, L2
4. å›å½’è¯„ä¼°æŒ‡æ ‡ï¼šMSE, RMSE, MAE, RÂ²
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os
import time

plt.rcParams["font.sans-serif"] = ["Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

print("=" * 60)
print("Phase 4 å®æˆ˜é¡¹ç›®ï¼šæˆ¿ä»·é¢„æµ‹ MLP å›å½’")
print("=" * 60)


# =============================================================================
# 1. é…ç½®
# =============================================================================
class Config:
    # æ•°æ®
    test_size = 0.2
    val_size = 0.1

    # æ¨¡å‹
    hidden_dims = [128, 64, 32]
    dropout_rate = 0.2

    # è®­ç»ƒ
    batch_size = 64
    learning_rate = 0.001
    weight_decay = 1e-4  # L2 æ­£åˆ™åŒ–
    num_epochs = 100
    patience = 15  # æ—©åœ

    # ä¿å­˜
    save_dir = "./outputs"

    # è®¾å¤‡
    device = torch.device(
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )


config = Config()
os.makedirs(config.save_dir, exist_ok=True)
print(f"\nä½¿ç”¨è®¾å¤‡: {config.device}")


# =============================================================================
# 2. æ•°æ®å‡†å¤‡
# =============================================================================
print("\n" + "=" * 60)
print("ã€1. æ•°æ®å‡†å¤‡ã€‘")

# åŠ è½½ California Housing æ•°æ®é›†
housing = fetch_california_housing()
X, y = housing.data, housing.target
feature_names = housing.feature_names

print(f"\næ•°æ®é›†ä¿¡æ¯:")
print(f"  æ ·æœ¬æ•°: {X.shape[0]}")
print(f"  ç‰¹å¾æ•°: {X.shape[1]}")
print(f"  ç‰¹å¾å: {feature_names}")
print(f"  ç›®æ ‡èŒƒå›´: {y.min():.2f} ~ {y.max():.2f} (å•ä½: $100,000)")

# åˆ’åˆ†æ•°æ®é›†
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=config.test_size, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=config.val_size / (1 - config.test_size), random_state=42
)

print(f"\næ•°æ®åˆ’åˆ†:")
print(f"  è®­ç»ƒé›†: {len(X_train)}")
print(f"  éªŒè¯é›†: {len(X_val)}")
print(f"  æµ‹è¯•é›†: {len(X_test)}")

# ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

# ç›®æ ‡å€¼ä¹Ÿæ ‡å‡†åŒ–ï¼Œæœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()

print(f"\nç‰¹å¾æ ‡å‡†åŒ–å:")
print(f"  X å‡å€¼: {X_train_scaled.mean():.4f}")
print(f"  X æ ‡å‡†å·®: {X_train_scaled.std():.4f}")


# è½¬æ¢ä¸º PyTorch å¼ é‡
def to_tensor(X, y, device):
    X_t = torch.FloatTensor(X).to(device)
    y_t = torch.FloatTensor(y).unsqueeze(1).to(device)
    return TensorDataset(X_t, y_t)


train_dataset = to_tensor(X_train_scaled, y_train_scaled, config.device)
val_dataset = to_tensor(X_val_scaled, y_val_scaled, config.device)
test_dataset = to_tensor(X_test_scaled, y_test_scaled, config.device)

train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config.batch_size)
test_loader = DataLoader(test_dataset, batch_size=config.batch_size)


# =============================================================================
# 3. å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ
# =============================================================================
print("\n" + "=" * 60)
print("ã€2. æ•°æ®å¯è§†åŒ–ã€‘")

fig, axes = plt.subplots(2, 4, figsize=(14, 7))
axes = axes.flatten()

for i, (name, ax) in enumerate(zip(feature_names, axes)):
    ax.hist(X[:, i], bins=30, edgecolor="black", alpha=0.7)
    ax.set_title(name)
    ax.set_xlabel("Value")
    ax.set_ylabel("Count")

plt.suptitle("ç‰¹å¾åˆ†å¸ƒ", fontsize=14, y=1.02)
plt.tight_layout()
plt.savefig(
    f"{config.save_dir}/feature_distributions.png", dpi=100, bbox_inches="tight"
)
plt.close()
print(f"ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜: {config.save_dir}/feature_distributions.png")

# ç›®æ ‡åˆ†å¸ƒ
plt.figure(figsize=(8, 5))
plt.hist(y, bins=50, edgecolor="black", alpha=0.7, color="steelblue")
plt.xlabel("æˆ¿ä»·ä¸­ä½æ•° ($100,000)")
plt.ylabel("æ ·æœ¬æ•°")
plt.title("ç›®æ ‡å˜é‡åˆ†å¸ƒ")
plt.axvline(y.mean(), color="red", linestyle="--", label=f"å‡å€¼: {y.mean():.2f}")
plt.legend()
plt.savefig(f"{config.save_dir}/target_distribution.png", dpi=100)
plt.close()
print(f"ç›®æ ‡åˆ†å¸ƒå›¾å·²ä¿å­˜: {config.save_dir}/target_distribution.png")


# =============================================================================
# 4. æ¨¡å‹å®šä¹‰
# =============================================================================
print("\n" + "=" * 60)
print("ã€3. æ¨¡å‹å®šä¹‰ã€‘")


class MLPRegressor(nn.Module):
    """
    å¤šå±‚æ„ŸçŸ¥æœºå›å½’æ¨¡å‹

    ç‰¹ç‚¹:
    - He åˆå§‹åŒ– (é€‚åˆ ReLU)
    - BatchNorm (åŠ é€Ÿè®­ç»ƒã€ç¨³å®šæ¢¯åº¦)
    - Dropout (é˜²æ­¢è¿‡æ‹Ÿåˆ)
    - çº¿æ€§è¾“å‡ºå±‚ (å›å½’ä»»åŠ¡)
    """

    def __init__(self, input_dim, hidden_dims, dropout_rate=0.2):
        super().__init__()

        layers = []
        prev_dim = input_dim

        for i, hidden_dim in enumerate(hidden_dims):
            # çº¿æ€§å±‚
            linear = nn.Linear(prev_dim, hidden_dim)

            # He åˆå§‹åŒ–
            nn.init.kaiming_normal_(linear.weight, mode="fan_in", nonlinearity="relu")
            nn.init.zeros_(linear.bias)

            layers.append(linear)

            # BatchNorm (é™¤äº†æœ€åä¸€å±‚)
            if i < len(hidden_dims) - 1:
                layers.append(nn.BatchNorm1d(hidden_dim))

            # æ¿€æ´»å‡½æ•°
            layers.append(nn.ReLU(inplace=True))

            # Dropout (é™¤äº†æœ€åä¸€å±‚)
            if i < len(hidden_dims) - 1:
                layers.append(nn.Dropout(dropout_rate))

            prev_dim = hidden_dim

        self.hidden = nn.Sequential(*layers)

        # è¾“å‡ºå±‚ - çº¿æ€§ï¼ˆå›å½’ä»»åŠ¡æ— æ¿€æ´»å‡½æ•°ï¼‰
        self.output = nn.Linear(hidden_dims[-1], 1)
        nn.init.kaiming_normal_(
            self.output.weight, mode="fan_in", nonlinearity="linear"
        )
        nn.init.zeros_(self.output.bias)

    def forward(self, x):
        x = self.hidden(x)
        x = self.output(x)
        return x


# åˆ›å»ºæ¨¡å‹
input_dim = X.shape[1]
model = MLPRegressor(
    input_dim=input_dim,
    hidden_dims=config.hidden_dims,
    dropout_rate=config.dropout_rate,
).to(config.device)

print(f"\næ¨¡å‹ç»“æ„:\n{model}")

# ç»Ÿè®¡å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\næ€»å‚æ•°é‡: {total_params:,}")
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")


# =============================================================================
# 5. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
# =============================================================================
print("\n" + "=" * 60)
print("ã€4. è®­ç»ƒé…ç½®ã€‘")

criterion = nn.MSELoss()
optimizer = optim.Adam(
    model.parameters(),
    lr=config.learning_rate,
    weight_decay=config.weight_decay,  # L2 æ­£åˆ™åŒ–
)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.5, patience=5
)

print(f"æŸå¤±å‡½æ•°: MSE")
print(f"ä¼˜åŒ–å™¨: Adam (lr={config.learning_rate}, weight_decay={config.weight_decay})")
print(f"å­¦ä¹ ç‡è°ƒåº¦: ReduceLROnPlateau")


# =============================================================================
# 6. è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# =============================================================================
def train_epoch(model, loader, criterion, optimizer):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0

    for X_batch, y_batch in loader:
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * len(X_batch)

    return total_loss / len(loader.dataset)


def evaluate(model, loader, criterion):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    predictions = []
    targets = []

    with torch.no_grad():
        for X_batch, y_batch in loader:
            pred = model(X_batch)
            loss = criterion(pred, y_batch)
            total_loss += loss.item() * len(X_batch)
            predictions.append(pred.cpu().numpy())
            targets.append(y_batch.cpu().numpy())

    predictions = np.concatenate(predictions)
    targets = np.concatenate(targets)
    avg_loss = total_loss / len(loader.dataset)

    return avg_loss, predictions, targets


# =============================================================================
# 7. è®­ç»ƒå¾ªç¯
# =============================================================================
print("\n" + "=" * 60)
print("ã€5. å¼€å§‹è®­ç»ƒã€‘")

history = {"train_loss": [], "val_loss": [], "lr": []}

best_val_loss = float("inf")
best_model_state = None
epochs_without_improvement = 0
start_time = time.time()

for epoch in range(config.num_epochs):
    # è®­ç»ƒ
    train_loss = train_epoch(model, train_loader, criterion, optimizer)

    # éªŒè¯
    val_loss, _, _ = evaluate(model, val_loader, criterion)

    # è®°å½•å†å²
    current_lr = optimizer.param_groups[0]["lr"]
    history["train_loss"].append(train_loss)
    history["val_loss"].append(val_loss)
    history["lr"].append(current_lr)

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = model.state_dict().copy()
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1

    # æ‰“å°è¿›åº¦
    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(
            f"Epoch {epoch + 1:3d}/{config.num_epochs} | "
            f"Train Loss: {train_loss:.4f} | "
            f"Val Loss: {val_loss:.4f} | "
            f"LR: {current_lr:.6f}"
        )

    # æ—©åœ
    if epochs_without_improvement >= config.patience:
        print(f"\næ—©åœè§¦å‘! éªŒè¯æŸå¤±å·² {config.patience} ä¸ª epoch æœªæ”¹å–„")
        break

elapsed_time = time.time() - start_time
print(f"\nè®­ç»ƒå®Œæˆ! ç”¨æ—¶: {elapsed_time:.2f}s")
print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


# =============================================================================
# 8. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
# =============================================================================
print("\n" + "=" * 60)
print("ã€6. è®­ç»ƒå¯è§†åŒ–ã€‘")

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# æŸå¤±æ›²çº¿
ax1 = axes[0]
ax1.plot(history["train_loss"], label="è®­ç»ƒæŸå¤±", color="blue")
ax1.plot(history["val_loss"], label="éªŒè¯æŸå¤±", color="orange")
ax1.set_xlabel("Epoch")
ax1.set_ylabel("MSE Loss")
ax1.set_title("è®­ç»ƒæ›²çº¿")
ax1.legend()
ax1.grid(True, alpha=0.3)

# å­¦ä¹ ç‡æ›²çº¿
ax2 = axes[1]
ax2.plot(history["lr"], color="green")
ax2.set_xlabel("Epoch")
ax2.set_ylabel("Learning Rate")
ax2.set_title("å­¦ä¹ ç‡å˜åŒ–")
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{config.save_dir}/training_curves.png", dpi=100)
plt.close()
print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {config.save_dir}/training_curves.png")


# =============================================================================
# 9. æµ‹è¯•è¯„ä¼°
# =============================================================================
print("\n" + "=" * 60)
print("ã€7. æµ‹è¯•è¯„ä¼°ã€‘")

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(best_model_state)

# æµ‹è¯•
test_loss, predictions_scaled, targets_scaled = evaluate(model, test_loader, criterion)

# åæ ‡å‡†åŒ–é¢„æµ‹å€¼
predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()
targets = scaler_y.inverse_transform(targets_scaled.reshape(-1, 1)).flatten()

# è®¡ç®—å›å½’æŒ‡æ ‡
mse = mean_squared_error(targets, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(targets, predictions)
r2 = r2_score(targets, predictions)

print(f"\næµ‹è¯•é›†è¯„ä¼°æŒ‡æ ‡:")
print(f"  MSE:  {mse:.4f}")
print(f"  RMSE: {rmse:.4f}")
print(f"  MAE:  {mae:.4f}")
print(f"  RÂ²:   {r2:.4f}")

# è§£é‡Š RÂ²
if r2 > 0.8:
    r2_comment = "ä¼˜ç§€"
elif r2 > 0.6:
    r2_comment = "è‰¯å¥½"
elif r2 > 0.4:
    r2_comment = "ä¸€èˆ¬"
else:
    r2_comment = "è¾ƒå·®"
print(f"\næ¨¡å‹è¡¨ç°: {r2_comment} (RÂ² = {r2:.4f})")


# =============================================================================
# 10. é¢„æµ‹å¯è§†åŒ–
# =============================================================================
print("\n" + "=" * 60)
print("ã€8. é¢„æµ‹å¯è§†åŒ–ã€‘")

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# é¢„æµ‹ vs çœŸå®å€¼æ•£ç‚¹å›¾
ax1 = axes[0]
ax1.scatter(targets, predictions, alpha=0.5, s=10)
ax1.plot(
    [targets.min(), targets.max()],
    [targets.min(), targets.max()],
    "r--",
    linewidth=2,
    label="ç†æƒ³é¢„æµ‹",
)
ax1.set_xlabel("çœŸå®æˆ¿ä»· ($100,000)")
ax1.set_ylabel("é¢„æµ‹æˆ¿ä»· ($100,000)")
ax1.set_title(f"é¢„æµ‹ vs çœŸå®å€¼ (RÂ² = {r2:.4f})")
ax1.legend()
ax1.grid(True, alpha=0.3)

# æ®‹å·®åˆ†å¸ƒ
ax2 = axes[1]
residuals = predictions - targets
ax2.hist(residuals, bins=50, edgecolor="black", alpha=0.7, color="steelblue")
ax2.axvline(0, color="red", linestyle="--", linewidth=2)
ax2.set_xlabel("æ®‹å·® (é¢„æµ‹ - çœŸå®)")
ax2.set_ylabel("æ ·æœ¬æ•°")
ax2.set_title(f"æ®‹å·®åˆ†å¸ƒ (å‡å€¼: {residuals.mean():.4f})")
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{config.save_dir}/predictions.png", dpi=100)
plt.close()
print(f"é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜: {config.save_dir}/predictions.png")


# =============================================================================
# 11. ç‰¹å¾é‡è¦æ€§åˆ†æ
# =============================================================================
print("\n" + "=" * 60)
print("ã€9. ç‰¹å¾é‡è¦æ€§åˆ†æã€‘")

# ä½¿ç”¨ç¬¬ä¸€å±‚æƒé‡çš„ç»å¯¹å€¼ä½œä¸ºç®€å•çš„ç‰¹å¾é‡è¦æ€§ä¼°è®¡
first_layer_weight = model.hidden[0].weight.data.cpu().numpy()
importance = np.abs(first_layer_weight).mean(axis=0)
importance = importance / importance.sum()  # å½’ä¸€åŒ–

# æ’åº
sorted_idx = np.argsort(importance)[::-1]
sorted_features = [feature_names[i] for i in sorted_idx]
sorted_importance = importance[sorted_idx]

plt.figure(figsize=(10, 6))
bars = plt.barh(range(len(sorted_features)), sorted_importance[::-1], color="steelblue")
plt.yticks(range(len(sorted_features)), sorted_features[::-1])
plt.xlabel("ç›¸å¯¹é‡è¦æ€§")
plt.title("ç‰¹å¾é‡è¦æ€§ (åŸºäºç¬¬ä¸€å±‚æƒé‡)")
plt.grid(True, alpha=0.3, axis="x")

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, val in zip(bars, sorted_importance[::-1]):
    plt.text(
        val + 0.01,
        bar.get_y() + bar.get_height() / 2,
        f"{val:.3f}",
        va="center",
        fontsize=9,
    )

plt.tight_layout()
plt.savefig(f"{config.save_dir}/feature_importance.png", dpi=100)
plt.close()
print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {config.save_dir}/feature_importance.png")

print("\nç‰¹å¾é‡è¦æ€§æ’å:")
for i, (name, imp) in enumerate(zip(sorted_features, sorted_importance), 1):
    print(f"  {i}. {name}: {imp:.4f}")


# =============================================================================
# 12. æ®‹å·®åˆ†æ
# =============================================================================
print("\n" + "=" * 60)
print("ã€10. æ®‹å·®åˆ†æã€‘")

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# æ®‹å·® vs é¢„æµ‹å€¼
ax1 = axes[0]
ax1.scatter(predictions, residuals, alpha=0.5, s=10)
ax1.axhline(0, color="red", linestyle="--", linewidth=2)
ax1.set_xlabel("é¢„æµ‹å€¼")
ax1.set_ylabel("æ®‹å·®")
ax1.set_title("æ®‹å·® vs é¢„æµ‹å€¼")
ax1.grid(True, alpha=0.3)

# Q-Q å›¾ (ç®€åŒ–ç‰ˆ)
ax2 = axes[1]
sorted_residuals = np.sort(residuals)
theoretical_quantiles = np.linspace(0.001, 0.999, len(sorted_residuals))
from scipy import stats

theoretical_values = (
    stats.norm.ppf(theoretical_quantiles) * residuals.std() + residuals.mean()
)
ax2.scatter(theoretical_values, sorted_residuals, alpha=0.5, s=10)
ax2.plot(
    [theoretical_values.min(), theoretical_values.max()],
    [theoretical_values.min(), theoretical_values.max()],
    "r--",
    linewidth=2,
)
ax2.set_xlabel("ç†è®ºåˆ†ä½æ•°")
ax2.set_ylabel("æ ·æœ¬åˆ†ä½æ•°")
ax2.set_title("Q-Q å›¾ (æ­£æ€æ€§æ£€éªŒ)")
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{config.save_dir}/residuals.png", dpi=100)
plt.close()
print(f"æ®‹å·®åˆ†æå·²ä¿å­˜: {config.save_dir}/residuals.png")


# =============================================================================
# 13. ä¿å­˜æ¨¡å‹
# =============================================================================
print("\n" + "=" * 60)
print("ã€11. ä¿å­˜æ¨¡å‹ã€‘")

# ä¿å­˜æœ€ä½³æ¨¡å‹
model_path = f"{config.save_dir}/best_model.pth"
torch.save(
    {
        "model_state_dict": best_model_state,
        "config": {
            "input_dim": input_dim,
            "hidden_dims": config.hidden_dims,
            "dropout_rate": config.dropout_rate,
        },
        "scaler_X_mean": scaler_X.mean_,
        "scaler_X_scale": scaler_X.scale_,
        "scaler_y_mean": scaler_y.mean_,
        "scaler_y_scale": scaler_y.scale_,
        "metrics": {"mse": mse, "rmse": rmse, "mae": mae, "r2": r2},
    },
    model_path,
)
print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")


# =============================================================================
# 14. æ¨ç†ç¤ºä¾‹
# =============================================================================
print("\n" + "=" * 60)
print("ã€12. æ¨ç†ç¤ºä¾‹ã€‘")


def predict_house_price(model, features, scaler_X, scaler_y, device):
    """é¢„æµ‹å•ä¸ªæ ·æœ¬çš„æˆ¿ä»·"""
    model.eval()
    with torch.no_grad():
        # æ ‡å‡†åŒ–
        features_scaled = scaler_X.transform(features.reshape(1, -1))
        # è½¬ä¸ºå¼ é‡
        features_tensor = torch.FloatTensor(features_scaled).to(device)
        # é¢„æµ‹
        prediction_scaled = model(features_tensor).cpu().numpy()
        # åæ ‡å‡†åŒ–
        prediction = scaler_y.inverse_transform(prediction_scaled.reshape(-1, 1))
    return prediction[0, 0]


# éšæœºé€‰æ‹©å‡ ä¸ªæµ‹è¯•æ ·æœ¬
np.random.seed(42)
sample_indices = np.random.choice(len(X_test), 5, replace=False)

print("\né¢„æµ‹ç¤ºä¾‹:")
print("-" * 60)
for idx in sample_indices:
    sample_features = X_test[idx]
    true_price = y_test[idx]
    pred_price = predict_house_price(
        model, sample_features, scaler_X, scaler_y, config.device
    )
    error = abs(pred_price - true_price)
    print(
        f"çœŸå®: ${true_price * 100000:,.0f} | é¢„æµ‹: ${pred_price * 100000:,.0f} | è¯¯å·®: ${error * 100000:,.0f}"
    )


# =============================================================================
# 15. æ€»ç»“
# =============================================================================
print("\n" + "=" * 60)
print("ã€é¡¹ç›®æ€»ç»“ã€‘")
print("=" * 60)

print(f"""
åº”ç”¨çš„ Phase 4 çŸ¥è¯†ç‚¹:
  âœ… å¤šå±‚æ„ŸçŸ¥æœº (MLP) æ¶æ„
  âœ… He åˆå§‹åŒ– (é€‚åˆ ReLU æ¿€æ´»)
  âœ… ReLU æ¿€æ´»å‡½æ•° (éšè—å±‚)
  âœ… BatchNorm (åŠ é€Ÿè®­ç»ƒã€ç¨³å®šæ¢¯åº¦)
  âœ… Dropout (é˜²æ­¢è¿‡æ‹Ÿåˆ)
  âœ… L2 æ­£åˆ™åŒ– (weight_decay)
  âœ… Adam ä¼˜åŒ–å™¨
  âœ… å­¦ä¹ ç‡è°ƒåº¦
  âœ… æ—©åœç­–ç•¥

å›å½’ä»»åŠ¡ç‰¹ç‚¹:
  â€¢ è¾“å‡ºå±‚æ— æ¿€æ´»å‡½æ•° (çº¿æ€§è¾“å‡º)
  â€¢ MSE æŸå¤±å‡½æ•°
  â€¢ è¯„ä¼°æŒ‡æ ‡: RMSE, MAE, RÂ²

ç”Ÿæˆæ–‡ä»¶:
  ğŸ“Š {config.save_dir}/feature_distributions.png
  ğŸ“Š {config.save_dir}/target_distribution.png
  ğŸ“Š {config.save_dir}/training_curves.png
  ğŸ“Š {config.save_dir}/predictions.png
  ğŸ“Š {config.save_dir}/feature_importance.png
  ğŸ“Š {config.save_dir}/residuals.png
  ğŸ’¾ {config.save_dir}/best_model.pth
""")

print("âœ… Phase 4 å®æˆ˜é¡¹ç›®å®Œæˆï¼")
